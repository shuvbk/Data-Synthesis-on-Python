{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7d43cf50",
   "metadata": {},
   "source": [
    "# 1. are pandas series immutable  ? \n",
    "# 2. What is the difference between implicit and explicit Series Indexing ?\n",
    "# 3. Name the special Index attributes and the special rule ? \n",
    "# 4. Can an index object have its own name ? \n",
    "# 5. In a DataFrame what does it mean if a sequence of Series objects is ALIGNED\n",
    "# 6. index attribute provides information about row value labels and the column attribute provides information \n",
    "# about column labels T or F\n",
    "# 7. TO access a Pandas DataFrame, we can pass integer, slice or Boolean values to index and column attributes T or F\n",
    "# 8. Depending on what is passsed via slicing or list values, we can get single rows/columns or multiple row/columns T or F\n",
    "# 9. We use these attributes to access values, not the dataframe. T or F\n",
    "# 10. What is the correct syntax for accessing attributes and methods? \n",
    "# 11. What does iter(obj) do and why do we want to use it? \n",
    "# 12. What is the correct syntax for storing long and complex numbers ? \n",
    "# 13. If you want to enhance a string what would you use? \n",
    "# 14. String elements are zero-indexed and accessible by their position in []. T or F\n",
    "# 15. Which of these are string methods: len, split, join, replace, capitalize, center, find, strip Replace creates whole\n",
    "#  # new string since strings are immutale. \n",
    "# 16. how is == different from  = and what would 2 != '2' print , None == False ? , True == 1 , \n",
    "# 17. what are the two ways to create a set? \n",
    "# 18. What is a ternary expression?\n",
    "# 19.Rangeproduces all integers included in parameter? T or F\n",
    "# 20. what if you want to seperate multiple statements on a single line?\n",
    "# 21. The textual content of an HTTP reply is typically written in which language? \n",
    "# 22. Are strings and tuples immutable? \n",
    "# 23. What are scalar types? \n",
    "# 24. What is type casting?\n",
    "# 25. Sets are similar to dictionaries except... \n",
    "# 26. A ternary expression is...\n",
    "# 27. what system handles http requests ? \n",
    "# 28. What does an HTTP request return ?\n",
    "# 29. what is the difference between r.content and r.text\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "076ed657",
   "metadata": {},
   "source": [
    "30. What command do you use to write HTML in a python block?\n",
    "31. whats the point of a html.parser(this is included in beautiful soup)?\n",
    "32. what is the difference between find and find_all\n",
    "33. Which of these is not a tag attribute\n",
    "    a. bytes b. name c. contents d. children e. text\n",
    "34. T or F \n",
    "Not all find and find_all searches need to start from the original BSoup object \n",
    "35. Attributes are always specified in which part of the tag? \n",
    "36. What is an example of a two levels(outer and inner) list? \n",
    "37. How do you find the datatype of a soup object? \n",
    "38.  What does the function enumerate do?\n",
    "39. Why is it easier to use <thead> or <tbody> ? \n",
    "40. What is the first step of webscraping? \n",
    "41. what is the difference between r.text and  ( r.content, 'html.parser') ? \n",
    "42. soup.title attribute or function ? \n",
    "43. What is the array object in python called? What object type is it? \n",
    "44. T or F : numby array provide scientific functionality and modeling?\n",
    "45. if you wanted to find the data type of an array what should you use? \n",
    "46. describe shape and rank?\n",
    "47. the random module in numpy with the randint() method gives us truly random numbers t or f?\n",
    "48. what does np.random.seed(0) accomplish ? \n",
    "49. numpy subarray slices return _____ rather than copies of array data\n",
    "50. what is the size of this numpy array? [[1 2 3]]\n",
    "51. please define a universal function and what its used for?\n",
    "52. difference between add and sum?\n",
    "53. What is vectorization? \n",
    "54. give me an example of a unary ufuncs operation?\n",
    "55. np.empty(10) what does this print? \n",
    "56. What does broadcasting do ? \n",
    "57. What is the difference between np.sum vs np.add.reduce\n",
    "58. what is the difference between np.matmul(A,B) and np.multiple(A,B)?\n",
    "59. What does the timeit module do in python ? \n",
    "60. Which is faster an iterative sum or a vectorized sum ? \n",
    "61. T or F \n",
    "you must import random  with numpy to generate random numbers.\n",
    "62. T or F \n",
    "numpy arrays only use single types for each field\n",
    "63. what is one difference between index objects and NumPy arrays \n",
    "64. difference between loc and iloc? \n",
    "65. Complete the sentence: \n",
    "Think of a DataFrame as a sequence of aligned Series objects, where 'aligned' means that they share the same index. (sentence is complete nvm)\n",
    "66. In a Pandas DataFram, the rows are _____, and the columns are _____\n",
    "67. reviews = reviews.iloc[:,1:] what does this mean? \n",
    "68. Does sort_values() method modify the existing dataframe? (no)\n",
    "two methods to save it 1. assign the sorted Dataframe to another variable\n",
    "                       2.use the inplace argument and set it to true.\n",
    "69. What is the main difference between np.concatenate and pd.concat?\n",
    "70. WHat is the difference between pd.concat and pd.merge?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8df5e46a",
   "metadata": {},
   "source": [
    "71. T or F \n",
    "all key-value pairs are seperated by a comma in a JSON file.\n",
    "72. What is the difference between Json and Dictionaries, even though they are visually similar? \n",
    "73. APis usually return what type of files?\n",
    "74. difference btween rest and streaming API?\n",
    "75. repsonse.json() what does it do ? ?\n",
    "76. What are API endpoints?\n",
    "77. What does ascending = False do? \n",
    "78. We usually use regression to test if there is a ____ association between Y and X \n",
    "79. Each data point in a dataset is an _______, and the ______\n",
    "are the properties or characteristics of each of those observations.\n",
    "(we are using pearson correlation)\n",
    "80. what does r = 1 mean?\n",
    "81. correlation calculatoin using numpy np.corrcoef or x.corr(y) ? \n",
    "82. What is a world cloud? \n",
    "83. Matplotlib, pandas visualization, seaborn which one has the higest level interface. \n",
    "84. rng.randint(0, 10, 6)} what is this telling us? \n",
    "85. Describe Pandas crossTab.\n",
    "86. Why is GroupBy useful ? \n",
    "87. What is API tiered access\n",
    "88. df.to_csv('DemoCSV4-6.csv')  what is this doing?\n",
    "89. What is the primary purpose of an API Wrapper?\n",
    "90. T or F\n",
    ".text and .json() are similar.\n",
    "91. A Rest API request is placed through the requests library, using\n",
    "____ but the URL coresponds to an API service\n",
    "92. What does gps-coordinates.net do in the Nasa API?\n",
    "93. if you wanted to know the next time that the ISS will pass over a given location on earth, what is one method you can use? \n",
    "94. Some news APIs are _____ because they do not require user authentication via an API key. \n",
    "95. Most news APIs require a ________ key. \n",
    "96. What news API does not require a specified user agent? \n",
    "97. describe hacker news? \n",
    "98. the website _____ will be on top of the web page but not page source.\n",
    "99. item_a.get_text(strip=True) whats the point of this? \n",
    "100. How to retrive only top ten stores from news api? \n",
    "Customize URL with embedded query or dictionary of parameters.\n",
    "101. BeautifulSoup .find() what goes inside parenthesis \n",
    "102. API key - , API secret - , TOKEN - , TOKEN secret - \n",
    "103. Is news API a streaming or simple HTTP rest API?\n",
    "104. By default, the concatenation takes place row-wise within the dataframe(axis = 0) \n",
    "105. The join default is _____ (join = 'outer'), but it can be changed to the _______ of the columns using join = 'inner' (union of input columns, intersection). \n",
    "106. Sort_values() method sorts the column, regardless of whether it is a string or numeric\n",
    "107. How many dimensions does a Pandas Series have? \n",
    "one. \n",
    "108. Does the data type within a pandas series have to be homogenous(same type) no! you can mix numeric and non-numeric data types within the same series. Also, a series can have missing values(None) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "68405c21",
   "metadata": {},
   "source": [
    "Mostly notes no more questionaire\n",
    "sort_values\n",
    " - if the column is string, it will be sorted in alphabetical order\n",
    " - the default sorting approach is ascending. To reverse it, specify\n",
    "    ascending = False as a parameter\n",
    " - The sort DOES NOT modify the existing DataFrame. If you need to save the sorting version there are two options: \n",
    "     1. Assign the sorted DataFrame to another variable\n",
    "     2. Use the inplace argument and set it to true\n",
    "Filtering is also known as masking\n",
    "   - filter under one condition requires setting the condition inside square brackets\n",
    "     - if the comparison is equal to, do not forget to use the double equal sign (==)\n",
    "     - if the comparison is not equal to, use '!='\n",
    "  • Filter under more than one condition is done with\n",
    "the Boolean operators (‘and’ ‘or’) depending on\n",
    "the case\n",
    "• The comparisons can be assigned to variables for\n",
    "more clarity  \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66f63536",
   "metadata": {},
   "source": [
    "top_5= df.sort_values(by=\"Median\", ascending=False).head()\n",
    "df.sort_values (syntax)\n",
    "\n",
    "top_median.plot(x = '' , y = [] , kind = 'bar')\n",
    "matplotlib syntax for plots\n",
    "\n",
    "series attributes \n",
    ".values -  returns a tuple with an array of the Series values in the first index and the\n",
    "data type in the second index.\n",
    ".index - returns you the RangeIndex with three objects: The starting index of the\n",
    "list, The index that appears after the last index of the list, and the amount that the\n",
    "indexes increase each at each value\n",
    ".type -  returns the type of values in the Series. In this case it returns ‘O’ for object\n",
    "\n",
    "dataframe attributes\n",
    ".shape -  returns a tuple with the number of rows and columns in the object it is\n",
    "called on. \n",
    ".columns - returns a list of the column headers of the dataframe.\n",
    ".axes - attribute bundles .index and .columns into single output making it even\n",
    "easier to understand the structure of the data that you are working with.\n",
    "\n",
    "includes in lec23\n",
    "\n",
    "\n",
    "\n",
    "TO correct coding errors: \n",
    "- check capitilization \n",
    "- understand the type of error \n",
    "- search for the exact error message online\n",
    "- Inspect the part of the code identified as 'not working'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa889331",
   "metadata": {},
   "source": [
    "quick list of statistical functions available in Pandas as methods\n",
    "count()\n",
    "first() , last()\n",
    "mean() , last()\n",
    "mean() , median() \n",
    "min() , max() \n",
    "std() , var() \n",
    "mad() - mean absolute dev\n",
    "prod()\n",
    "sum()\n",
    "The other option is describe method\n",
    "Before computing these statistics, it is a good practice to check whether there are missing values and consider whether they should be changed to another value. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "294afe48",
   "metadata": {},
   "source": [
    "Pandas use sentinels for missing data, and two already-existing Python \n",
    "null values: Nan and None.\n",
    "\n",
    "None: Pythonic missing data\n",
    "  - The first sentinel value used by Pandas is None, a Python singleton object\n",
    "that is often used for missing data in Python code. Because None is a\n",
    "Python object, it cannot be used in any arbitrary NumPy/Pandas array,\n",
    "but only in arrays with data type 'object' (i.e., arrays of Python objects)\n",
    "NaN: Missing numerical data\n",
    "The other missing data representation, NaN (acronym for Not a Number),\n",
    "is different; it is a special floating-point value recognized by all systems\n",
    "that use the standard IEEE floating-point representation\n",
    "• NaN and None in Pandas\n",
    "Pandas is built to handle the two of them nearly interchangeably,\n",
    "converting between them where appropriate\n",
    "• Checking for missing values using isnull() and notnull()\n",
    "• Both functions help in checking whether a value is NaN or not. These\n",
    "functions can also be used in Pandas Series in order to find null values\n",
    "in a series.\n",
    "• Filling missing values using fillna(), replace() and interpolate()\n",
    "• In order to fill null values in a datasets, we could use fillna(), replace()\n",
    "and interpolate() functions to replace NaN values with some value of\n",
    "their own.\n",
    "• Interpolate() function is basically used to fill NA values in the dataframe\n",
    "but it uses various interpolation techniques to fill the missing values\n",
    "rather than hard-coding the value.\n",
    "• Dropping missing values using dropna()\n",
    "• In order to drop a null values from a dataframe, we used dropna()\n",
    "• This function drops Rows/Columns of datasets with Null values in\n",
    "different ways.\n",
    "\n",
    "Pandas series objects are immutable. (LC14) \n",
    "(lecture 14 for code) creating datafraime from a single series object\n",
    "Constructing DataFrame Objects\n",
    "Different ways of creating a pd.DataFrame (Ex.2)\n",
    "• From a dictionary (Ex 2-1)\n",
    "• From a single Series object (Ex 2-2)\n",
    "• From a list of dicts (Ex 2-3)\n",
    "• Note: missing keys will be filled out with Nan (Ex 2.4)\n",
    "• From a dictionary series objects (Ex 2.5)\n",
    "• From a two-dimensional NumPy array (Ex 2.6)\n",
    "• From a NumPy structured array (Ex 2.7)\n",
    "\n",
    "\n",
    "• In a Pandas DataFrame, the rows are indexed, and\n",
    "the columns are labeled\n",
    "• Index attribute provides information about row value\n",
    "labels and the column attribute provides information\n",
    "about column labels\n",
    "• To access a Panda DataFrame, we can pass integer,\n",
    "slice or Boolean values to index and column\n",
    "attributes\n",
    "• Depending on what is passed via slicing or list values, we\n",
    "can get single rows/columns or multiple row/columns\n",
    "• Note that these attributes are used to access the\n",
    "DataFrame, not to set their values.\n",
    "\n",
    "Mapping Python operators and Pandas Methods\n",
    "+ add()\n",
    "- sub() , subtract()\n",
    "* mul().multiply()\n",
    "/ truediv(), div(), divide()\n",
    "// floordiv()\n",
    "% mod()\n",
    "** pow()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14553cfe",
   "metadata": {},
   "source": [
    "Pandas allow element-wise operations, both with basic\n",
    "arithmetic (addition, subtraction, multiplication, etc.)\n",
    "and with more sophisticated operations (trigonometric\n",
    "functions, exponential and logarithmic functions, etc.).\n",
    "• This functionality is inherited from NumPy and uFuncs\n",
    "but Pandas include a couple useful twists:\n",
    "• for unary operations like negation and trigonometric\n",
    "functions, these ufuncs will preserve index and column labels\n",
    "in the output, and\n",
    "• for binary operations such as addition and multiplication,\n",
    "Pandas will automatically align indices when passing the\n",
    "objects to the ufunc.\n",
    "\n",
    "• Create a Pandas DataFrame by loading the data in the CSV file\n",
    "• DataFrameName = pd.read_csv(“datafile path”)\n",
    "\n",
    "CSV (Comma or Character Seperated Values) Files are typically created from spreadsheets and represent data in two-dimensional table) LEC 15\n",
    "\n",
    "dataframe = dataframe.iloc [: , 1: ] \n",
    "skip first column and continue\n",
    "dataframe.shape ( rows, columns             ) tuple\n",
    "dataframe.info() columns 0 \n",
    "                  column 1   row count\n",
    "                         2 row count (same one as first tuple point)\n",
    "                         ..\n",
    "                         3\n",
    "                         4\n",
    "                         5\n",
    " Filtering is also known as masking\n",
    " filter under one condition requires setting the condition inside square brackets. \n",
    " If the comparison is equal to, do not forget to use the double \n",
    " sign (==) \n",
    " if the comparison is not equal to use, '!='\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "00427f4e",
   "metadata": {},
   "source": [
    "LC 28 CONTENT\n",
    "\n",
    "Quiz questions:\n",
    "\n",
    "Which of these pivot table statements is correct to change missing values to 0?\n",
    "\t\tpd.pivot_table(data=df, index=['Type'], values='Salary', ..., fill_nan = 0)\n",
    "\t\tpd.pivot_table(data=df, index=['Type'], values='Salary', ..., fill_value = 0)\n",
    "\t\tpd.pivot_table(data=df, index=['Type'], values='Salary', ..., change_nan = 0)\n",
    "\t\tpd.pivot_table(data=df, index=['Type'], values='Salary', ..., nan_value = 0)\n",
    "correct answer is fill_value\n",
    "\n",
    "df = pd.read_csv()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.ColumnName.value_counts() \n",
    "iterating through the rows of the column and retrieving their values\n",
    "\n",
    "df[['ColumnName01' , 'ColumnName02']].describe()\n",
    "count , mean , std , min , 25% , 50 % , 75%  , max \n",
    "\n",
    "dfmask_paid = df.Price > 0 \n",
    "dfmake_rating = df['User Rating'] < 1.5 # why couldn't you use       \n",
    "                                        df.user rating ?\n",
    "df[dfmask_paid & dfmask_ratings].head(20) \n",
    "\n",
    "fser = df[dfmask_paid & dfmask_ratings]\n",
    "len(fser) \n",
    "this just gives us the count that satisfys filter conditions\n",
    "\n",
    "fser[['ID', 'Price' , 'User Rating]].head(len(fser))\n",
    "\n",
    "dfmask_free = df[df.Price == 0.00] \n",
    "numFreeApps = len(dfmask_free)\n",
    "#print(numFreeApps)\n",
    "allrows = df.shape[0]\n",
    "Free_apps = (numFreeApps/allrows) *100\n",
    "Free_apps_f = '{:.2f}'.format(Free_apps)\n",
    "print('The percentage of free apps is: ', Free_apps_f) \n",
    "83.57\n",
    "print('The percentage of free apps is: {:.2f}%'.format(Free_apps)) 83.57%\n",
    "\n",
    "lambda\n",
    "df['NumCharName'] = df.apply(lambda x: len(x['Name']), axis = 1)\n",
    "df.info\n",
    "df.head(10) \n",
    "# this has the same number of rows as the original column 'Name'\n",
    "\n",
    "df['Languages'].fillna('0' , inplace =True )\n",
    "df['NumLanguages'] = df.apply(lambda x: (len(x['Languages'].split(','))), axis = 1) \n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "adding columns\n",
    "\n",
    "df['ReleaseYear'] = pd.DatetimeIndex(df['Original Release Date']).year\n",
    "#df.info()\n",
    "df.head(20)\n",
    "df['ReleaseYear'].value_counts()\n",
    "OUTPUT: \n",
    "2016    3126\n",
    "2017    2818\n",
    "2018    2504\n",
    "2015    2216\n",
    "2014    1893\n",
    "2019    1570\n",
    "2013    1161\n",
    "2012     729\n",
    "2011     490\n",
    "2010     277\n",
    "2009     176\n",
    "2008      47\n",
    "Name: ReleaseYear, dtype: int64\n",
    "\n",
    "this is only the year so we used pd.DatetimeIndex\n",
    "\n",
    "# again, we are adding a new column \n",
    "df['StarDate_Year'] = pd.to_datetime(df['StartDateTime'].dt.year\n",
    "year01 = [2017, 2018, 2019]\n",
    "\n",
    "df1 = df[df.StartDate_Year.isin(year01)].copy()\n",
    "# df1 is df with just selected years\n",
    "df1.info()\n",
    "\n",
    "#Step 2: Data analysis\n",
    "\n",
    "#Req 1: Print the number rows and columns in the reduced dataset, and missing values check \n",
    "#and whether there are any missing values in the entire dataframe in one word (True or False).\n",
    "print(' 1. The reduced dataset has {0} rows and {1} columns.'.format(df1.shape[0], df1.shape[1]))\n",
    "print(\"Are there missing values? {} \\n\".format(df1.isnull().any().any()))\n",
    "\n",
    "Whats the difference if I were to use .any() instead of .any().any()\n",
    ".any() will give you the true false for nan values per row instead of the entire dataset all at once. \n",
    "Whether there are any missing values in the entire dataframe in one word (True or False).\n",
    "\n",
    "\n",
    "# pivot tables with the number of permits by category for all boroughs in your assigned period with marginal totals.\n",
    "\n",
    "table1 = df1.pivot_table('EventID', index = 'StartDate_Year', columns = 'Borough', aggfunc = 'count' , margins= True)\n",
    "display(table1) # this is a real function \n",
    "\n",
    "#Req 3: Pivot table with the max permit duration by year and category \n",
    "table2 = df1.pivot_table('DurationHrs', index = 'StartDate_Year', columns = 'Category', aggfunc = 'max')\n",
    "print ('3) Create a pivot table to show the maximum permit duration by year and by category.')\n",
    "display(table2)\n",
    "\n",
    "\n",
    "#Req 4:\tPivot table with number of permits by year in assigned category (after filtering for assigned category)\n",
    "#number of permits by borough and by year (with years in the columns).\n",
    "df2 = df1[df1.Category == 'Commercial'].copy()  #df2 is df with assigned category \n",
    "table3 = df2.pivot_table('EventID', index = 'Borough', columns='StartDate_Year', aggfunc = 'count')\n",
    "print ('4) Filter your dataframe to retain your assigned category only and create a pivot table to show the number of permits by borough and by year (with years in the columns).')\n",
    "display(table3)\n",
    "\n",
    "OUTPUT\n",
    "StartDate_Year\t2017\t2018\t2019\n",
    "Borough\t\t\t\n",
    "Bronx\t13.0\t19.0\t6.0\n",
    "Brooklyn\t185.0\t220.0\t176.0\n",
    "Manhattan\t352.0\t329.0\t275.0\n",
    "Queens\t42.0\t34.0\t27.0\n",
    "Staten Island\t5.0\tNaN\t7.0\n",
    "\n",
    "#Req 5: Display graphically the pivot table shown above with the legend outside the graph\n",
    "print ('5) Graphically show the pivot table created above and make sure that the Boroughs are in the X axis and the legend appears outside the graph.  ')\n",
    "display(table3.plot(kind ='bar').legend(loc='center left', bbox_to_anchor=(1, 0.5)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7edce4b",
   "metadata": {},
   "source": [
    "LC 27: \n",
    "Data science process diagram (Part A and Part B ) \n",
    "\n",
    "Exploratory Data Analysis\n",
    "Part A: \n",
    "Raw Data Collected, Data is processed, clean data set\n",
    "\n",
    "Part B: \n",
    "Data Product, Communicate Visualize Report, Models & Algorithms\n",
    "\n",
    "making decisions\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "#Deleting the unnamed column \n",
    "advert.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "advert.head()\n",
    "\n",
    "#R2: checking for missing values\n",
    "print(\"Are there any missing values? {}\".format(advert.isnull().any().any()))\n",
    "\n",
    "#R3: Show all statistics of sales and a histogram\n",
    "advert['Sales'].describe()\n",
    "\n",
    "#Show a histogram with the distribution of sales\n",
    "import seaborn as sns\n",
    "sns.histplot(advert['Sales'], bins=20)\n",
    "\n",
    "#R4: Correlation Matrix and heatmap\n",
    "corr_mat=advert.corr().round(2)\n",
    "#corr_mat\n",
    "sns.heatmap(data=corr_mat, annot=True)\n",
    "\n",
    "#R5: Multiple Regression Analysis\n",
    "import statsmodels.formula.api as smf\n",
    "reg=smf.ols(formula='Sales~TV+Radio+Newspaper',data=advert).fit()\n",
    "print(reg.summary())\n",
    "\n",
    "OUTPUT\n",
    "                             OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                  Sales   R-squared:                       0.897\n",
    "Model:                            OLS   Adj. R-squared:                  0.896\n",
    "Method:                 Least Squares   F-statistic:                     570.3\n",
    "Date:                Sat, 14 May 2022   Prob (F-statistic):           1.58e-96\n",
    "Time:                        18:12:01   Log-Likelihood:                -386.18\n",
    "No. Observations:                 200   AIC:                             780.4\n",
    "Df Residuals:                     196   BIC:                             793.6\n",
    "Df Model:                           3                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept      2.9389      0.312      9.422      0.000       2.324       3.554\n",
    "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
    "Radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
    "Newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
    "==============================================================================\n",
    "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
    "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
    "Kurtosis:                       6.332   Cond. No.                         454.\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "OUTPUT END\n",
    "\n",
    "remember smf.ols gives you the ols results statistics\n",
    "what if we wanted the formula? \n",
    "\n",
    "#print(reg.params.round(2)) #[0] Intercept, [1] coeff of TV...\n",
    "Eq = reg.params.round(2)\n",
    "#Eq\n",
    "\n",
    "print('Regression equation is: YHat = ', Eq[0], '+',Eq[1],'*Xtv +',Eq[2],'*Xradio +', Eq[3], '*Xnews')\n",
    "OP : Regression equation is: YHat =  2.94 + 0.05 *Xtv + 0.19 *Xradio + -0.0 *Xnews\n",
    "what does Eq give you ? \n",
    "Intercept    2.94\n",
    "TV           0.05\n",
    "Radio        0.19\n",
    "Newspaper   -0.00\n",
    "dtype: float64\n",
    "\n",
    "print(type(reg.params))\n",
    "print(reg.params)\n",
    "print('this is reg.params[0]', reg.params[0])\n",
    "\n",
    "OP: \n",
    "<class 'pandas.core.series.Series'>\n",
    "Intercept    2.938889\n",
    "TV           0.045765\n",
    "Radio        0.188530\n",
    "Newspaper   -0.001037\n",
    "dtype: float64\n",
    "this is reg.params[0] 2.938889369459407\n",
    "\n",
    "___________________________________________________________________-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('appstoregames.csv')\n",
    "\n",
    "#Q1 - Listing the characteristics of the DataFrame with .info()\n",
    "df.info()\n",
    "\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 17007 entries, 0 to 17006\n",
    "Data columns (total 14 columns):\n",
    " #   Column                        Non-Null Count  Dtype  \n",
    "---  ------                        --------------  -----  \n",
    " 0   ID                            17007 non-null  int64  \n",
    " 1   Name                          17007 non-null  object \n",
    " 2   Subtitle                      5261 non-null   object \n",
    " 3   User Rating                   7561 non-null   float64\n",
    " 4   Count of Ratings              7561 non-null   float64\n",
    " 5   Price                         16983 non-null  float64\n",
    " 6   In-app Purchases              7683 non-null   object \n",
    " 7   Developer                     17007 non-null  object \n",
    " 8   Age Rating                    17007 non-null  object \n",
    " 9   Languages                     16947 non-null  object \n",
    " 10  Size                          17006 non-null  float64\n",
    " 11  Genre                         17007 non-null  object \n",
    " 12  Original Release Date         17007 non-null  object \n",
    " 13  Current Version Release Date  17007 non-null  object \n",
    "dtypes: float64(4), int64(1), object(9)\n",
    "memory usage: 1.8+ MB\n",
    "____________________________________________________________________________\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "243f12fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LC 26: webscrape review\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "url = 'https://www.worldometers.info/geography/how-many-countries-in-africa/'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "name = soup.find('h1') # finds heading tag h1 \n",
    "\n",
    "a_table = soup.find('table') # finds the first table \n",
    "\n",
    "headers = [] # initialize a list to store headers of table \n",
    "for header in a_table.find('thead').find_all('th'): # pulling headers from table. will be used for dictionary key \n",
    "    headers.append(header.text) #appends the 4 headers to list \n",
    "\n",
    "countrydata = [] # initialize list of dictionaries\n",
    "\n",
    "rows = a_table.find('tbody').find_all('tr')\n",
    "for row in rows: \n",
    "    values = []  # initialize list that will be values for dictionary   \n",
    "    for col in row.find_all('td'): # iterate through td tags, data cells of table \n",
    "        values.append(col.text)\n",
    "    if values:\n",
    "        a_dict = {}  # initialize dictionary \n",
    "        for i in range(len(values)):\n",
    "            a_dict[headers[i]]=values[i] # 4 elements in headers is key for dictionary \n",
    "    countrydata.append(a_dict)\n",
    "\n",
    "totalarea = 0 # initialize variable to store total numerical values\n",
    "for country in countrydata:   # iterate through our list of dictionaries \n",
    "    population = country['Population(2020)'].replace(',','')  # remove comma \n",
    "    totalarea += float(population)\n",
    "\n",
    "avg = totalarea/len(countrydata) \n",
    "\n",
    "\n",
    "print ('1) The heading of the webpage is: ', name.text, '\\n')\n",
    "print ('2) List of dictionaries: ')\n",
    "print (countrydata)\n",
    "    \n",
    "print ('\\n 3. Average Population in 2020: ', \"{:,.2f}\".format(avg))\n",
    "_____________\n",
    "OUTPUT\n",
    "1) The heading of the webpage is:  Countries in Africa: \n",
    "\n",
    "2) List of dictionaries: \n",
    "[{'#': '1', 'Country': 'Nigeria', 'Population(2020)': '206,139,589', 'Subregion': 'Western Africa'}, {'#': '2', 'Country': 'Ethiopia', 'Population(2020)': '114,963,588', 'Subregion': 'Eastern Africa'}, {'#': '3', 'Country': 'Egypt', 'Population(2020)': '102,334,404', 'Subregion': 'Northern Africa'}, {'#': '4', 'Country': 'DR Congo', 'Population(2020)': '89,561,403', 'Subregion': 'Middle Africa'}, {'#': '5', 'Country': 'Tanzania', 'Population(2020)': '59,734,218', 'Subregion': 'Eastern Africa'}, {'#': '6', 'Country': 'South Africa', 'Population(2020)': '59,308,690', 'Subregion': 'Southern Africa'}, {'#': '7', 'Country': 'Kenya', 'Population(2020)': '53,771,296', 'Subregion': 'Eastern Africa'}, {'#': '8', 'Country': 'Uganda', 'Population(2020)': '45,741,007', 'Subregion': 'Eastern Africa'}, ......\n",
    "3. Average Population in 2020:  24,793,085.26\n",
    "__________________________________________________________________________________________________\n",
    "\n",
    "#Save scraped data in a pandas dataframe\n",
    "import pandas as pd\n",
    "df_country = pd.DataFrame(countrydata)\n",
    "df_country.rename(columns = {'#':'Number'}, inplace=True) # how to rename a column of a dataframe? \n",
    "df_country\n",
    "OUTPUT______________________________________________________\n",
    "   Number\tCountry\tPopulation(2020)\tSubregion\n",
    "0\t1\tNigeria\t206,139,589\tWestern Africa\n",
    "1\t2\tEthiopia\t114,963,588\tEastern Africa\n",
    "2\t3\tEgypt\t102,334,404\tNorthern Africa\n",
    "3\t4\tDR Congo\t89,561,403\tMiddle Africa\n",
    "4\t5\tTanzania\t59,734,218\tEastern Africa\n",
    "5\t6\tSouth Africa\t59,308,690\tSouthern Africa\n",
    "6\t7\tKenya\t53,771,296\tEastern Africa\n",
    "7\t8\tUganda\t45,741,007\tEastern Africa\n",
    "8\t9\tAlgeria\t43,851,044\tNorthern Africa\n",
    "9\t10\tSudan\t43,849,260\tNorthern Africa\n",
    "10\t11\tMorocco\t36,910,560\tNorthern Africa\n",
    "11\t12\tAngola\t32,866,272\tMiddle Africa\n",
    "12\t13\tMozambique\t31,255,435\tEastern Africa\n",
    "13\t14\tGhana\t31,072,940\tWestern Africa\n",
    "14\t15\tMadagascar\t27,691,018\tEastern Africa\n",
    "15\t16\tCameroon\t26,545,863\tMiddle Africa\n",
    "16\t17\tCôte d'Ivoire\t26,378,274\tWestern Africa\n",
    "17\t18\tNiger\t24,206,644\tWestern Africa\n",
    "18\t19\tBurkina Faso\t20,903,273\tWestern Africa\n",
    "19\t20\tMali\t20,250,833\tWestern Africa\n",
    "20\t21\tMalawi\t19,129,952\tEastern Africa\n",
    "21\t22\tZambia\t18,383,955\tEastern Africa\n",
    "22\t23\tSenegal\t16,743,927\tWestern Africa\n",
    "23\t24\tChad\t16,425,864\tMiddle Africa\n",
    "24\t25\tSomalia\t15,893,222\tEastern Africa\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "53\n",
    "\n",
    "adding columns\n",
    "df_country['Population']= df_country['Population(2020)'].str.replace(',','').astype(int)\n",
    "df_country.info()\n",
    "OUTPUT__________________________________\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 54 entries, 0 to 53\n",
    "Data columns (total 5 columns):\n",
    " #   Column            Non-Null Count  Dtype \n",
    "---  ------            --------------  ----- \n",
    " 0   Number            54 non-null     object\n",
    " 1   Country           54 non-null     object\n",
    " 2   Population(2020)  54 non-null     object\n",
    " 3   Subregion         54 non-null     object\n",
    " 4   Population        54 non-null     int32 \n",
    "dtypes: int32(1), object(4)\n",
    "memory usage: 2.0+ KB\n",
    "__________________________________________________\n",
    "\n",
    "pp_avg = df_country['Population'].mean()\n",
    "\n",
    "print ('Average Population in 2020: ', \"{:,.2f}\".format(pp_avg))\n",
    "\n",
    "OP _________\n",
    "Average Population in 2020:  24,793,085.26\n",
    "______________\n",
    "\n",
    "#Analysis by Subregion\n",
    "#Web Scraping Challenge #2\n",
    "df_subr = df_country.groupby('Subregion').agg({'Population': ['mean', 'median']})\n",
    "df_subr\n",
    "#df_subr.plot(kind='bar')\n",
    "                          Population\n",
    "                        mean\tmedian\n",
    "Subregion\t\t\n",
    "Eastern Africa\t2.467986e+07\t15378073.0\n",
    "Middle Africa\t1.995501e+07\t5518087.0\n",
    "Northern Africa\t4.093920e+07\t40379910.0\n",
    "Southern Africa\t1.350073e+07\t2351627.0\n",
    "Western Africa\t2.511595e+07\t12627997.5\n",
    "____________________________________________________\n",
    "\n",
    "df_subr_tot = df_country.groupby('Subregion').sum().plot(kind='bar')\n",
    "\n",
    "bar graph with population sums for subregions\n",
    "\n",
    "NOTE: The main difference between statsmodels.formula.api (smf) and\n",
    "statsmodels.api (sm) is that in the latter\n",
    "the predictors do not have to be enumerated individually. They can be\n",
    "assigned to an object as in the scikit-learn library. This can be\n",
    "extremely helpful, especially with large data sets that have many\n",
    "variables.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4549c1b",
   "metadata": {},
   "source": [
    "LC 25: Boston Hosing sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.keys())\n",
    "OP __________________________\n",
    "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
    "______________________\n",
    "\n",
    "print(boston_dataset.DESCR)\n",
    "OP_________________________________________________________________\n",
    ".. _boston_dataset:\n",
    "\n",
    "Boston house prices dataset\n",
    "---------------------------\n",
    "\n",
    "**Data Set Characteristics:**  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
    "\n",
    "\n",
    "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    "prices and the demand for clean air', J. Environ. Economics & Management,\n",
    "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    "pages 244-261 of the latter.\n",
    "\n",
    "The Boston house-price data has been used in many machine learning papers that address regression\n",
    "problems.   \n",
    "     \n",
    ".. topic References\n",
    "\n",
    "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "_______________________________________________________________________________________________________________\n",
    "\n",
    "#Load the data into a Pandas dataframe and inspect it\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston.head()\n",
    "        CRIM\tZN\tINDUS\tCHAS\tNOX\tRM\tAGE\tDIS\tRAD\tTAX\t               PTRATIO   B\t    LSTAT\n",
    "0\t0.00632\t18.0\t2.31\t0.0\t0.538\t6.575\t65.2\t4.0900\t1.0\t296.0\t15.3\t396.90\t4.98\n",
    "1\t0.02731\t0.0\t7.07\t0.0\t0.469\t6.421\t78.9\t4.9671\t2.0\t242.0\t17.8\t396.90\t9.14\n",
    "2\t0.02729\t0.0\t7.07\t0.0\t0.469\t7.185\t61.1\t4.9671\t2.0\t242.0\t17.8\t392.83\t4.03\n",
    "3\t0.03237\t0.0\t2.18\t0.0\t0.458\t6.998\t45.8\t6.0622\t3.0\t222.0\t18.7\t394.63\t2.94\n",
    "4\t0.06905\t0.0\t2.18\t0.0\t0.458\t7.147\t54.2\t6.0622\t3.0\t222.0\t18.7\t396.90\t5.33\n",
    "\n",
    "__________________________\n",
    "\n",
    "#Note that the target variable (MEDV: median price of houses) is missing\n",
    "#Add it to the dataframe\n",
    "boston['MEDV']= boston_dataset.target\n",
    "boston.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df85e177",
   "metadata": {},
   "source": [
    "#Note that the target variable (MEDV: median price of houses) is missing       THIS IS STILL LC 25\n",
    "#Add it to the dataframe                                                             you will find general workflow info slides\n",
    "boston['MEDV']= boston_dataset.target\n",
    "boston.info()\n",
    "_________________________________________________\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 506 entries, 0 to 505\n",
    "Data columns (total 14 columns):\n",
    " #   Column   Non-Null Count  Dtype  \n",
    "---  ------   --------------  -----  \n",
    " 0   CRIM     506 non-null    float64\n",
    " 1   ZN       506 non-null    float64\n",
    " 2   INDUS    506 non-null    float64\n",
    " 3   CHAS     506 non-null    float64\n",
    " 4   NOX      506 non-null    float64\n",
    " 5   RM       506 non-null    float64\n",
    " 6   AGE      506 non-null    float64\n",
    " 7   DIS      506 non-null    float64\n",
    " 8   RAD      506 non-null    float64\n",
    " 9   TAX      506 non-null    float64\n",
    " 10  PTRATIO  506 non-null    float64\n",
    " 11  B        506 non-null    float64\n",
    " 12  LSTAT    506 non-null    float64\n",
    " 13  MEDV     506 non-null    float64\n",
    "dtypes: float64(14)\n",
    "memory usage: 55.5 KB\n",
    "\n",
    "#Print dimensions of complete dataset\n",
    "print(\"The complete dataframe has {0} rows and {1} columns\".format(boston.shape[0], boston.shape[1]))\n",
    "\n",
    "OP ___________________\n",
    "The complete dataframe has 506 rows and 14 columns\n",
    "\n",
    "#Checking for missing values and print if there are any\n",
    "print(\"Are there missing values? {}\".format(boston.isnull().any().any()))\n",
    "\n",
    "Are there missing values? False\n",
    "\n",
    "EXPLORATORY ANALYSIS\n",
    "(GRAPH ON PDF) \n",
    "#Distribution of the target variable (MEDV)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)}) # just sizing for seaborn histograms\n",
    "sns.histplot(boston['MEDV'], bins=30)\n",
    "plt.show()\n",
    "\n",
    "boston['MEDV'].describe()\n",
    "count    506.000000\n",
    "mean      22.532806\n",
    "std        9.197104\n",
    "min        5.000000\n",
    "25%       17.025000\n",
    "50%       21.200000\n",
    "75%       25.000000\n",
    "max       50.000000\n",
    "Name: MEDV, dtype: float64\n",
    "\n",
    "#Correlation matrix and visualization\n",
    "correlation_matrix = boston.corr().round(2)\n",
    "# annot = True to print the values inside each square\n",
    "sns.heatmap(data=correlation_matrix, annot=True)\n",
    "<AxesSubplot:>\n",
    "\n",
    "\n",
    "#Scatter plot of LSTAT vs. MEDV (GRAPH ON PDF)\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = boston['LSTAT']\n",
    "y = boston['MEDV']\n",
    "plt.scatter(x, y, marker='o')\n",
    "plt.title('LSTAT vs MEDV')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "\n",
    "\n",
    "#Scatter plot of RM vs. MEDV\n",
    "plt.figure(figsize=(20, 8))\n",
    "x = boston['RM']\n",
    "y = boston['MEDV']\n",
    "plt.scatter(x, y,marker='o')\n",
    "plt.title('RM vs MEDV')\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('MEDV')\n",
    "\n",
    "GRAPHS ON PDF\n",
    "\n",
    "REGRESSION ANALYSES\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "model1 = smf.ols('MEDV ~ LSTAT', data=boston)\n",
    "model1 = model1.fit()\n",
    "\n",
    "#Predict values\n",
    "price_pred1 = model1.predict()\n",
    "\n",
    "NO OP NO OP NO OP NO OUTPUT \n",
    "\n",
    "#Plot regression against actual data\n",
    "plt.figure(figsize= (12,6))\n",
    "plt.plot(boston['LSTAT'], boston['MEDV'], 'o') #scatter plot of actual data\n",
    "plt.plot(boston['LSTAT'], price_pred1, 'r', linewidth = 3) #regression line\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "pl\n",
    "t.title('LSTAT vs. MEDV')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "___________\n",
    "#Showing regression results\n",
    "print(model1.summary())\n",
    "\n",
    "OP__________________________\n",
    " OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                   MEDV   R-squared:                       0.544\n",
    "Model:                            OLS   Adj. R-squared:                  0.543\n",
    "Method:                 Least Squares   F-statistic:                     601.6\n",
    "Date:                Tue, 17 May 2022   Prob (F-statistic):           5.08e-88\n",
    "Time:                        01:05:32   Log-Likelihood:                -1641.5\n",
    "No. Observations:                 506   AIC:                             3287.\n",
    "Df Residuals:                     504   BIC:                             3295.\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept     34.5538      0.563     61.415      0.000      33.448      35.659\n",
    "LSTAT         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n",
    "==============================================================================\n",
    "Omnibus:                      137.043   Durbin-Watson:                   0.892\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\n",
    "Skew:                           1.453   Prob(JB):                     5.36e-64\n",
    "Kurtosis:                       5.319   Cond. No.                         29.7\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "______________________________________________\n",
    "\n",
    "\n",
    "model2 = smf.ols('MEDV ~ RM', data=boston)\n",
    "model2 = model2.fit()\n",
    "\n",
    "#Predict values\n",
    "price_pred2 = model2.predict()\n",
    "print(model2.summary())\n",
    "\n",
    "#Plot regression against actual data\n",
    "plt.figure(figsize= (12,6))\n",
    "plt.plot(boston['RM'], boston['MEDV'], 'o') #scatter plot of actual data\n",
    "plt.plot(boston['RM'], price_pred2, 'g', linewidth = 4) #regression line   # 'g' means green 'r' means red\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('MEDV')\n",
    "plt.title('RM vs. MEDV')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MULTIPLE REGRESSION\n",
    "#import statsmodels.formula.api as smf\n",
    "reg=smf.ols(formula=\"MEDV ~ RM+LSTAT\",data=boston).fit()\n",
    "print(reg.summary())\n",
    "\n",
    "OP___________________________________\n",
    "OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                   MEDV   R-squared:                       0.639\n",
    "Model:                            OLS   Adj. R-squared:                  0.637\n",
    "Method:                 Least Squares   F-statistic:                     444.3\n",
    "Date:                Tue, 17 May 2022   Prob (F-statistic):          7.01e-112\n",
    "Time:                        01:05:32   Log-Likelihood:                -1582.8\n",
    "No. Observations:                 506   AIC:                             3172.\n",
    "Df Residuals:                     503   BIC:                             3184.\n",
    "Df Model:                           2                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept     -1.3583      3.173     -0.428      0.669      -7.592       4.875\n",
    "RM             5.0948      0.444     11.463      0.000       4.222       5.968\n",
    "LSTAT         -0.6424      0.044    -14.689      0.000      -0.728      -0.556\n",
    "==============================================================================\n",
    "Omnibus:                      145.712   Durbin-Watson:                   0.834\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              457.690\n",
    "Skew:                           1.343   Prob(JB):                    4.11e-100\n",
    "Kurtosis:                       6.807   Cond. No.                         202.\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "____________________________________________________\n",
    "\n",
    "\n",
    "print(reg.rsquared.round(3))\n",
    "0.639\n",
    "\n",
    "print(reg.params)\n",
    "Intercept   -1.358273\n",
    "RM           5.094788\n",
    "LSTAT       -0.642358\n",
    "dtype: float64\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d10c5d1",
   "metadata": {},
   "source": [
    "LC 24\n",
    "##1.1 Correlation calculation using numpy\n",
    "import numpy as np\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "r = np.corrcoef(x, y) #note that r returns the correlation matrix\n",
    "r\n",
    "\n",
    "OP_______\n",
    "array([[1.        , 0.75864029],\n",
    "       [0.75864029, 1.        ]])\n",
    "--------------------------------------\n",
    "\n",
    "#np.corrcoef returns a correlation matrix with 1 in the diagonal\n",
    "#the correlation coefficient between x and y is in r[0,1] or r[1,0]\n",
    "print(\"the correlation coefficient between x and y is: \", r[0,1])\n",
    "\n",
    "OP___________________\n",
    "the correlation coefficient between x and y is:  0.7586402890911867\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "#1.2 Correlation using scipy\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "r1 = scipy.stats.pearsonr(x, y)    # Pearson's r\n",
    "r1\n",
    "\n",
    "OP____________________________________________________________\n",
    "\n",
    "(0.7586402890911869, 0.010964341301680832)\n",
    "-----------------------------------------------------\n",
    "\n",
    "#the method returns the correlation coefficient and the p-value\n",
    "print(\"the correlation coefficient between x and y is: \", r1[0])\n",
    "\n",
    "OP____________\n",
    "the correlation coefficient between x and y is:  0.7586402890911869\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#1.3 Correlation using pandas\n",
    "import pandas as pd\n",
    "x = pd.Series(range(10, 20))\n",
    "y = pd.Series([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "r2= x.corr(y)                     # Pearson's r\n",
    "r2\n",
    "print(\"the correlation coefficient between x and y is: \", r2)\n",
    "\n",
    "OP___________________________\n",
    "the correlation coefficient between x and y is:  0.7586402890911867\n",
    "------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "z = np.array([5, 3, 2, 1, 0, -2, -8, -11, -15, -16])\n",
    "xyz = np.array([[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
    "                [2, 1, 4, 5, 8, 12, 18, 25, 96, 48],\n",
    "                [5, 3, 2, 1, 0, -2, -8, -11, -15, -16]])\n",
    "corr_matrix = np.corrcoef(xyz).round(decimals=2)\n",
    "corr_matrix\n",
    "\n",
    "OP___________________________________\n",
    "\n",
    "array([[ 1.  ,  0.76, -0.97],\n",
    "       [ 0.76,  1.  , -0.83],\n",
    "       [-0.97, -0.83,  1.  ]])\n",
    "       \n",
    "\n",
    "#Visualization in a heatmap  (GRAPH PDF)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(corr_matrix)\n",
    "im.set_clim(-1, 1)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1, 2), ticklabels=('x', 'y', 'z'))\n",
    "ax.yaxis.set(ticks=(0, 1, 2), ticklabels=('x', 'y', 'z'))\n",
    "ax.set_ylim(2.5, -0.5)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, corr_matrix[i, j], ha='center', va='center',\n",
    "                color='r')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, format='% .2f')\n",
    "plt.show()\n",
    "\n",
    "_______________________________\n",
    "PT 3: Correlation and Regression\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "result = scipy.stats.linregress(x, y)\n",
    "print(\"the slope of the regression line is: \", result.slope)\n",
    "print(\"the intercept of the regression line is: \", result.intercept)\n",
    "\n",
    "OP________________________\n",
    "the slope of the regression line is:  7.4363636363636365\n",
    "the intercept of the regression line is:  -85.92727272727274\n",
    "\n",
    "line = f'Regression line: y={result.intercept:.2f}+{result.slope:.2f}x, r={result.rvalue:.2f}'\n",
    "line\n",
    "\n",
    "'Regression line: y=-85.93+7.44x, r=0.76'\n",
    "\n",
    "#Visualization (GRAPH PDF)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, linewidth=0, marker='s', label='Data points')\n",
    "ax.plot(x, result.intercept + result.slope * x, label=line)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "_______________________________\n",
    "\n",
    "Part IV: Application to advertising dataset\n",
    "\n",
    "#4.1 Linear regression with one predictor using statsmodel\n",
    "advert = pd.read_csv(\"Advertising.csv\")\n",
    "advert.head()\n",
    "Unnamed: 0\tTV\tRadio\tNewspaper\tSales\n",
    "0\t1\t230.1\t37.8\t69.2\t22.1\n",
    "1\t2\t44.5\t39.3\t45.1\t10.4\n",
    "2\t3\t17.2\t45.9\t69.3\t9.3\n",
    "3\t4\t151.5\t41.3\t58.5\t18.5\n",
    "4\t5\t180.8\t10.8\t58.4\t12.9\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Initialize and fit linear regression model using statsmodels\n",
    "model = smf.ols('Sales ~ TV', data=advert)\n",
    "model = model.fit()\n",
    "NO OP\n",
    "# Predict values\n",
    "sales_pred = model.predict()\n",
    "NO OP\n",
    "\n",
    "\n",
    "# Plot regression against actual data on pdf\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(advert['TV'], advert['Sales'], 'o')           # scatter plot showing actual data\n",
    "plt.plot(advert['TV'], sales_pred, 'r', linewidth=2)   # regression line\n",
    "plt.xlabel('TV Advertising Costs')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('TV vs Sales')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "__________________________________\n",
    "\n",
    "#4.2 Linear regression with two predictors using sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build linear regression model using TV and Radio as predictors\n",
    "# Split data into predictors X and output Y\n",
    "predictors = ['TV', 'Radio']\n",
    "X = advert[predictors]\n",
    "y = advert['Sales']\n",
    "\n",
    "# Initialize and fit model\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)\n",
    "\n",
    "no op\n",
    "\n",
    "print(f'alpha = {model.intercept_}')\n",
    "print(f'betas = {model.coef_}')\n",
    "\n",
    "op\n",
    "alpha = 2.9210999124051362\n",
    "betas = [0.04575482 0.18799423]\n",
    "-------------------------------------------\n",
    "\n",
    "alpha = model.intercept_\n",
    "betas = model.coef_\n",
    "#Therefore, themodel can be written as:\n",
    "print('Sales = ', alpha, '+', betas[0], '*TV + ', betas[1], '*Radio')\n",
    "#Sales = 2.921 + 0.046*TV + 0.1880*Radio.\n",
    "\n",
    "op--------------\n",
    "Sales =  2.9210999124051362 + 0.04575481510107615 *TV +  0.1879942266203092 *Radio\n",
    "\n",
    "\n",
    "Some Notes\n",
    "• Other two correlation metrics (Spearman and Kendall\n",
    "are for rank data)\n",
    "• In Python, correlations can be calculated in numpy,\n",
    "pandas, and scipy\n",
    "\n",
    "\n",
    "• To visually represent the relation between two\n",
    "features, use and x-y plot (also called scatter\n",
    "chart). \n",
    "\n",
    "X-y plot is also called a scatter chart\n",
    "\n",
    "Establishes a mathematical function whereby a dependent\n",
    "variable 𝑦𝑦 is explained (or predicted) by a set of independent variables (or features), or 𝐱𝐱= (𝑥𝑥₁, …, 𝑥𝑥ᵣ)\n",
    "• Regression is useful to understand a phenomenon and/or to\n",
    "make predictions (forecast a response) based on a set of\n",
    "variables.\n",
    "• OLS (Ordinary Least Square) Regression is a method that fits a\n",
    "line to a pattern of observations by minimizing the sum of\n",
    "squared residuals.\n",
    "• The residuals are the differences between the observations (in the\n",
    "dataset) and their predicted values according to the regression line.\n",
    "• The coefficient of determination, or 𝑅𝑅², indicates the amount of\n",
    "variation in 𝑦𝑦 can be explained by the dependence on 𝐱𝐱 using the particular regression model.\n",
    "• Larger 𝑅² indicates a better fit and means that the model can better\n",
    "explain the variation of the output with different inputs.\n",
    "\n",
    "Simple linear regression uses only predictor or x\n",
    " Multiple linear regression uses more than one\n",
    "predictor.\n",
    "\n",
    "• OLS (Ordinary Least Square) simple linear\n",
    "regression visualization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af8c4b89",
   "metadata": {},
   "source": [
    "LC 23: \n",
    "#Source: https://realpython.com/pandas-plot-python/\n",
    "import pandas as pd\n",
    "________________________________________\n",
    "d_url = (\"https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv\")\n",
    "df = pd.read_csv(d_url)\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "# the display.max.column option is used to configure the display and make sure\n",
    "# that pandas does not hid any columns. This does not affect the horizontal scrolling. It just shows all the columns\n",
    "# instead of hiding some behind. \n",
    "df.head()\n",
    "________________\n",
    "\n",
    "df.shape\n",
    "(173, 21)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Show the distribution of earnings graphically\n",
    "df.plot(x=\"Rank\", y=[\"P25th\", \"Median\", \"P75th\"])\n",
    "regular line plot\n",
    "graph on pdf\n",
    "\n",
    "#Survey your data by creating a histogram for the median column\n",
    "median_column=df['Median']\n",
    "median_column.plot(kind='hist')\n",
    "_____________________________\n",
    "\n",
    "#Analysis of outliers (top 5 values) in Median column\n",
    "top_5= df.sort_values(by=\"Median\", ascending=False).head()\n",
    "top_5.plot(x='Major', y='Median', kind='bar', fontsize=10)\n",
    "\n",
    "_______________________graph pdf\n",
    "\n",
    "\n",
    "#Which are the majors whose median salary is above $60,000?\n",
    "top_medians = df[df[\"Median\"] > 60000].sort_values(\"Median\")\n",
    "top_medians.plot(x='Major', y=[\"P25th\",'Median',\"P75th\"], kind='bar').legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "-----graph pdf\n",
    "\n",
    "\n",
    "#Compare the median salary with unemployement-rate \n",
    "#Is there a noticeable pattern between these two?\n",
    "df.plot(x=\"Median\", y=\"Unemployment_rate\",kind=\"scatter\")\n",
    "---------graph pdf\n",
    "\n",
    "#Analyze categorical data by grouping\n",
    "#With .groupby(), you create a DataFrameGroupBy object and \n",
    "#with .sum(), you create a Series.\n",
    "cat_totals = df.groupby(\"Major_category\")[\"Total\"].sum().sort_values()\n",
    "cat_totals\n",
    "cat_totals.plot(kind=\"barh\", fontsize=10)\n",
    "\n",
    "\n",
    "#Then create a pie chart to visualize ratios\n",
    "#Lump smaller categories (with a total under 100K) into \"Other\"\n",
    "small_cat_totals = cat_totals[cat_totals < 100_000]\n",
    "big_cat_totals = cat_totals[cat_totals > 100_000]\n",
    "small_sums = pd.Series([small_cat_totals.sum()], index=[\"Other\"])\n",
    "big_cat_totals = big_cat_totals.append(small_sums)\n",
    "big_cat_totals.plot(kind=\"pie\", label=\"\")\n",
    "------------------------------------------------------\n",
    "#Analyze the distribution of data within a category (e.g. Engineering)\n",
    "df[df[\"Major_category\"] == \"Engineering\"][\"Median\"].plot(kind=\"hist\")\n",
    "\n",
    "GEOVISUALIZATION\n",
    "\n",
    "#Source: https://www.learnpythonwithrune.org/3-steps-to-plot-shooting-incident-in-ny-on-a-map-using-python/\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv('CUNYColleges.csv')\n",
    "df.info()\n",
    "df\n",
    "--------------------------------\n",
    "\n",
    "#Relabeling the columns of the dataframe\n",
    "longitude = df['lon']\n",
    "latitude = df['lat']\n",
    "\n",
    "# The boundaries of the image map\n",
    "map_box = [-74.4461, -73.5123, 40.4166, 41.0359]\n",
    "\n",
    "#Getting the map ready from the file with the image\n",
    "map_img = plt.imread('nyc_map.png')\n",
    "\n",
    "#Code to plot each entry on the map\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(longitude, latitude)\n",
    "ax.set_ylim(map_box[2], map_box[3])\n",
    "ax.set_xlim(map_box[0], map_box[1])\n",
    "ax.imshow(map_img, extent=map_box, alpha=0.9)\n",
    "\n",
    "#Saving the new figure and showing the map\n",
    "#plt.savefig('Data/nyc_map_mod1.png')\n",
    "plt.show()\n",
    "\n",
    "---------geograph on pdf\n",
    "\n",
    "SOme NOtes: \n",
    "• Magic command %matplotlib inline\n",
    "• The %matplotlib inline magic command sets up\n",
    "your Jupyter Notebook to display plots within\n",
    "the Notebook. \n",
    "Visualizing Geographic Data\n",
    "• Consists of adding a layer of information to a map\n",
    "• There are different approaches for geovisualization, such as:\n",
    "• With an API (e.g., Google Maps API)\n",
    "• Creating a map with Folium (Python library)\n",
    "• Using Matplotlib\n",
    "• The input to geo-visualization are the latitude and\n",
    "longitude coordinates of the places to plot.\n",
    "• If you have a list of addresses instead, you need to\n",
    "convert each address to its corresponding geo location\n",
    "first\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d46e0e1",
   "metadata": {},
   "source": [
    "LC 22: Qualitative(words) vs Quantitative(numbers)\n",
    "import requests \n",
    "import pandas as pd\n",
    "\n",
    "secret = 'dc8619a4d2ae434393e9fd15f8b6c094'\n",
    "#headers = {apiKey: '*******'}\n",
    "query_params = {\n",
    "      'q': \"metaverse\",\n",
    "      'pageSize': 50,\n",
    "      'apiKey': secret\n",
    "}\n",
    "main_url = 'https://newsapi.org/v2/everything?'\n",
    " \n",
    "# fetching data in json format\n",
    "res = requests.get(main_url, params=query_params)\n",
    "result_page = res.json()\n",
    " \n",
    "# getting all articles in a string article\n",
    "article = result_page[\"articles\"]\n",
    " \n",
    "# empty list to hold all trending news\n",
    "art_num = []\n",
    "art_title = []\n",
    "art_nchar = []\n",
    "art_nword= []\n",
    "     \n",
    "for ar in article:\n",
    "    art_title.append(ar[\"title\"])\n",
    "\n",
    "# printing all trending news       \n",
    "for i in range(len(art_title)):            \n",
    "    art_num.append(i + 1)\n",
    "    art_nchar.append(len(art_title[i]))\n",
    "    art_nword.append(len(art_title[i].split()))\n",
    "# theres another way to poplate data ss google docs\n",
    "newsdf = pd.DataFrame()\n",
    "newsdf['Num'] = art_num\n",
    "newsdf['Title'] = art_title\n",
    "newsdf['Nchar'] = art_nchar\n",
    "newsdf['Nword'] = art_nword\n",
    "newsdf\n",
    "\n",
    "OP________________________________\n",
    "\n",
    "Num\t                     Title\t                        Nchar\tNword\n",
    "0\t1\tLizzo will perform in the first metaverse musi...\t59\t10\n",
    "1\t2\tFacebook Pay will soon become Meta Pay\t38\t7\n",
    "2\t3\tSnap CEO Evan Spiegel thinks the metaverse is ...\t74\t11\n",
    "3\t4\tMark Zuckerberg shows off what Meta's next hea...\t57\t10\n",
    "4\t5\tHere's what NFTs look like on Instagram\t39\t7\n",
    "5\t6\tMeta’s Augmented Reality Glasses Are Still Yea...\t53\t8\n",
    "6\t7\tInvesting giant Fidelity enters the metaverse ...\t55\t8\n",
    "7\t8\tCan we create a moral metaverse?\t32\t6\n",
    "8\t9\tUniversity of Waterloo professors help build t...\t72\t11\n",
    "9\t10\tHow to Watch Meta's 4/20 VR Quest Gaming Showcase\t49\t9\n",
    "10\t11\tHow Seoul is creating a metaverse for a smarte...\t52\t10\n",
    "11\t12\t$2.5M of Bored Ape Yacht Club NFTs Stolen in I...\t66\t13\n",
    "12\t13\tWhat will it take to make the metaverse a posi...\t63\t12\n",
    "13\t14\tFive states move to shut down a metaverse casi...\t169\t28\n",
    "14\t15\tItaly's Serie A enters the Metaverse to showca...\t82\t15\n",
    "15\t16\tGaming platforms FlickPlay, The Sandbox take s...\t77\t11\n",
    "16\t17\tLuxury brand Hermes considers metaverse as mea...\t77\t11\n",
    "17\t18\tFidelity enters the metaverse in search of you...\t72\t11\n",
    "18\t19\tHow To Build Your Business Brand in the Metaverse\t49\t9\n",
    "19\t20\tMeta's Project Cambria VR headset likened to '...\t68\t12\n",
    "20\t21\tSnapchat’s Evan Spiegel dismisses Facebook’s m...\t72\t8\n",
    "21\t22\tEarly Days of the Metaverse in the Built World\t46\t9\n",
    "22\t23\tInsider's Guide to the Metaverse: How to play,...\t118\t22\n",
    "23\t24\tApeCoin dips 16% following Bored Ape Yacht Clu...\t85\t13\n",
    "24\t25\tMark Zuckerberg's metaverse obsession is drivi...\t144\t22\n",
    "25\t26\tThe Vatican prepares a gallery in the metavers...\t58\t10\n",
    "26\t27\tMusic Week - Music Week\t23\t5\n",
    "27\t28\tThe Metaverse Wars: What is the Future of Soci...\t55\t10\n",
    "28\t29\tSnapchat's Evan Spiegel Dismisses Facebook's M...\t72\t8\n",
    "29\t30\tMetaverse Company To Offer Immortality Through...\t66\t9\n",
    "30\t31\tA study indicates that, although most of us ha...\t98\t18\n",
    "31\t32\tFor Meta or for Worse?\t22\t5\n",
    "32\t33\tHorizon Worlds to expand beyond Quest 2 with w...\t75\t13\n",
    "33\t34\tBored Ape Yacht Club creator Yuga Labs is abou...\t145\t25\n",
    "34\t35\tSpotify 1st music streaming service to be avai...\t78\t12\n",
    "35\t36\tIt’s time for businesses to embrace the immers...\t59\t9\n",
    "36\t37\tThe metaverse is a transformational opportunit...\t70\t10\n",
    "37\t38\tManticore Games brings Core to iOS for its fir...\t60\t10\n",
    "38\t39\tThe TEC de Monterrey teaches its first class i...\t61\t11\n",
    "39\t40\tFacebook-owner Meta to open first physical sto...\t75\t12\n",
    "40\t41\tCan you truly own anything in the metaverse? N...\t77\t14\n",
    "41\t42\tJohnson & Johnson will train Mexican students ...\t74\t12\n",
    "42\t43\tThe Morning After: Elon Musk faces lawsuit ove...\t70\t11\n",
    "43\t44\tGamesBeat Summit: What brands need to understa...\t68\t10\n",
    "44\t45\tKore.ai CEO says conversational AI is the foun...\t69\t11\n",
    "45\t46\tMetaverse Fashion Week Was a Promising Prototy...\t76\t12\n",
    "46\t47\tUntamed Planet raises $24.3M for nature-orient...\t58\t8\n",
    "47\t48\tHow the metaverse could unlock sustainable rev...\t69\t10\n",
    "48\t49\tThe metaverse merges science fiction, tech, an...\t53\t8\n",
    "49\t50\tApple Says Plan for Nearly 50% Commission on M...\t93\t14\n",
    "\n",
    "\n",
    "______________________________________________________________________-\n",
    "#Source: https://www.datacareer.de/blog/accessing-the-news-api-in-python/\n",
    "\n",
    "# Different way of populating each cell of the dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(results)):\n",
    "    print(i+1, results[i], len(results[1]), len(results[i].split()))\n",
    "    df.loc[i, 'Num'] = i + 1\n",
    "    df.loc[i, 'Title'] = results[i]\n",
    "    df.loc[i, 'Ncar'] = len(df.loc[i, 'Title'])\n",
    "    df.loc[i, 'Nword'] = len(df.loc[i, 'Title'].split())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Code to create a word cloud with the headlines\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create an empty string\n",
    "text_combined = ''\n",
    "for ar in result_page[\"articles\"]:\n",
    "    text_combined += ar['title'] + ' '\n",
    "\n",
    "#Print the first 300 characters to screen for inspection\n",
    "print(text_combined[0:300])\n",
    "\n",
    "_________________________________________________________________________\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text_combined)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "________________________________________________________\n",
    "\n",
    "\n",
    "#Analysis of outliers github recent grads\n",
    "top_5 = df.sort_values(by=\"Median\", ascending=False).head()\n",
    "top_5.plot(x=\"Major\", y=\"Median\", kind=\"bar\", rot=5, fontsize=4)\n",
    "\n",
    "\n",
    "• Basic descriptive graphs:\n",
    "• Line: for continuous x axis / to show trend over time\n",
    "• Bar: for discontinuous x axis / to show comparison across categories\n",
    "• Pie: for proportions to a whole / to show ratios or relative sizes\n",
    "• Advanced graphs:\n",
    "• Histogram: graphically summarizes the distribution of a variable\n",
    "• Scatter chart: shows the relationship between two variables\n",
    "• Box plot: shows the distribution of numerical data and skewness\n",
    "through displaying the data quartiles (or percentiles) and averages\n",
    "• The type of graph is passed as string in the kind parameter\n",
    "\n",
    "\"area\" is for the area of plots. \n",
    "\"bar\" is for vertical bar charts\n",
    "\"barh\" od for horizontal bar charts\n",
    "\"box\" is for box plots\n",
    "\"hexbin\" is for hexbin plots\n",
    "\"hist\" is for histograms\n",
    "\"kde\" is for kernal density estimate charts.\n",
    "\"density\" is an alias for \"kde\" \n",
    "\"line\" is for the line graph\n",
    "\"pie\" \n",
    "\"scatter\"\n",
    "\n",
    "•\"Median\" is the median earnings of full-time, year-round workers.\n",
    "•\"P25th\" is the 25th percentile of earnings.\n",
    "•\"P75th\" is the 75th percentile of earnings.\n",
    "•\"Rank\" is the major’s rank by median earnings.\n",
    "• Two notes about the code that follows:\n",
    "1. The display.max.columns option is used to configure the display and make\n",
    "sure that pandas does not hide any columns.\n",
    "2. The %matplotlib inline magic command sets up your Jupyter Notebook to\n",
    "display plots within the Notebook.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f56c4a1",
   "metadata": {},
   "source": [
    "LC 21: \n",
    "import seaborn as sns\n",
    "planets = sns.load_dataset('planets')\n",
    "\n",
    "\n",
    "planets.head()\n",
    "method\tnumber\torbital_period\tmass\tdistance\tyear\n",
    "0\tRadial Velocity\t1\t269.300\t7.10\t77.40\t2006\n",
    "1\tRadial Velocity\t1\t874.774\t2.21\t56.95\t2008\n",
    "2\tRadial Velocity\t1\t763.000\t2.60\t19.84\t2011\n",
    "3\tRadial Velocity\t1\t326.030\t19.40\t110.62\t2007\n",
    "4\tRadial Velocity\t1\t516.220\t10.50\t119.47\t2009\n",
    "\n",
    "\n",
    "\n",
    "planets.shape    .shape() gives an error dont use parenthesis\n",
    "\n",
    "(1035, 6)\n",
    "\n",
    "planets.groupby('method')\n",
    "OP____ \n",
    "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000026E35645580>\n",
    "------------\n",
    "\n",
    "planets.groupby('method')['orbital_period']\n",
    "\n",
    "<pandas.core.groupby.generic.SeriesGroupBy object at 0x0000026E3A85E9A0>\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "planets.groupby('method')['orbital_period'].median()\n",
    "\n",
    "OP\n",
    "method\n",
    "Astrometry                         631.180000\n",
    "Eclipse Timing Variations         4343.500000\n",
    "Imaging                          27500.000000\n",
    "Microlensing                      3300.000000\n",
    "Orbital Brightness Modulation        0.342887\n",
    "Pulsar Timing                       66.541900\n",
    "Pulsation Timing Variations       1170.000000\n",
    "Radial Velocity                    360.200000\n",
    "Transit                              5.714932\n",
    "Transit Timing Variations           57.011000\n",
    "Name: orbital_period, dtype: float64\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "#Iteration over groups\n",
    "for (method, group) in planets.groupby('method'):\n",
    "    print(\"{0:30s} shape={1}\".format(method, group.shape))\n",
    "\n",
    "Astrometry                     shape=(2, 6)\n",
    "Eclipse Timing Variations      shape=(9, 6)\n",
    "Imaging                        shape=(38, 6)\n",
    "Microlensing                   shape=(23, 6)\n",
    "Orbital Brightness Modulation  shape=(3, 6)\n",
    "Pulsar Timing                  shape=(5, 6)\n",
    "Pulsation Timing Variations    shape=(1, 6)\n",
    "Radial Velocity                shape=(553, 6)\n",
    "Transit                        shape=(397, 6)\n",
    "Transit Timing Variations      shape=(4, 6)  \n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#Dispatch methods allows to call pandas methods on the groups\n",
    "planets.groupby('method').describe().unstack()\n",
    "\n",
    "number  count  Astrometry                          2.0\n",
    "               Eclipse Timing Variations           9.0\n",
    "               Imaging                            38.0\n",
    "               Microlensing                       23.0\n",
    "               Orbital Brightness Modulation       3.0\n",
    "                                                 ...  \n",
    "year    max    Pulsar Timing                    2011.0\n",
    "               Pulsation Timing Variations      2007.0\n",
    "               Radial Velocity                  2014.0\n",
    "               Transit                          2014.0\n",
    "               Transit Timing Variations        2014.0\n",
    "Length: 400, dtype: float64\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "#Challenge: Analyze planet discovery by method and by decade\n",
    "#Count discovered planets by method and decade\n",
    "#Calculate decade by dividing each year by 10 and adding a 0 at the end\n",
    "decade = 10* (planets['year'] //10)\n",
    "#Cast decade as a string and add an s at the end\n",
    "decade = decade.astype(str)+'s'\n",
    "#name the object to label the row in the results table\n",
    "decade.name = 'decade'\n",
    "\n",
    "print(type(decade))\n",
    "print(decade)\n",
    "\n",
    "OP\n",
    "<class 'pandas.core.series.Series'>\n",
    "0       2000s\n",
    "1       2000s\n",
    "2       2010s\n",
    "3       2000s\n",
    "4       2000s\n",
    "        ...  \n",
    "1030    2000s\n",
    "1031    2000s\n",
    "1032    2000s\n",
    "1033    2000s\n",
    "1034    2000s\n",
    "Name: decade, Length: 1035, dtype: object\n",
    "\n",
    "#Create a new columnn in the dataframe with decade\n",
    "planets['Decade'] = decade\n",
    "planets.info()\n",
    "\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1035 entries, 0 to 1034\n",
    "Data columns (total 7 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   method          1035 non-null   object \n",
    " 1   number          1035 non-null   int64  \n",
    " 2   orbital_period  992 non-null    float64\n",
    " 3   mass            513 non-null    float64\n",
    " 4   distance        808 non-null    float64\n",
    " 5   year            1035 non-null   int64  \n",
    " 6   Decade          1035 non-null   object \n",
    "dtypes: float64(3), int64(2), object(2)\n",
    "memory usage: 56.7+ KB\n",
    "\n",
    "--------------------------\n",
    "\n",
    "#Use groupby to answer the question\n",
    "planets.groupby(['method','Decade'])['number'].sum().unstack().fillna(0)\n",
    "\n",
    "\n",
    "Decade\t                        1980s\t1990s\t2000s\t2010s\n",
    "method\t\t\t\t\n",
    "Astrometry\t                         0.0\t0.0\t0.0\t2.0\n",
    "Eclipse Timing Variations\t        0.0\t0.0\t5.0\t10.0\n",
    "Imaging\t                   0.0\t0.0\t29.0\t21.0\n",
    "Microlensing\t0.0\t0.0\t12.0\t15.0\n",
    "Orbital Brightness Modulation\t0.0\t0.0\t0.0\t5.0\n",
    "Pulsar Timing\t0.0\t9.0\t1.0\t1.0\n",
    "Pulsation Timing Variations\t0.0\t0.0\t1.0\t0.0\n",
    "Radial Velocity\t1.0\t52.0\t475.0\t424.0\n",
    "Transit\t0.0\t0.0\t64.0\t712.0\n",
    "Transit Timing Variations\t0.0\t0.0\t0.0\t9.0\n",
    "----------------------------------------------------\n",
    "\n",
    "#Use pivottables instead of groupby and compare the code\n",
    "planets.pivot_table('number', index='method', columns='Decade', aggfunc='sum')\n",
    "\n",
    "------------------------------------------------\n",
    "Decade\t1980s\t1990s\t2000s\t2010s\n",
    "method\t\t\t\t\n",
    "Astrometry\tNaN\tNaN\tNaN\t2.0\n",
    "Eclipse Timing Variations\tNaN\tNaN\t5.0\t10.0\n",
    "Imaging\tNaN\tNaN\t29.0\t21.0\n",
    "Microlensing\tNaN\tNaN\t12.0\t15.0\n",
    "Orbital Brightness Modulation\tNaN\tNaN\tNaN\t5.0\n",
    "Pulsar Timing\tNaN\t9.0\t1.0\t1.0\n",
    "Pulsation Timing Variations\tNaN\tNaN\t1.0\tNaN\n",
    "Radial Velocity\t1.0\t52.0\t475.0\t424.0\n",
    "Transit\tNaN\tNaN\t64.0\t712.0\n",
    "Transit Timing Variations\tNaN\tNaN\tNaN\t9.0\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#keep only the last decade and remove the other years from the dataset\n",
    "#Define a list of years to keep\n",
    "Years_Keep =[2010, 2011, 2012, 2013, 2014]\n",
    "\n",
    "#Create a copy of the data with only the 2010s\n",
    "rdf = planets[planets.year.isin(Years_Keep)].copy()\n",
    "rdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "method\tnumber\torbital_period\tmass\tdistance\tyear\tDecade\n",
    "2\tRadial Velocity\t1\t763.000000\t2.60\t19.84\t2011\t2010s\n",
    "9\tRadial Velocity\t2\t452.800000\t1.99\t74.79\t2010\t2010s\n",
    "10\tRadial Velocity\t2\t883.000000\t0.86\t74.79\t2010\t2010s\n",
    "20\tRadial Velocity\t5\t0.736540\tNaN\t12.53\t2011\t2010s\n",
    "26\tRadial Velocity\t1\t691.900000\tNaN\t81.50\t2012\t2010s\n",
    "...\t...\t...\t...\t...\t...\t...\t...\n",
    "1025\tTransit\t1\t3.067850\tNaN\t60.00\t2012\t2010s\n",
    "1026\tTransit\t1\t0.925542\tNaN\t470.00\t2014\t2010s\n",
    "1027\tImaging\t1\tNaN\tNaN\t19.20\t2011\t2010s\n",
    "1028\tTransit\t1\t3.352057\tNaN\t3200.00\t2012\t2010s\n",
    "1029\tImaging\t1\tNaN\tNaN\t10.10\t2012\t2010s\n",
    "597 rows × 7 columns\n",
    "------------------------------------------------\n",
    "\n",
    "#Another way of keeping only the 2010s is to use the newly created\n",
    "#Decade column and filter usingn that column\n",
    "rdf1=planets[planets.Decade =='2010s'].copy()\n",
    "rdf1\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "method\tnumber\torbital_period\tmass\tdistance\tyear\tDecade\n",
    "2\tRadial Velocity\t1\t763.000000\t2.60\t19.84\t2011\t2010s\n",
    "9\tRadial Velocity\t2\t452.800000\t1.99\t74.79\t2010\t2010s\n",
    "10\tRadial Velocity\t2\t883.000000\t0.86\t74.79\t2010\t2010s\n",
    "20\tRadial Velocity\t5\t0.736540\tNaN\t12.53\t2011\t2010s\n",
    "26\tRadial Velocity\t1\t691.900000\tNaN\t81.50\t2012\t2010s\n",
    "...\t...\t...\t...\t...\t...\t...\t...\n",
    "1025\tTransit\t1\t3.067850\tNaN\t60.00\t2012\t2010s\n",
    "1026\tTransit\t1\t0.925542\tNaN\t470.00\t2014\t2010s\n",
    "1027\tImaging\t1\tNaN\tNaN\t19.20\t2011\t2010s\n",
    "1028\tTransit\t1\t3.352057\tNaN\t3200.00\t2012\t2010s\n",
    "1029\tImaging\t1\tNaN\tNaN\t10.10\t2012\t2010s\n",
    "597 rows × 7 columns\n",
    "--------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Print dimensions of reduced dataset\n",
    "print(\"The reduced dataset has {0} rows and {1} columns\".format(rdf.shape[0], rdf.shape[1]))\n",
    "---------\n",
    "The reduced dataset has 597 rows and 7 columns\n",
    "\n",
    "-------------\n",
    "\n",
    "# checking for missing values\n",
    "print(\"Are there missing values? {}\".format(rdf.isnull().any().any()))\n",
    "\n",
    "Are there missing values? True\n",
    "\n",
    "\n",
    "planets.pivot_table('number', index='method', columns='Decade',\n",
    "                   aggfunc='sum')\n",
    "                   \n",
    "----------------------------------------------------------------\n",
    "\n",
    "Decade\t1980s\t1990s\t2000s\t2010s\n",
    "method\t\t\t\t\n",
    "Astrometry\tNaN\tNaN\tNaN\t2.0\n",
    "Eclipse Timing Variations\tNaN\tNaN\t5.0\t10.0\n",
    "Imaging\tNaN\tNaN\t29.0\t21.0\n",
    "Microlensing\tNaN\tNaN\t12.0\t15.0\n",
    "Orbital Brightness Modulation\tNaN\tNaN\tNaN\t5.0\n",
    "Pulsar Timing\tNaN\t9.0\t1.0\t1.0\n",
    "Pulsation Timing Variations\tNaN\tNaN\t1.0\tNaN\n",
    "Radial Velocity\t1.0\t52.0\t475.0\t424.0\n",
    "Transit\tNaN\tNaN\t64.0\t712.0\n",
    "Transit Timing Variations\tNaN\tNaN\tNaN\t9.0\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "titanic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.tail(5)\n",
    "\n",
    "    survived\tpclass\tsex\tage\tsibsp\tparch\tfare\tembarked\tclass\twho\tadult_male\tdeck\tembark_town\talive\talone\n",
    "886\t0\t2\tmale\t27.0\t0\t0\t13.00\tS\tSecond\tman\tTrue\tNaN\tSouthampton\tno\tTrue\n",
    "887\t1\t1\tfemale\t19.0\t0\t0\t30.00\tS\tFirst\twoman\tFalse\tB\tSouthampton\tyes\tTrue\n",
    "888\t0\t3\tfemale\tNaN\t1\t2\t23.45\tS\tThird\twoman\tFalse\tNaN\tSouthampton\tno\tFalse\n",
    "889\t1\t1\tmale\t26.0\t0\t0\t30.00\tC\tFirst\tman\tTrue\tC\tCherbourg\tyes\tTrue\n",
    "890\t0\t3\tmale\t32.0\t0\t0\t7.75\tQ\tThird\tman\tTrue\tNaN\tQueenstown\tno\tTrue\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "# getting an overview of our data\n",
    "print(\"The dataset has {0} rows and {1} columns\".format(titanic.shape[0], titanic.shape[1]))\n",
    "# checking for missing values\n",
    "print(\"Are there missing values? {}\".format(titanic.isnull().any().any()))\n",
    "# general information about column data types and number of values\n",
    "titanic.info()\n",
    "----------------------------------------------\n",
    "The dataset has 891 rows and 15 columns\n",
    "Are there missing values? True\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 891 entries, 0 to 890\n",
    "Data columns (total 15 columns):\n",
    " #   Column       Non-Null Count  Dtype   \n",
    "---  ------       --------------  -----   \n",
    " 0   survived     891 non-null    int64   \n",
    " 1   pclass       891 non-null    int64   \n",
    " 2   sex          891 non-null    object  \n",
    " 3   age          714 non-null    float64 \n",
    " 4   sibsp        891 non-null    int64   \n",
    " 5   parch        891 non-null    int64   \n",
    " 6   fare         891 non-null    float64 \n",
    " 7   embarked     889 non-null    object  \n",
    " 8   class        891 non-null    category\n",
    " 9   who          891 non-null    object  \n",
    " 10  adult_male   891 non-null    bool    \n",
    " 11  deck         203 non-null    category\n",
    " 12  embark_town  889 non-null    object  \n",
    " 13  alive        891 non-null    object  \n",
    " 14  alone        891 non-null    bool    \n",
    "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
    "memory usage: 80.7+ KB\n",
    "----------------------------------------------------------------\n",
    "# pivot tables by hand with aggregate\n",
    "titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()\n",
    "\n",
    "----------------------------------------------------\n",
    "class\tFirst\tSecond\tThird\n",
    "sex\t\t\t\n",
    "female\t0.968085\t0.921053\t0.500000\n",
    "male\t0.368852\t0.157407\t0.135447\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#Q1: What is the percentage of people who survived by gender and class?\n",
    "\n",
    "titanic.pivot_table('survived', index='sex', columns='class')\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "class\tFirst\tSecond\tThird\n",
    "sex\t\t\t\n",
    "female\t0.968085\t0.921053\t0.500000\n",
    "male\t0.368852\t0.157407\t0.135447\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "#Q2: In absolute values, how many people survived per gender and class?\n",
    "table= titanic.pivot_table('survived', index='sex', columns='class', aggfunc='sum')\n",
    "table\n",
    "-------------------------------------\n",
    "class\tFirst\tSecond\tThird\n",
    "sex\t\t\t\n",
    "female\t91\t70\t72\n",
    "male\t45\t17\t47\n",
    "----------------------------------------\n",
    "titanic.pivot_table('survived', index='sex', columns='class', margins=True)\n",
    "\n",
    "class\tFirst\tSecond\tThird\tAll\n",
    "sex\t\t\t\t\n",
    "female\t0.968085\t0.921053\t0.500000\t0.742038\n",
    "male\t0.368852\t0.157407\t0.135447\t0.188908\n",
    "All\t0.629630\t0.472826\t0.242363\t    0.383838\n",
    " -----------------------------------------------------------------------\n",
    " \n",
    " #Show table above graphically in a bar chart\n",
    "table.plot(kind='bar')\n",
    "graph PDF\n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "# is there an easy way to calculate a crosstabulation with frequencies?\n",
    "pd.crosstab(index=titanic['sex'], columns=titanic['class'], margins=True)\n",
    "\n",
    "\n",
    "class\tFirst\tSecond\tThird\tAll\n",
    "sex\t\t\t\t\n",
    "female\t94\t76\t144\t314\n",
    "male\t122\t108\t347\t577\n",
    "All\t    216\t184\t491\t891\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "#Q4: How many survived and how much they paid on average?\n",
    "titanic.pivot_table(index='sex', columns = 'class', aggfunc = {'survived':sum, 'fare':'mean'})\n",
    "\n",
    "          fare                    \t   survived\n",
    "class\tFirst\tSecond\tThird\tFirst\tSecond\tThird\n",
    "sex\t\t\t\t\t\t\n",
    "female\t106.125798\t21.970121\t16.118810\t91\t70\t72\n",
    "male\t67.226127\t19.741782\t12.661633\t45\t17\t47\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "877d6416",
   "metadata": {},
   "source": [
    "LC 20: \n",
    "\n",
    "#Ex.1. \n",
    "import pandas as pd\n",
    "\n",
    "#Create a dataframe\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "\n",
    "#simple GroupBy\n",
    "#Operations on groupby\n",
    "#Which type of object created by GroupBy?\n",
    "\n",
    "\n",
    "#Ex.2. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#initialization of random seed in rng to use in randint\n",
    "rng = np.random.RandomState(0)\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0, 10, 6)},\n",
    "                    columns = ['key', 'data1', 'data2'])\n",
    "df\n",
    "\n",
    "key\tdata1\tdata2\n",
    "0\tA\t0\t5\n",
    "1\tB\t1\t0\n",
    "2\tC\t2\t3\n",
    "3\tA\t3\t3\n",
    "4\tB\t4\t7\n",
    "5\tC\t5\t9\n",
    "\n",
    "\n",
    "#Aggregation\n",
    "# aggregation can take a string, a function, or a list thereof\n",
    "# and compute all the aggregates at once\n",
    "df.groupby('key').aggregate(['min', np.median, max])\n",
    "\n",
    "       data1\t     data2\n",
    "    min\tmedian\tmax\tmin\tmedian\tmax\n",
    "key\t\t\t\t\t\t\n",
    "A\t0\t1.5\t3\t3\t4.0\t5\n",
    "B\t1\t2.5\t4\t0\t3.5\t7\n",
    "C\t2\t3.5\t5\t3\t6.0\t9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0ba45d3",
   "metadata": {},
   "source": [
    "# aggregate can also recieve and pass a dictionary mapping column\n",
    "# names to operations to be applied on that column\n",
    "df.groupby('key').aggregate({'data1':'min',\n",
    "                              'data2':'max'})\n",
    " \n",
    "OUTPUT\n",
    "data1\tdata2\n",
    "key\t\t\n",
    "A\t0\t5\n",
    "B\t1\t7\n",
    "C\t2\t9\n",
    "______________________\n",
    "\n",
    "\n",
    "#Filtering\n",
    "def filter_func(x):\n",
    "    return x['data2'].std() > 4\n",
    "\n",
    "print(df); print(df.groupby('key').std());\n",
    "print(df.groupby('key').filter(filter_func)) # filter based on defined function\n",
    "\n",
    "OP___________________________________\n",
    "key  data1  data2\n",
    "0   A      0      5\n",
    "1   B      1      0\n",
    "2   C      2      3\n",
    "3   A      3      3\n",
    "4   B      4      7\n",
    "5   C      5      9\n",
    "       data1     data2\n",
    "key                   \n",
    "A    2.12132  1.414214\n",
    "B    2.12132  4.949747\n",
    "C    2.12132  4.242641\n",
    "  key  data1  data2\n",
    "1   B      1      0\n",
    "2   C      2      3\n",
    "4   B      4      7\n",
    "5   C      5      9\n",
    "\n",
    "--------------------------------------------\n",
    "#Transformation\n",
    "def trans_func(x):\n",
    "    return x - x.mean()\n",
    "df.groupby('key').transform(trans_func)\n",
    "\n",
    "data1\tdata2\n",
    "0\t-1.5\t1.0\n",
    "1\t-1.5\t-3.5\n",
    "2\t-1.5\t-3.0\n",
    "3\t1.5\t-1.0\n",
    "4\t1.5\t3.5\n",
    "5\t1.5\t3.0\n",
    "----------------------------\n",
    "#Apply\n",
    "def norm_by_data2(x):\n",
    "    # x is a DataFrame of group values\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "print(df)\n",
    "print(df.groupby('key').apply(norm_by_data2))\n",
    "\n",
    "# WHAT THE HELL IS GOING ON ?? dont worry\n",
    "# a in data2 zeroth potion 5 , 3rd position 3 = 8\n",
    "# b in data2 1st position 0 , 4th position 7 = 7\n",
    "# c 2nd position 3 , 5th position 9 = .sum() = 12\n",
    "# 0/8 , 1/7 , 2/12 ; 3/8 , 4/7 , 5/12\n",
    "# A     B      C       A    B     C\n",
    "\n",
    " key  data1  data2\n",
    "0   A      0      5\n",
    "1   B      1      0\n",
    "2   C      2      3\n",
    "3   A      3      3\n",
    "4   B      4      7\n",
    "5   C      5      9\n",
    "  key     data1  data2\n",
    "0   A  0.000000      5\n",
    "1   B  0.142857      0\n",
    "2   C  0.166667      3\n",
    "3   A  0.375000      3\n",
    "4   B  0.571429      7\n",
    "5   C  0.416667      9\n",
    "\n",
    "\n",
    "print(x['data2'].sum())\n",
    "name 'x' is not defined\n",
    "\n",
    "\n",
    "Groupby 'method' and orbital period\n",
    "planets.groupby('method')\n",
    "\n",
    "planets.groupby('method')['orbital_period']\n",
    "\n",
    "planets.groupby('method')['orbital_period'].median()\n",
    "\n",
    "\n",
    "method\n",
    "Astrometry                         631.180000\n",
    "Eclipse Timing Variations         4343.500000\n",
    "Imaging                          27500.000000\n",
    "Microlensing                      3300.000000\n",
    "Orbital Brightness Modulation        0.342887\n",
    "Pulsar Timing                       66.541900\n",
    "Pulsation Timing Variations       1170.000000\n",
    "Radial Velocity                    360.200000\n",
    "Transit                              5.714932\n",
    "Transit Timing Variations           57.011000\n",
    "Name: orbital_period, dtype: float64\n",
    "\n",
    "\n",
    "#Iteration over groups\n",
    "for (method, group) in planets.groupby('method'):\n",
    "    print(\"{0:30s} shape={1}\".format(method, group.shape))\n",
    "\n",
    "Astrometry                     shape=(2, 6)\n",
    "Eclipse Timing Variations      shape=(9, 6)\n",
    "Imaging                        shape=(38, 6)\n",
    "Microlensing                   shape=(23, 6)\n",
    "Orbital Brightness Modulation  shape=(3, 6)\n",
    "Pulsar Timing                  shape=(5, 6)\n",
    "Pulsation Timing Variations    shape=(1, 6)\n",
    "Radial Velocity                shape=(553, 6)\n",
    "Transit                        shape=(397, 6)\n",
    "Transit Timing Variations      shape=(4, 6)\n",
    "\n",
    "\n",
    "#Dispatch methods allows to cal pandas methods on the groups\n",
    "planets.groupby('method')['year'].describe().unstack()\n",
    "\n",
    "method                       \n",
    "count  Astrometry                          2.0\n",
    "       Eclipse Timing Variations           9.0\n",
    "       Imaging                            38.0\n",
    "       Microlensing                       23.0\n",
    "       Orbital Brightness Modulation       3.0\n",
    "                                         ...  \n",
    "max    Pulsar Timing                    2011.0\n",
    "       Pulsation Timing Variations      2007.0\n",
    "       Radial Velocity                  2014.0\n",
    "       Transit                          2014.0\n",
    "       Transit Timing Variations        2014.0\n",
    "Length: 80, dtype: float64\n",
    "\n",
    "some notes lecture 20 \n",
    "\n",
    "The split step involves breaking up and grouping a\n",
    "DataFrame depending on the value of the specified\n",
    "key.\n",
    "• The apply step involves computing some function,\n",
    "usually an aggregate,\n",
    "transformation, or filtering,\n",
    "within the individual\n",
    "groups.\n",
    "• The combine step merges the results of these\n",
    "operations into an output\n",
    "array.\n",
    "• These steps can be done\n",
    "manually with masking,\n",
    "aggregation, and merge but\n",
    "are more efficiently done\n",
    "by using the parameters of\n",
    "GroupBy.\n",
    "• With GroupBy, the\n",
    "intermediate steps do not\n",
    "need to be specified.\n",
    "\n",
    "When you use the GroupBy operation in a\n",
    "DataFrame, Python returns a GroupBy object\n",
    "which is an abstraction, or a special view of the\n",
    "DataFrame.\n",
    "• This view is not a set of DataFrames, but it works as if it\n",
    "was a collection of them. To calculate values, an\n",
    "aggregation operation is needed.\n",
    "• The DataFrame Group-By object is a flexible\n",
    "abstraction that supports any valid DataFrame\n",
    "operation.\n",
    "• Iteration over groups\n",
    "• Dispatch methods\n",
    "Source: VanderPlas, 2017 (Chp. V3, pp. 161-177)\n",
    "We will see examples of these\n",
    "In the Planets dataset exercise\n",
    "\n",
    "• Aggregation. The aggregate() method allows for more flexibility\n",
    "than GroupBy aggregations with sum(), median(), and so on. It can\n",
    "take a string, a function, or a list thereof, and compute all the\n",
    "aggregates at once.\n",
    "• Filtering. A filtering operation allows you to drop data based on the\n",
    "group properties. For example, keep all groups in which the\n",
    "standard deviation is larger than some critical value.\n",
    "• Transformation. It is used to return some transformed version of\n",
    "the full data to recombine. For such a transformation, the output is\n",
    "the same shape as the input. An example of this would be to center\n",
    "the data by subtracting the group-wise mean\n",
    "• Apply. The apply() method lets you apply an arbitrary function to\n",
    "the group results. The function takes a DataFrame and returns\n",
    "either a Pandas object or a scalar\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e0c1ed3",
   "metadata": {},
   "source": [
    "LC 19: \n",
    "# Source: https://www.geeksforgeeks.org/fetching-top-news-using-news-api/\n",
    "# BBC news api with authorization header and parameters\n",
    "\n",
    "import requests \n",
    "headers = {'Authorization': 'dc8619a4d2ae434393e9fd15f8b6c094'}\n",
    "query_params = {\n",
    "      \"source\": \"bbc-news\",\n",
    "      \"sortBy\": \"top\"\n",
    "}\n",
    "main_url = \" https://newsapi.org/v1/articles\"\n",
    " \n",
    "# fetching data in json format\n",
    "res = requests.get(main_url, headers=headers, params=query_params)\n",
    "bbc_res = res.json()\n",
    " \n",
    "# getting all articles in a string article\n",
    "article = bbc_res[\"articles\"]\n",
    " \n",
    "# empty list to hold all trending news\n",
    "results = []\n",
    "     \n",
    "for ar in article:\n",
    "    results.append(ar[\"title\"])\n",
    "\n",
    "# printing all trending news       \n",
    "for i in range(len(results)):            \n",
    "    print(i + 1, results[i])\n",
    "\n",
    "OP_________________________________________________________\n",
    "1 Seven dead in shooting at Buffalo supermarket - reports\n",
    "2 Mayor speaks of pain in wake of Buffalo shooting\n",
    "3 Abortion rights rallies held across US cities\n",
    "4 Her father went to prison - so she went to law school\n",
    "5 Eurovision 2022: Ukraine wins, while the UK's Sam Ryder comes second\n",
    "6 Britney Spears announces 'devastating' miscarriage\n",
    "7 Full lunar eclipse to bring super blood Moon\n",
    "8 Ukraine war: Putin warns Finland joining Nato would be 'mistake'\n",
    "9 Australia all-rounder Symonds dies in car crash\n",
    "10 Covid: What will the pandemic look like in North Korea?\n",
    "_____________________________________________________________________________\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\n",
    "print(df)\n",
    "\n",
    "   A         B         C         D\n",
    "0  0.706801  0.184470  0.230169  0.194437\n",
    "1  0.004659  0.524851  0.247930  0.442037\n",
    "2  0.453656  0.496082  0.294891  0.807396\n",
    "3  0.232726  0.148879  0.100661  0.438266\n",
    "4  0.646990  0.591890  0.597129  0.923895\n",
    "5  0.166579  0.650289  0.100622  0.945672\n",
    "6  0.561936  0.798703  0.870025  0.313548\n",
    "7  0.310525  0.837748  0.617784  0.316766\n",
    "8  0.920114  0.800027  0.536337  0.637796\n",
    "9  0.289683  0.519506  0.068978  0.668017\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "import requests\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "apiKey = '2001888'\n",
    "data_url = 'http://www.omdbapi.com/?apikey='+apiKey\n",
    "\n",
    "movieTitle = 'Fast & Furious'\n",
    "year = ''\n",
    "\n",
    "params = {\n",
    "    's':movieTitle,\n",
    "    'type':'movie',\n",
    "    'y':year    \n",
    "}\n",
    "\n",
    "response = requests.get(data_url,params=params).json()\n",
    "pp.pprint(response)\n",
    "\n",
    "\n",
    "---------------------\n",
    "'Response': 'True',\n",
    " 'Search': [{'Poster': 'https://m.media-amazon.com/images/M/MV5BMTM3NTg2NDQzOF5BMl5BanBnXkFtZTcwNjc2NzQzOQ@@._V1_SX300.jpg',\n",
    "             'Title': 'Fast & Furious 6',\n",
    "             'Type': 'movie',\n",
    "             'Year': '2013',\n",
    "             'imdbID': 'tt1905041'},\n",
    "            {'Poster': 'https://m.media-amazon.com/images/M/MV5BNzlkNzVjMDMtOTdhZC00MGE1LTkxODctMzFmMjkwZmMxZjFhXkEyXkFqcGdeQXVyNjU0OTQ0OTY@._V1_SX300.jpg',\n",
    "             'Title': 'The Fast and the Furious',\n",
    "             'Type': 'movie',\n",
    "             'Year': '2001',\n",
    "             'imdbID': 'tt0232500'},\n",
    " ------------------------------------------------\n",
    "\n",
    "Results = []\n",
    "for movie in response['Search']:\n",
    "    print(movie['Title'], movie['Year'])\n",
    "    Results.append(movie['Title'])\n",
    "------------------------------------------OP:\n",
    "Fast & Furious 6 2013\n",
    "The Fast and the Furious 2001\n",
    "Fast & Furious 2009\n",
    "2 Fast 2 Furious 2003\n",
    "The Fast and the Furious: Tokyo Drift 2006\n",
    "Fast & Furious Presents: Hobbs & Shaw 2019\n",
    "Turbo Charged Prelude to 2 Fast 2 Furious 2003\n",
    "Fast & Furious: Supercharged 2015\n",
    "The Fast and the Furious 1954\n",
    "Fast and Furious 1939\n",
    "\n",
    "______________________________________________________\n",
    "\n",
    "print('The search found: ', len(Results))\n",
    "\n",
    "The search found:  10\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "for i,movie in enumerate(response['Search']):\n",
    "    print(f\"{i+1}. {movie['Title']}, {movie['Year']}\")\n",
    "----------------------------------------------------------\n",
    "1. Fast & Furious 6, 2013\n",
    "2. The Fast and the Furious, 2001\n",
    "3. Fast & Furious, 2009\n",
    "4. 2 Fast 2 Furious, 2003\n",
    "5. The Fast and the Furious: Tokyo Drift, 2006\n",
    "6. Fast & Furious Presents: Hobbs & Shaw, 2019\n",
    "7. Turbo Charged Prelude to 2 Fast 2 Furious, 2003\n",
    "8. Fast & Furious: Supercharged, 2015\n",
    "9. The Fast and the Furious, 1954\n",
    "10. Fast and Furious, 1939\n",
    "--------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(Results)\n",
    "print(df)\n",
    "-----------------------op\n",
    "                                    0\n",
    "0                           Fast & Furious 6\n",
    "1                   The Fast and the Furious\n",
    "2                             Fast & Furious\n",
    "3                           2 Fast 2 Furious\n",
    "4      The Fast and the Furious: Tokyo Drift\n",
    "5      Fast & Furious Presents: Hobbs & Shaw\n",
    "6  Turbo Charged Prelude to 2 Fast 2 Furious\n",
    "7               Fast & Furious: Supercharged\n",
    "8                   The Fast and the Furious\n",
    "9                           Fast and Furious\n",
    "----------------------------------------------------\n",
    "\n",
    "some notes LC 19: \n",
    "With respect to the site load imposed by the API:\n",
    "• Rate Limits restrict the amount of data a user can\n",
    "request over a period of time. In general, streaming APIs\n",
    "provide constant real-time data, while REST API limit the\n",
    "number of requests per period.\n",
    "• Limit restrictions seek to balance the site load and\n",
    "maintain acceptable response times for all users.\n",
    "• Some sites establish levels of the data volume\n",
    "transmitted, and charge accordingly. This is known as\n",
    "tiered access.\n",
    "• Example: Twitter’s access tiers are firehose, gardenhose, and\n",
    "Spritzer and spritzer is the free API.\n",
    "\n",
    "Practical uses of API\n",
    "• For individual programmers / data scientists to\n",
    "extract information from a web source and\n",
    "analyze it\n",
    "• For companies to connect their systems\n",
    "automatically and offer enhanced services to\n",
    "customers\n",
    "• Ex: Ecommerce site connecting with courier companies\n",
    "(e.g., FedEx, UPS) for delivery tracking information\n",
    "• For app developers to connect their apps to other\n",
    "systems to enhance the functionality or usability\n",
    "• Ex: Social login through Facebook or Google in game apps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a286b00d",
   "metadata": {},
   "source": [
    "LC 18:\n",
    "#Ex.1\n",
    "#Review of Web Scraping: Hacker News\n",
    "#Source: Adapted from Broucke & Baessen (Chp. B9)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Articles is an list that will hold info about each article\n",
    "articles = []\n",
    "\n",
    "url = 'https://news.ycombinator.com/news'\n",
    "r = requests.get(url)\n",
    "print(r) # all this gives us is a reponse probability of being\n",
    "# on exam is extremely high <Response [200]>\n",
    "\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "for item in html_soup.find_all('tr', class_='athing'):\n",
    "    item_a = item.find('a', class_='titlelink')\n",
    "    item_link = item_a.get('href') if item_a else None\n",
    "    item_text = item_a.get_text(strip=True) if item_a else None\n",
    "    next_row = item.find_next_sibling('tr')\n",
    "    item_score = next_row.find('span', class_='score')\n",
    "    item_score = item_score.get_text(strip=True) if item_score else '0 points'\n",
    "#Append the article information as a dictionary to the list\n",
    "    articles.append({\n",
    "        'link' : item_link,\n",
    "        'title' : item_text,\n",
    "        'score' : item_score})\n",
    "OP_________________\n",
    "<Response [200]>\n",
    "------------------------\n",
    "#print each article\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    \n",
    "op------------------------\n",
    "\n",
    "{'link': 'https://aeon.co/ideas/what-i-learned-as-a-hired-consultant-for-autodidact-physicists', 'title': 'What I learned as a hired consultant to autodidact physicists (2016)', 'score': '181 points'}\n",
    "{'link': 'https://jacobian.org/2022/apr/4/exit-interviews-are-a-trap/', 'title': 'Exit Interviews Are a Trap', 'score': '106 points'}\n",
    "{'link': 'https://grumpygamer.com/return_to_monkey_island', 'title': 'Return to Monkey Island', 'score': '92 points'}\n",
    "{'link': 'https://haleynahman.substack.com/p/95-are-you-baby-a-litmus-test', 'title': 'Are you a baby? A litmus test', 'score': '219 points'}\n",
    "{'link': 'https://code.jsoftware.com/wiki/Jd/Overview', 'title': 'Jd', 'score': '866 points'}\n",
    "{'link': 'https://spectrum.ieee.org/single-chip-processors-have-reached-their-limits', 'title': 'Single-Chip Processors Have Reached Their Limits', 'score': '43 points'}\n",
    "-------------------------------------------------------\n",
    "#Ex. 2\n",
    "#Using the Hacker News API\n",
    "#Source: Broucke & Baessen (Chp. B9)\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://hacker-news.firebaseio.com/v0'\n",
    "top_stories = requests.get(url + '/topstories.json?orderBy=\"$key\"').json()\n",
    "\n",
    "#print(top_stories)\n",
    "#How many stories are retrieved?\n",
    "print(len(top_stories))\n",
    "----------------\n",
    "500\n",
    "-------------------\n",
    "\n",
    "\n",
    "#Ex2-a: Customize the request to retrieve only 10 stories\n",
    "#Method 1: Extending the url to embed a query (see slides)\n",
    "#Method 2: Using a dictionary to pass the parameter \"limitToFirst\"\n",
    "\n",
    "base_url= 'https://hacker-news.firebaseio.com/v0/topstories.json?orderBy=\"$key\"'\n",
    "tenstories = {'limitToFirst':10}\n",
    "response= requests.get(base_url, params=tenstories)\n",
    "top_stories_10=response.json()\n",
    "print(top_stories_10)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "\n",
    "[31389797, 31390506, 31389893, 31388277, 31389647, 31390111, 31390564, 31387693, 31390220, 31382024]\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "# or directly into the URL \n",
    "url10 = 'https://hacker-news.firebaseio.com/v0/topstories.json?orderBy=\"$key\"&limitToFirst=10'\n",
    "ten_top_stories = requests.get(url10).json()\n",
    "print(ten_top_stories)\n",
    "---------------------------------------------------\n",
    "[31389797, 31390506, 31389893, 31388277, 31389647, 31390111, 31390564, 31387693, 31390220, 31382024]\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Ex2-b: Retrieve the URL of each story (needs another requests)\n",
    "aarticles=[]\n",
    "for story_id in top_stories_10:\n",
    "    story_url = url + '/item/{}.json'.format(story_id)\n",
    "    print('Fetching:', story_url)\n",
    "    r = requests.get(story_url)\n",
    "    story_dict = r.json()\n",
    "    aarticles.append(story_dict)\n",
    "\n",
    "#print the title of each article\n",
    "for aarticle in aarticles:\n",
    "    print(aarticle['title'], ',' , aarticle['score'])\n",
    "    #print(aarticle)\n",
    "-----------------------------------------------------------\n",
    "\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31389797.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31390506.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31389893.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31388277.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31389647.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31390111.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31390564.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31387693.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31390220.json\n",
    "Fetching: https://hacker-news.firebaseio.com/v0/item/31382024.json\n",
    "Total Eclipse of the Moon: 2022 May 16 , 108\n",
    "Fly.io: The Reclaimer of Heroku's Magic , 36\n",
    "The weird Hewlett Packard FreeDOS option , 63\n",
    "Japanese audio brand Onkyo files for bankruptcy , 173\n",
    "The problem with Bitcoin miners , 163\n",
    "Sleep Helps Process Emotions , 19\n",
    "Breaking into the black box of artificial intelligence , 8\n",
    "Sound Blaster (DOS) , 61\n",
    "Soupault 4.0.0: as extensible as Jekyll, still statically linked , 13\n",
    "Introduction to Microsoft Excel (1992) [video] , 38\n",
    "----------------------------------------------------------------\n",
    "for i, aarticle in enumerate(aarticles):\n",
    "    print(i+1, '->', aarticle['title'], ',' , aarticle['score'])\n",
    "--------------------------------------------------------------------\n",
    "1 -> What I learned as a hired consultant to autodidact physicists (2016) , 214\n",
    "2 -> Exit Interviews Are a Trap , 148\n",
    "3 -> Return to Monkey Island , 128\n",
    "4 -> Are you a baby? A litmus test , 236\n",
    "5 -> Jd , 882\n",
    "6 -> Single-Chip Processors Have Reached Their Limits , 55\n",
    "7 -> Tinygrad , 33\n",
    "8 -> Why don't more languages offer flow typing? , 107\n",
    "9 -> A blue connector does not USB 3.0 make , 137\n",
    "10 -> Jitx (YC S18) is hiring senior developers to automate circuit board design , 1\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "#Ex. 3\n",
    "#Code below has been modified from the original provided at the following website:\n",
    "#https://towardsdatascience.com/a-beginners-guide-to-accessing-data-with-web-apis-using-python-23d262181467\n",
    "import requests, json\n",
    "payload = {\n",
    "    'limit': 12,\n",
    "    't' : 'all'\n",
    "}\n",
    "headers = {\n",
    "    'User-agent': 'Reddit bot 1.0'\n",
    "}\n",
    "#The original example uses the /r/funny subreddit as endpoint\n",
    "#endpoint = 'https://www.reddit.com/r/funny/top.json'\n",
    "\n",
    "endpoint = 'https://www.reddit.com/r/news/top.json'\n",
    "r = requests.get(endpoint, headers = headers, params = payload)\n",
    "rj = json.loads(r.content)\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(r)\n",
    "\n",
    "print(type(rj['data']['children']))\n",
    "\n",
    "ls_data_children = rj['data']['children']\n",
    "\n",
    "#for sub in (r['data']['children']):\n",
    "for sub in (ls_data_children):\n",
    "    title = sub['data']['title']\n",
    "    print(title)\n",
    "-----------------\n",
    "<class 'list'>\n",
    "Joe Biden elected president of the United States\n",
    "Chauvin found guilty of murder, manslaughter in George Floyd's death\n",
    "President Donald Trump says he has tested positive for coronavirus\n",
    "Blizzard Employees Staged a Walkout After the Company Banned a Gamer for Pro-Hong Kong Views\n",
    "Trump has left the White House for the last time as President\n",
    "Kobe Bryant killed in helicopter crash in California\n",
    "'Jeopardy' host Alex Trebek dies at 80 due to pancreatic cancer\n",
    "Scientist Stephen Hawking has died aged 76\n",
    "Jeffrey Epstein's autopsy more consistent with homicidal strangulation than suicide, Dr. Michael Baden reveals\n",
    "F.C.C. Announces Plan to Repeal Net Neutrality\n",
    "---------------------------------\n",
    "\n",
    "\n",
    "notes: \n",
    "\n",
    "To make the API requests more specific, use headers\n",
    "and parameters in the request\n",
    "• Headers\n",
    "• Parameters are like filters to modify the scope of the\n",
    "request. \n",
    "\n",
    "• News API is a simple HTTP REST API for searching\n",
    "and retrieving live articles from various sources\n",
    "• It has several endpoints\n",
    "• /v2/top-headlines\n",
    "• /v2/sources\n",
    "• /v2/everything\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52d0c3ef",
   "metadata": {},
   "source": [
    "LC 17: handling json\n",
    "\n",
    "#Ex. 0\n",
    "#Handling json files\n",
    "#Upload json file and update path accordingly\n",
    "\n",
    "import json\n",
    "json_data = open('data-text.json').read()\n",
    "#print(json_data)\n",
    "data = json.loads(json_data)\n",
    "print(type(data))\n",
    "for item in data:\n",
    "    print(item)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "========================\n",
    "<class 'list'>\n",
    "{'Indicator': 'Life expectancy at birth (years)', 'PUBLISH STATES': 'Published', 'Year': 1990, 'WHO region': 'Europe', 'World Bank income group': 'High-income', 'Country': 'Andorra', 'Sex': 'Both sexes', 'Display Value': 77, 'Numeric': 77.0, 'Low': '', 'High': '', 'Comments': ''}\n",
    "{'Indicator': 'Life expectancy at birth (years)', 'PUBLISH STATES': 'Published', 'Year': 2000, 'WHO region': 'Europe', 'World Bank income group': 'High-income', 'Country': 'Andorra', 'Sex': 'Both sexes', 'Display Value': 80, 'Numeric': 80.0, 'Low': '', 'High': '', 'Comments': ''}\n",
    "{'Indicator': 'Life expectancy at age 60 (years)', 'PUBLISH STATES': 'Published', 'Year': 2012, 'WHO region': 'Europe', 'World Bank income group': 'High-income', 'Country': 'Andorra', 'Sex': 'Female', 'Display Value': 28, 'Numeric': 28.0, 'Low': '', 'High': '', 'Comments': ''}\n",
    "{'Indicator': 'Life expectancy at age 60 (years)', 'PUBLISH STATES': 'Published', 'Year': 2000, 'WHO region': 'Europe', 'World Bank income group': 'High-income', 'Country': 'Andorra', 'Sex': 'Both sexes', 'Display Value': 23, 'Numeric': 23.0, 'Low': '', 'High': '', 'Comments': ''}\n",
    "{'Indicator': 'Life expectancy at birth (years)', 'PUBLISH STATES': 'Published', 'Year': 2012, 'WHO region': 'Eastern Mediterranean', 'World Bank income group': 'High-income', 'Country': 'United Arab Emirates', 'Sex': 'Female', 'Display Value': 78, 'Numeric': 78.0, 'Low': '', 'High': '', 'Comments': ''}\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "#Ex. 1\n",
    "#From https://medium.com/quick-code/absolute-beginners-guide-to-slaying-apis-using-python-7b380dc82236\n",
    "import requests\n",
    "#request = requests.get('http://api.open-notify.org')\n",
    "#print(request.text)\n",
    "\n",
    "people = requests.get('http://api.open-notify.org/astros.json')\n",
    "print(people.text)\n",
    "\n",
    "--------------------------\n",
    "{\"people\": [{\"craft\": \"ISS\", \"name\": \"Oleg Artemyev\"}, {\"craft\": \"ISS\", \"name\": \"Denis Matveev\"}, {\"craft\": \"ISS\", \"name\": \"Sergey Korsakov\"}, {\"craft\": \"ISS\", \"name\": \"Kjell Lindgren\"}, {\"craft\": \"ISS\", \"name\": \"Bob Hines\"}, {\"craft\": \"ISS\", \"name\": \"Samantha Cristoforetti\"}, {\"craft\": \"ISS\", \"name\": \"Jessica Watkins\"}], \"message\": \"success\", \"number\": 7}\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "people_json  = people.json()\n",
    "#Similar to people.text\n",
    "#print(people_json)\n",
    "\n",
    "#To print the  number of people in space\n",
    "#print(\"Number of people in space:\",people_json['number'])\n",
    "\n",
    "#To print the names of people in space using a for loop\n",
    "for p in people_json['people']:\n",
    "    print(p['name'], 'is in: ', p['craft'])\n",
    "    \n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Oleg Artemyev is in:  ISS\n",
    "Denis Matveev is in:  ISS\n",
    "Sergey Korsakov is in:  ISS\n",
    "Kjell Lindgren is in:  ISS\n",
    "Bob Hines is in:  ISS\n",
    "Samantha Cristoforetti is in:  ISS\n",
    "Jessica Watkins is in:  ISS\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "#Ex.2\n",
    "#Adapted from: http://open-notify.org/Open-Notify-API/ISS-Location-Now/\n",
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "print(response.text)\n",
    "pos = response.json()\n",
    "print('Timestamp is: ', pos['timestamp'])\n",
    "print('Latitude is: ', pos['iss_position']['latitude'])\n",
    "print('Longitude is: ', pos['iss_position']['longitude'])\n",
    "---------------------------------------------------------------\n",
    "\n",
    "{\"iss_position\": {\"latitude\": \"-35.9427\", \"longitude\": \"-158.2486\"}, \"timestamp\": 1648670326, \"message\": \"success\"}\n",
    "Timestamp is:  1648670326\n",
    "Latitude is:  -35.9427\n",
    "Longitude is:  -158.2486\n",
    "--------------------------------------------------------------\n",
    "\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.fromtimestamp(int(pos['timestamp'])))\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "#Ex.3\n",
    "#Note: Latitude and longitude can be directly added to the end point as shown below\n",
    "#response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74\")\n",
    "#In this example, we will work with parameters and use the latitude and longitude of New York\n",
    "base_url = 'http://api.open-notify.org/'\n",
    "newyork = {'lat': 40.71, 'lon': -74 }\n",
    "response = requests.get(base_url + 'iss-pass.json', params=newyork)\n",
    "r= response.json()\n",
    "print(r)\n",
    "\n",
    "------------------------------------------------------------------\n",
    "\n",
    "{'message': 'success', 'request': {'altitude': 100, 'datetime': 1652638504, 'latitude': 40.71, 'longitude': -74.0, 'passes': 5}, 'response': [{'duration': 616, 'risetime': 1652660181}, {'duration': 640, 'risetime': 1652665970}, {'duration': 569, 'risetime': 1652671854}, {'duration': 573, 'risetime': 1652677722}, {'duration': 642, 'risetime': 1652683534}]}\n",
    "\n",
    "--------------------------------------------------------------\n",
    "\n",
    "import datetime\n",
    "for p in r['response']:\n",
    "    #convert from UNIX time\n",
    "    date = datetime.datetime.fromtimestamp(int(p['risetime']))\n",
    "    print(date)\n",
    "-------------------------------\n",
    "\n",
    "2022-03-30 16:22:46\n",
    "2022-03-30 18:00:53\n",
    "2022-03-30 19:38:33\n",
    "2022-03-30 21:15:20\n",
    "2022-03-30 22:52:23\n",
    "--------------------------------\n",
    "\n",
    "\n",
    "some notes: \n",
    "\n",
    "Options to change the number of columns and\n",
    "rows to display in Pandas\n",
    "• The pd.set_option changes the “…” to actual column or\n",
    "row names depending on the case. The default is 20\n",
    "• To remove scrolling, you may need to adjust the width\n",
    "\n",
    "JSON Files\n",
    "• JSON data is one of the most commonly used formats for data transfers because it\n",
    "is clean, easy to read, and easy to parse • The acronym means JavaScriptObject\n",
    "Notation\n",
    "• If you open the JSON file in your code\n",
    "editor, you will see that each data record\n",
    "looks like a Python dictionary. • There is a key and value for each row,\n",
    "separated by a :, with each entry separated\n",
    "by a ,.\n",
    "• There are also opening and closing curly\n",
    "braces ({}).\n",
    "• To handle json files, you need to import\n",
    "the Python json library • Note the json library loads function expects a\n",
    "string, not a file. In contrast, the Python csv\n",
    "library’s reader function expects an open file.\n",
    "\n",
    "JSON Methods\n",
    "• Parsing: • Json.load reads the json document from file and converts it to a dictionary. • Use json.loads (with s) if the json file is presented as a string • Serialization (convert to json): • Json.dump is used to write Python serialized object as JSON formatted data\n",
    "into a file.\n",
    "• Json.dump\n",
    "s (with s) encodes any Python object into JSON formatted String.\n",
    "• Pretty Printing: • means to display JSON in an easy\n",
    "-to\n",
    "-read\n",
    "(for human readers) format using\n",
    "indentation, key\n",
    "-value separators, and whitespace.\n",
    "• Import pprint module and method and use\n",
    "pprint\n",
    "\n",
    "APIs usually return JSON, XML, or CSV files\n",
    "\n",
    "To make use of web APIs, we have to make a ‘request’ through the HTTP protocol.\n",
    "• Similar to web scraping but using an API in this case\n",
    "• The request has to be directed to a certain location, or\n",
    "‘endpoint’\n",
    "• The endpoint is determined by what we are trying to do, or what\n",
    "information we are requesting.\n",
    "• As we make the request, we can pass along a collection of\n",
    "parameters to increase the specificity of the request\n",
    "• Headers can also be passed for more specificity\n",
    "• The server returns a response code\n",
    "• 200 if successful or another code if there was a problem\n",
    "• The server will then (hopefully) provide a response\n",
    "• Response is usually in JSON form\n",
    "A Rest API request is placed through the requests library, using .get but the URL corresponds to an API service\n",
    "• API Response codes:\n",
    "• 200: Everything went okay, and the result has been returned (if any).\n",
    "• 301: The server is redirecting you to a different endpoint. Can happen when a company switches domain names, or endpoint name changes.\n",
    "• 400: The server thinks you made a bad request. Can happen when you do not send along the right data, among other things.\n",
    "• 401: The server thinks you are not authenticated. Many APIs require login credentials, so this happens when you don’t send the right\n",
    "credentials to access an API.\n",
    "• 403: The resource you are trying to access is forbidden. You do not have the right permissions to see it.\n",
    "• 404: The resource you tried to access was not found on the server.\n",
    "• 503: The server is not ready to handle the request.\n",
    "• You should always consult the API documentation to ensure a\n",
    "successful request\n",
    "\n",
    "Without authentication\n",
    "• Open Notify API is used to access NASA data serves\n",
    "several endpoints and is very easy to use\n",
    "• Without Parameters\n",
    "• /astros.json returns the number (and names) of astronauts in the\n",
    "space (See Ex. 1)\n",
    "• /iss-now.json for the exact location of the space station right now\n",
    "returns timestamp, latitude and longitude (See Ex. 2)\n",
    "• With Parameters\n",
    "• /iss-pass.json returns the time at which the space station would\n",
    "do an overhead pass over a specific location defined by its\n",
    "latitude and longitude (See Ex. 3)\n",
    "\n",
    "• Usually, an API endpoint requires parameters. This\n",
    "would be explained in the API documentation\n",
    "• Ex. the http://api.open-notify.org/iss-pass.json endpoint tells us the\n",
    "next times that the international space station will pass over a given\n",
    "location on the earth.\n",
    "• It requires the lat (latitude) and long (longitude) parameters of the\n",
    "location of interest (e.g., New York City)\n",
    "• The coordinates for New York City (40. 71, -74) can be included in a\n",
    "dictionary and passed using the params argument."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e4b285f",
   "metadata": {},
   "source": [
    "LC 16:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Function to generate a synthetic dataframe\n",
    "def make_df(cols, ind):\n",
    "    dd = {} #Empty dictionary to store everything\n",
    "    for c in cols:\n",
    "        dlis =[] #Empty list to store values\n",
    "        for i in ind:         \n",
    "            val = str(c)+str(i) #creates each value\n",
    "            dlis.append(val)\n",
    "#            print(c, i, val) #printout flag to check\n",
    "            dd[c]=dlis #fills out the column\n",
    "#    print('this is dd: ', dd)\n",
    "#    print('this is ind: ', ind)\n",
    "    return pd.DataFrame(dd, ind) #converts dict and return\n",
    "NO OP \n",
    "\n",
    "# Calling the function to create a DataFrame\n",
    "make_df('XYZ', range(4)) \n",
    "\n",
    "------------\n",
    "\n",
    "    X\tY\tZ\n",
    "0\tX0\tY0\tZ0\n",
    "1\tX1\tY1\tZ1\n",
    "2\tX2\tY2\tZ2\n",
    "3\tX3\tY3\tZ3\n",
    "--------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"Simplification of make_df function in a single line \"\"\"\n",
    "def fmake_df(cols, ind):\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "fmake_df('AND', range(7))\n",
    "\n",
    "------------------------\n",
    "\n",
    "     A\tN\tD\n",
    "0\tA0\tN0\tD0\n",
    "1\tA1\tN1\tD1\n",
    "2\tA2\tN2\tD2\n",
    "3\tA3\tN3\tD3\n",
    "4\tA4\tN4\tD4\n",
    "5\tA5\tN5\tD5\n",
    "6\tA6\tN6\tD6\n",
    "--------------------------------\n",
    "\n",
    "Pandas Concat\n",
    "\n",
    "________________________-\n",
    "#Ex. 1: Pandas Concatenation\n",
    "\n",
    "#concatenate two dataframes with same columns\n",
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "\n",
    "print(\"df1 = \\n\", df1); print(\"df2 = \\n\", df2); \n",
    "print(\"df1 concatenated with df2 = \\n\", pd.concat([df1, df2])) \n",
    "#                                                           , axis = 1 for column concat (not part of code)\n",
    "-----------------------------\n",
    "\n",
    "df1 = \n",
    "     A   B\n",
    "1  A1  B1\n",
    "2  A2  B2\n",
    "df2 = \n",
    "     A   B\n",
    "3  A3  B3\n",
    "4  A4  B4\n",
    "df1 concatenated with df2 = \n",
    "     A   B\n",
    "1  A1  B1\n",
    "2  A2  B2\n",
    "3  A3  B3\n",
    "4  A4  B4\n",
    "------------------------\n",
    "#concatenate two dataframes with same columns\n",
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "print(\"df3 = \\n\", df3); print(\"df4 = \\n\", df4); \n",
    "print(\"df3 concatenated with df4 = \\n\", pd.concat([df3, df4], axis=1))\n",
    "----------------------------------------------------------\n",
    "df3 = \n",
    "     A   B\n",
    "0  A0  B0\n",
    "1  A1  B1\n",
    "df4 = \n",
    "     C   D\n",
    "0  C0  D0\n",
    "1  C1  D1\n",
    "df3 concatenated with df4 = \n",
    "     A   B   C   D\n",
    "0  A0  B0  C0  D0\n",
    "1  A1  B1  C1  D1\n",
    "-----------------------------\n",
    "\n",
    "#Ex. 2\n",
    "#Concatenating dataframes with only a few columns in common\n",
    "#two options: Inner Join and Outer join (default)\n",
    "\n",
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "print(\"df5 = \\n\",df5); print(\"df6 = \\n\",df6); \n",
    "print(\"df5 concatenated with df6 = \\n\", pd.concat([df5, df6]))\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "df5 = \n",
    "     A   B   C\n",
    "1  A1  B1  C1\n",
    "2  A2  B2  C2\n",
    "df6 = \n",
    "     B   C   D\n",
    "3  B3  C3  D3\n",
    "4  B4  C4  D4\n",
    "df5 concatenated with df6 = \n",
    "      A   B   C    D\n",
    "1   A1  B1  C1  NaN\n",
    "2   A2  B2  C2  NaN\n",
    "3  NaN  B3  C3   D3\n",
    "4  NaN  B4  C4   D4\n",
    "----------------------------------\n",
    "\n",
    "#inner join - only columns in common will survive the concatenation\n",
    "print(\"df5 = \\n\",df5); print(\"df6 = \\n\",df6); \n",
    "print(\"df5 concatenated with df6 = \\n\", pd.concat([df5, df6], join='inner'))\n",
    "\n",
    "-------------------------------------------------------------\n",
    "\n",
    "df5 = \n",
    "     A   B   C\n",
    "1  A1  B1  C1\n",
    "2  A2  B2  C2\n",
    "df6 = \n",
    "     B   C   D\n",
    "3  B3  C3  D3\n",
    "4  B4  C4  D4\n",
    "df5 concatenated with df6 = \n",
    "     B   C\n",
    "1  B1  C1\n",
    "2  B2  C2\n",
    "3  B3  C3\n",
    "4  B4  C4\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "PANDAS MERGE\n",
    "\n",
    "#Ex. 3\n",
    "#one-to-one join\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "               \n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "print(\"df1 = \\n\", df1); print(\"df2 = \\n\", df2)\n",
    "\n",
    "df3 = pd.merge(df1, df2)\n",
    "print(\"df1 merged with df2 = \\n\",df3)\n",
    "\n",
    "df1 = \n",
    "   employee        group\n",
    "0      Bob   Accounting\n",
    "1     Jake  Engineering\n",
    "2     Lisa  Engineering\n",
    "3      Sue           HR\n",
    "df2 = \n",
    "   employee  hire_date\n",
    "0     Lisa       2004\n",
    "1      Bob       2008\n",
    "2     Jake       2012\n",
    "3      Sue       2014\n",
    "df1 merged with df2 = \n",
    "   employee        group  hire_date\n",
    "0      Bob   Accounting       2008\n",
    "1     Jake  Engineering       2012\n",
    "2     Lisa  Engineering       2004\n",
    "3      Sue           HR       2014\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "#Many-to-one join\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "print(\"df3 = \\n\", df3); print(\"df4 = \\n\", df4)\n",
    "print(\"df3 merged with df4 = \\n\", pd.merge(df3, df4))\n",
    "\n",
    "---------------------------------\n",
    "df3 = \n",
    "   employee        group  hire_date\n",
    "0      Bob   Accounting       2008\n",
    "1     Jake  Engineering       2012\n",
    "2     Lisa  Engineering       2004\n",
    "3      Sue           HR       2014\n",
    "df4 = \n",
    "          group supervisor\n",
    "0   Accounting      Carly\n",
    "1  Engineering      Guido\n",
    "2           HR      Steve\n",
    "df3 merged with df4 = \n",
    "  \n",
    "  \n",
    "  employee        group  hire_date supervisor\n",
    "0      Bob   Accounting       2008      Carly\n",
    "1     Jake  Engineering       2012      Guido\n",
    "2     Lisa  Engineering       2004      Guido\n",
    "3      Sue           HR       2014      Steve\n",
    "\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "\n",
    "#Many-to-many join\n",
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "print(\"df1 = \\n\", df1); print(\"df5 = \\n\", df5); \n",
    "print(\"df1 merged with df5 = \\n\",pd.merge(df1, df5))\n",
    "-----------------------------------------------------------\n",
    "\n",
    "df1 = \n",
    "   employee        group\n",
    "0      Bob   Accounting\n",
    "1     Jake  Engineering\n",
    "2     Lisa  Engineering\n",
    "3      Sue           HR\n",
    "df5 = \n",
    "          group        skills\n",
    "0   Accounting          math\n",
    "1   Accounting  spreadsheets\n",
    "2  Engineering        coding\n",
    "3  Engineering         linux\n",
    "4           HR  spreadsheets\n",
    "5           HR  organization\n",
    "df1 merged with df5 = \n",
    "   employee        group        skills\n",
    "0      Bob   Accounting          math\n",
    "1      Bob   Accounting  spreadsheets\n",
    "2     Jake  Engineering        coding\n",
    "3     Jake  Engineering         linux\n",
    "4     Lisa  Engineering        coding\n",
    "5     Lisa  Engineering         linux\n",
    "6      Sue           HR  spreadsheets\n",
    "7      Sue           HR  organization\n",
    "-------------------------------------------\n",
    "\n",
    "#Ex. 4\n",
    "#Use of the on keyword\n",
    "print(\"df1 = \\n\", df1); print(\"df2 = \\n\", df2); \n",
    "print(\"Merge on Employee\\n\", pd.merge(df1, df2, on='employee'))\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "f1 = \n",
    "   employee        group\n",
    "0      Bob   Accounting\n",
    "1     Jake  Engineering\n",
    "2     Lisa  Engineering\n",
    "3      Sue           HR\n",
    "df2 = \n",
    "   employee  hire_date\n",
    "0     Lisa       2004\n",
    "1      Bob       2008\n",
    "2     Jake       2012\n",
    "3      Sue       2014\n",
    "Merge on Employee\n",
    "   employee        group  hire_date\n",
    "0      Bob   Accounting       2008\n",
    "1     Jake  Engineering       2012\n",
    "2     Lisa  Engineering       2004\n",
    "3      Sue           HR       2014\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\n",
    "#Left_on and right_on keywords\n",
    "df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "print(\"df1 = \\n\", df1); print(\"df3 = \\n\", df3);\n",
    "print(\"Merge with left_on right_on\\n\", pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\"))\n",
    "-------------------------------------------------------------------------------------------------\n",
    "\n",
    "df1 = \n",
    "   employee        group\n",
    "0      Bob   Accounting\n",
    "1     Jake  Engineering\n",
    "2     Lisa  Engineering\n",
    "3      Sue           HR\n",
    "df3 = \n",
    "    name  salary\n",
    "0   Bob   70000\n",
    "1  Jake   80000\n",
    "2  Lisa  120000\n",
    "3   Sue   90000\n",
    "Merge with left_on right_on\n",
    "   employee        group  name  salary\n",
    "0      Bob   Accounting   Bob   70000\n",
    "1     Jake  Engineering  Jake   80000\n",
    "2     Lisa  Engineering  Lisa  120000\n",
    "3      Sue           HR   Sue   90000\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "print(\"Merge with left_on right_on and dropping dup column\\n\")\n",
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "Merge with left_on right_on and dropping dup column\n",
    "\n",
    "\n",
    "employee\tgroup\tsalary\n",
    "0\tBob\tAccounting\t70000\n",
    "1\tJake\tEngineering\t80000\n",
    "2\tLisa\tEngineering\t120000\n",
    "3\tSue\tHR\t90000\n",
    "--------------\n",
    "#Ex.5\n",
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                    columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "\n",
    "print(\"df6 = \\n\", df6); print(\"df7 = \\n\", df7); \n",
    "print(\"Merge of df6 with df7\\n\", pd.merge(df6, df7))\n",
    "\n",
    "#inner join\n",
    "print(\"Merge of df6 with df7 with inner join\\n\", pd.merge(df6, df7, how='inner'))\n",
    "\n",
    "#outer join\n",
    "print(\"Merge of df6 with df7 with outer join\\n\", pd.merge(df6, df7, how='outer'))\n",
    "\n",
    "#left join\n",
    "print(\"Merge of df6 with df7 with left join\\n\", pd.merge(df6, df7, how='left'))\n",
    "#right join\n",
    "print(\"Merge of df6 with df7 with right join\\n\", pd.merge(df6, df7, how='right'))\n",
    "---------------------------\n",
    "\n",
    "df6 = \n",
    "     name   food\n",
    "0  Peter   fish\n",
    "1   Paul  beans\n",
    "2   Mary  bread\n",
    "df7 = \n",
    "      name drink\n",
    "0    Mary  wine\n",
    "1  Joseph  beer\n",
    "Merge of df6 with df7\n",
    "    name   food drink\n",
    "0  Mary  bread  wine\n",
    "Merge of df6 with df7 with inner join\n",
    "    name   food drink\n",
    "0  Mary  bread  wine\n",
    "Merge of df6 with df7 with outer join\n",
    "      name   food drink\n",
    "0   Peter   fish   NaN\n",
    "1    Paul  beans   NaN\n",
    "2    Mary  bread  wine\n",
    "3  Joseph    NaN  beer\n",
    "Merge of df6 with df7 with left join\n",
    "     name   food drink\n",
    "0  Peter   fish   NaN\n",
    "1   Paul  beans   NaN\n",
    "2   Mary  bread  wine\n",
    "Merge of df6 with df7 with right join\n",
    "      name   food drink\n",
    "0    Mary  bread  wine\n",
    "1  Joseph    NaN  beer\n",
    "-----------------------------------\n",
    "print(\"Merge of df6 with df7 with right join\\n\", pd.merge(df6, df7, how='right'))\n",
    "\n",
    "Merge of df6 with df7 with right join\n",
    "      name   food drink\n",
    "0    Mary  bread  wine\n",
    "1  Joseph    NaN  beer\n",
    "\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "data = '{\"row 1\": {\"col 1\": \"a\", \"col 2\": \"b\"}, \"row 2\":{\"col 1\": \"c\", \"col 2\": \"d\"}}'\n",
    "df = pd.read_json(data, orient='index')\n",
    "df\n",
    "        \n",
    "------------------------\n",
    "\n",
    "\tcol 1\tcol 2\n",
    "row 1\ta\tb\n",
    "row 2\tc\td\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "# import pandas as pd\n",
    "d = '{\"columns\":[\"col 1\", \"col 2\"], \"index\":[\"row 1\", \"row 2\"], \"data\":[[\"a\",\"b\"], [\"c\", \"d\"]]}'\n",
    "df = pd.read_json(d, orient = 'split')\n",
    "df\n",
    "--------------------------------\n",
    "\n",
    "# The parameter orient is used to align index and columns\n",
    "# for dataframe, the default is columns whereas other allowed values\n",
    "#split, record, index, values, and table\n",
    "import pandas as pd\n",
    "data = '[{\"col 1\":\"a\", \"col 2\":\"b\"},{\"col 1\":\"c\", \"col 2\": \"d\"}]'\n",
    "df = pd.read_json(data, orient = 'records')\n",
    "df\n",
    "---------------------------\n",
    "\n",
    "col 1\tcol 2\n",
    "0\ta\tb\n",
    "1\tc\td\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "# The string could be a file handle or a URL to a json file. \n",
    "import pandas as pd\n",
    "df = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')\n",
    "df\n",
    "----------------------------------------------------------------\n",
    "url\trepository_url\tlabels_url\tcomments_url\tevents_url\thtml_url\tid\tnode_id\tnumber\ttitle\t...\tupdated_at\tclosed_at\tauthor_association\tactive_lock_reason\tdraft\tpull_request\tbody\treactions\ttimeline_url\tperformed_via_github_app\n",
    "0\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://github.com/pandas-dev/pandas/pull/47032\t1236352230\tPR_kwDOAA0YD8431mtS\t47032\tCI: Start Testing on Python 3.11\t...\t2022-05-15 17:57:30+00:00\tNaT\tMEMBER\tNaN\t1.0\t{'url': 'https://api.github.com/repos/pandas-d...\t- [ ] xref #46680 (Replace xxxx with the Githu...\t{'url': 'https://api.github.com/repos/pandas-d...\thttps://api.github.com/repos/pandas-dev/pandas...\tNaN\n",
    "1\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://github.com/pandas-dev/pandas/issues/47030\t1236274176\tI_kwDOAA0YD85JsAwA\t47030\tDOC: DataFrame.rename Method\t...\t2022-05-15 14:41:00+00:00\tNaT\tNONE\tNaN\tNaN\tNaN\t### Pandas version checks\\n\\n- [x] I have chec...\t{'url': 'https://api.github.com/repos/pandas-d...\thttps://api.github.com/repos/pandas-dev/pandas...\tNaN\n",
    "2\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://github.com/pandas-dev/pandas/pull/47028\t1236240934\tPR_kwDOAA0YD8431R2u\t47028\tFix: Pandas rolling removes imaginary part of ...\t...\t2022-05-15 15:35:49+00:00\tNaT\tCONTRIBUTOR\tNaN\t0.0\t{'url': 'https://api.github.com/repos/pandas-d...\t- [ ] closes #46619\\r\\n- [ ] [Tests added and ...\t{'url': 'https://api.github.com/repos/pandas-d...\thttps://api.github.com/repos/pandas-dev/pandas...\tNaN\n",
    "3\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://github.com/pandas-dev/pandas/pull/47027\t1236168064\tPR_kwDOAA0YD8431EPP\t47027\tWIP: ENH Add float[pyarrow] dtype\t...\t2022-05-15 15:47:56+00:00\tNaT\tCONTRIBUTOR\tNaN\t1.0\t{'url': 'https://api.github.com/repos/pandas-d...\t- Basing this off https://github.com/pandas-de...\t{'url': 'https://api.github.com/repos/pandas-d...\thttps://api.github.com/repos/pandas-dev/pandas...\tNaN\n",
    "4\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://api.github.com/repos/pandas-dev/pandas...\thttps://github.com/pandas-dev/pandas/pull/47026\t1236162331\tPR_kwDO\n",
    "\n",
    "---------------------------\n",
    "\n",
    "some notes: \n",
    "Similar to concatenation of NumPy arrays via\n",
    "np.concatenate\n",
    "• Arguments is list of arrays to concatenate and axis\n",
    "• Pandas function, pd.concat(), has a similar syntax\n",
    "• By default, the concatenation takes place row-wise\n",
    "within the DataFrame (i.e., axis=0)\n",
    "• Difference between np.concatenate and pd.concat\n",
    "is that Pandas concatenation preserves indices,\n",
    "even if the result will have duplicate indices\n",
    "\n",
    "\n",
    "• pd.concat offers several options to join datasets\n",
    "with different column names\n",
    "• By default, the entries for which no data is available are\n",
    "filled with NA values.\n",
    "• To change default option, specify one of several options\n",
    "for the join parameters\n",
    "• the join default is a union of the input columns (join=\n",
    "'outer'), but it can be changed to the intersection of the\n",
    "columns using join='inner’\n",
    "\n",
    "\n",
    "• The pd.merge() looks for one or more\n",
    "matching column names between the two\n",
    "inputs, and uses this as the key\n",
    "• It implements different types of joins\n",
    "depending on the form of the input data.\n",
    "• One-to-one join is in similar to column-wise\n",
    "concatenation\n",
    "• Many-to-one joins are joins in which one of the\n",
    "two key columns contains duplicate entries.\n",
    "The resulting DataFrame will preserve those\n",
    "duplicate entries as appropriate.\n",
    "• Many-to-many joins happen if the key column\n",
    "in both the left and right array contains\n",
    "duplicates\n",
    "\n",
    "To specify the key, use the on keyword\n",
    "• This only works if the left and right DataFrame have the specified column name\n",
    "• To merge two datasets with different column names, use\n",
    "the left_on and right_on keywords to specify the two column names\n",
    "• The result has a redundant column that we can drop if desired\n",
    "using the drop() method\n",
    "• To merge on an index, instead of merging on a column, use the right_index and left_index. • For convenience, DataFrames implement the join() method, which performs a merge that defaults to joining on indices\n",
    "• To mix indices and columns, you can combine on and index\n",
    "• For example, left_index with right_on or left_on with right_index to\n",
    "get the desired behavior\n",
    "\n",
    "\n",
    "What happens when a value appears in one key\n",
    "column but not the other?\n",
    "• By default, the result contains the intersection of\n",
    "the two sets of inputs, known as an inner join\n",
    "• This can be specified explicitly using the how keyword\n",
    "(which defaults to 'inner’)\n",
    "• An outer join returns a join over the union of the\n",
    "input columns, and fills missing values with NAs\n",
    "• The left join and right join return join over the left\n",
    "entries and right entries, respectively\n",
    "\n",
    "pd.read_json (data, orient,…)\n",
    "• is used to Convert a VALID JSON\n",
    "string to a Pandas object.\n",
    "• The parameter orient is used to\n",
    "align index and columns.\n",
    "• For DataFrame, the default is columns\n",
    "whereas other allowed values are\n",
    "split, record, index, values and table.\n",
    "• The string could be a file handle or\n",
    "a URL to a json file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c6db9c1",
   "metadata": {},
   "source": [
    "LC 15:\n",
    "#Ex1: Pandas DataFrames\n",
    "#Source: https://www.dataquest.io/blog/pandas-python-tutorial/\n",
    "\n",
    "import pandas as pd\n",
    "reviews = pd.read_csv(\"ign.csv\")\n",
    "\n",
    "reviews.head()\n",
    "Unnamed: 0\tscore_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "0\t0\tAmazing\tLittleBigPlanet PS Vita\t/games/littlebigplanet-vita/vita-98907\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "1\t1\tAmazing\tLittleBigPlanet PS Vita -- Marvel Super Hero E...\t/games/littlebigplanet-ps-vita-marvel-super-he...\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "2\t2\tGreat\tSplice: Tree of Life\t/games/splice/ipad-141070\tiPad\t8.5\tPuzzle\tN\t2012\t9\t12\n",
    "3\t3\tGreat\tNHL 13\t/games/nhl-13/xbox-360-128182\tXbox 360\t8.5\tSports\tN\t2012\t9\t11\n",
    "4\t4\tGreat\tNHL 13\t/games/nhl-13/ps3-128181\tPlayStation 3\t8.5\tSports\tN\t2012\t9\t11\n",
    "\n",
    "\n",
    "print(type(reviews))\n",
    "\n",
    "--------------------------------------\n",
    "<class 'pandas.core.frame.DataFrame'\n",
    "-----------------------------------\n",
    "#removing the first column (all rows :, column 1 onwards 1:)\n",
    "reviews = reviews.iloc[:,1:]\n",
    "reviews.head()\n",
    "-------------------------\n",
    "\tscore_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "0\tAmazing\tLittleBigPlanet PS Vita\t/games/littlebigplanet-vita/vita-98907\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "1\tAmazing\tLittleBigPlanet PS Vita -- Marvel Super Hero E...\t/games/littlebigplanet-ps-vita-marvel-super-he...\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "------------------------------\n",
    "\n",
    "print('This is the result of reviews.shape: ', reviews.shape)\n",
    "---------------------------------------------------------\n",
    "\n",
    "This is the result of reviews.shape:  (18625, 10)\n",
    "--------------------------------------------------------\n",
    "reviews.info()\n",
    "-----------\n",
    "\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 18625 entries, 0 to 18624\n",
    "Data columns (total 10 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   score_phrase    18625 non-null  object \n",
    " 1   title           18625 non-null  object \n",
    " 2   url             18625 non-null  object \n",
    " 3   platform        18625 non-null  object \n",
    " 4   score           18625 non-null  float64\n",
    " 5   genre           18589 non-null  object \n",
    " 6   editors_choice  18625 non-null  object \n",
    " 7   release_year    18625 non-null  int64  \n",
    " 8   release_month   18625 non-null  int64  \n",
    " 9   release_day     18625 non-null  int64  \n",
    "dtypes: float64(1), int64(3), object(6)\n",
    "memory usage: 1.4+ MB\n",
    "---------------------------\n",
    "\n",
    "reviews['score'].describe()\n",
    "\n",
    "count    18625.000000\n",
    "mean         6.950459\n",
    "std          1.711736\n",
    "min          0.500000\n",
    "25%          6.000000\n",
    "50%          7.300000\n",
    "75%          8.200000\n",
    "max         10.000000\n",
    "Name: score, dtype: float64\n",
    "-----------------------\n",
    "reviews['score_phrase'].describe()\n",
    "----------------------------------\n",
    "\n",
    "count     18625\n",
    "unique       11\n",
    "top       Great\n",
    "freq       4773\n",
    "Name: score_phrase, dtype: object\n",
    "-----------------------------------\n",
    "\n",
    "#Sort by platform in descending order\n",
    "reviews.sort_values(\"platform\", ascending = False)\n",
    "------------------------------------------------------\n",
    "score_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "8321\tGreat\tZuma\t/games/zuma/ipod-855895\tiPod\t8.0\tPuzzle\tN\t2006\t9\t15\n",
    "10367\tGood\tBomberman\t/games/bomberman/ipod-14225887\tiPod\t7.9\tAction\tN\t2008\t1\t2\n",
    "10472\tMediocre\tPole Position Remix\t/games/pole-position-remix/ipod-14230189\tiPod\t5.0\tRacing\tN\t2008\t1\t29\n",
    "10907\tGood\tBubble Bash\t/games/bubble-bash/ipod-14237917\tiPod\t7.2\tPuzzle\tN\t2008\t3\t25\n",
    "10681\tGreat\tNaval Battle\t/games/naval-battle/ipod-14234843\tiPod\t8.5\tStrategy\tY\t2008\t2\t26\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "reviews.head(10)\n",
    "-----------\n",
    "score_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "0\tAmazing\tLittleBigPlanet PS Vita\t/games/littlebigplanet-vita/vita-98907\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "1\tAmazing\tLittleBigPlanet PS Vita -- Marvel Super Hero E...\t/games/littlebigplanet-ps-vita-marvel-super-he...\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "2\tGreat\tSplice: Tree of Life\t/games/splice/ipad-141070\tiPad\t8.5\tPuzzle\tN\t2012\t9\t12\n",
    "3\tGreat\tNHL 13\t/games/nhl-13/xbox-360-128182\tXbox 360\t8.5\tSports\tN\t2012\t9\t11\n",
    "4\tGreat\tNHL 13\t/games/nhl-13/ps3-128181\tPlayStation 3\t8.5\tSports\tN\t2012\t9\t11\n",
    "5\tGood\tTotal War Battles: Shogun\t/games/total-war-battles-shogun/mac-142565\tMacintosh\t7.0\tStrategy\tN\t2012\t9\t11\n",
    "6\tAwful\tDouble Dragon: Neon\t/games/double-dragon-neon/xbox-360-131320\tXbox 360\t3.0\tFighting\tN\t2012\t9\t11\n",
    "7\tAmazing\tGuild Wars 2\t/games/guild-wars-2/pc-896298\tPC\t9.0\tRPG\tY\t2012\t9\t11\n",
    "8\tAwful\tDouble Dragon: Neon\t/games/double-dragon-neon/ps3-131321\tPlayStation 3\t3.0\tFighting\tN\t2012\t9\t11\n",
    "9\tGood\tTotal War Battles: Shogun\t/games/total-war-battles-shogun/pc-142564\tPC\t7.0\tStrategy\tN\t2012\t9\t11\n",
    "----------------------\n",
    "#Filter by Editors choice \n",
    "eds_choice = reviews[\"editors_choice\"] == 'Y'\n",
    "reviews[eds_choice].head()\n",
    "-----------------------------\n",
    "score_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "0\tAmazing\tLittleBigPlanet PS Vita\t/games/littlebigplanet-vita/vita-98907\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "1\tAmazing\tLittleBigPlanet PS Vita -- Marvel Super Hero E...\t/games/littlebigplanet-ps-vita-marvel-super-he...\tPlayStation Vita\t9.0\tPlatformer\tY\t2012\t9\t12\n",
    "7\tAmazing\tGuild Wars 2\t/games/guild-wars-2/pc-896298\tPC\t9.0\tRPG\tY\t2012\t\n",
    "----------------------------------\n",
    "#Show titles chosen by editors with scores less than 9\n",
    "lo_score = reviews[\"score\"] < 9.0\n",
    "reviews[eds_choice & lo_score].head()\n",
    "----------------------------------------\n",
    "\n",
    "score_phrase\ttitle\turl\tplatform\tscore\tgenre\teditors_choice\trelease_year\trelease_month\trelease_day\n",
    "31\tGreat\tWorld of Warcraft: Mists of Pandaria\t/games/world-of-warcraft-mists-of-pandaria/pc-...\tPC\t8.7\tRPG\tY\t2012\t10\t4\n",
    "40\tGreat\tRock Band Blitz\t/games/rock-band-blitz/xbox-360-131273\tXbox 360\t8.5\tMusic\tY\t2012\t8\t27\n",
    "68\tGreat\tProfessor Layton and the Miracle Mask\t/games/professor-layton-and-the-mask-of-miracl...\tNintendo 3DS\t8.7\tAdventure\tY\t2012\t10\t26\n",
    "------------------------------------------------\n",
    "\n",
    "#Count the titles launched each year\n",
    "reviews[\"release_year\"].value_counts()\n",
    "-----------------------------------------\n",
    "2008    1915\n",
    "2009    1687\n",
    "2007    1610\n",
    "2010    1363\n",
    "2006    1208\n",
    "        ... \n",
    "1998     339\n",
    "2016     226\n",
    "1997     205\n",
    "1996     149\n",
    "1970       1\n",
    "Name: release_year, Length: 22, dtype: int64\n",
    "------------------------------------------------\n",
    "#Compute max and min of scores\n",
    "print('max is:',reviews[\"score\"].max())\n",
    "print('min is:', reviews[\"score\"].min())\n",
    "-----------------------------------------\n",
    "max is: 10.0\n",
    "min is: 0.5\n",
    "---------------\n",
    "\n",
    "\n",
    "#Compute max, min, average and standard deviation of scores\n",
    "print('Maximum score is: ', reviews.score.max())\n",
    "print('Minimum score is: ', reviews.score.min())\n",
    "print('Average score is: ', reviews.score.mean())\n",
    "print('Stdev of score is: ', reviews.score.std())\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "Maximum score is:  10.0\n",
    "Minimum score is:  0.5\n",
    "Average score is:  6.950459060402666\n",
    "Stdev of score is:  1.7117358608045874\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "NOTES: \n",
    ".head() — prints the first N rows, where N is a number\n",
    "passed as an argument to the function\n",
    "• If you don’t pass any argument, the default is 5.\n",
    "• .tail() — prints the last N rows of a dataframe\n",
    "• The default is also 5.\n",
    "• .info() prints information about a DataFrame including\n",
    "the index dtype and column dtypes, non-null values\n",
    "and memory usage\n",
    "• .describe() gets a summary of the numeric values in the\n",
    "dataset\n",
    "• Note: .shape is a property that shows how many rows\n",
    "and columns are there in the dataset\n",
    "\n",
    "sort_values() method sorts the column, regardless of\n",
    "whether it is string or numeric.\n",
    "• If the column is string, it will be sorted in alphabetical\n",
    "order.\n",
    "• The default sorting approach is ascending. To reverse\n",
    "it, specify ascending=False as a parameter.\n",
    "• The sort does not modify the existing DataFrame. If\n",
    "you need to save the sorting version there are two\n",
    "options:\n",
    "1. Assign the sorted DataFrame to another variable\n",
    "2. Use the inplace argument and set it to true\n",
    "\n",
    "\n",
    "Filtering is also known as masking\n",
    "• Filter under one condition requires setting the\n",
    "condition inside square brackets.\n",
    "• If the comparison is equal to, do not forget to use the\n",
    "double equal sign (==)\n",
    "• If the comparison is not equal to, use ‘!=‘\n",
    "• Filter under more than one condition is done with\n",
    "the Boolean operators (‘and’ ‘or’) depending on\n",
    "the case\n",
    "• The comparisons can be assigned to variables for\n",
    "more clarity\n",
    "\n",
    "Pandas use sentinels for missing data, and two already-existing\n",
    "Python null values: NaN and None.\n",
    "• None: Pythonic missing data\n",
    "The first sentinel value used by Pandas is None, a Python singleton object\n",
    "that is often used for missing data in Python code. Because None is a\n",
    "Python object, it cannot be used in any arbitrary NumPy/Pandas array,\n",
    "but only in arrays with data type 'object' (i.e., arrays of Python objects):\n",
    "• NaN: Missing numerical data\n",
    "The other missing data representation, NaN (acronym for Not a Number),\n",
    "is different; it is a special floating-point value recognized by all systems\n",
    "that use the standard IEEE floating-point representation\n",
    "• NaN and None in Pandas\n",
    "Pandas is built to handle the two of them nearly interchangeably,\n",
    "converting between them where appropriate\n",
    "\n",
    "\n",
    "Checking for missing values using isnull() and notnull()\n",
    "• Both functions help in checking whether a value is NaN or not. These\n",
    "functions can also be used in Pandas Series in order to find null values\n",
    "in a series.\n",
    "• Filling missing values using fillna(), replace() and interpolate()\n",
    "• In order to fill null values in a datasets, we could use fillna(), replace()\n",
    "and interpolate() functions to replace NaN values with some value of\n",
    "their own.\n",
    "• Interpolate() function is basically used to fill NA values in the dataframe\n",
    "but it uses various interpolation techniques to fill the missing values\n",
    "rather than hard-coding the value.\n",
    "• Dropping missing values using dropna()\n",
    "• In order to drop a null values from a dataframe, we used dropna()\n",
    "• This function drops Rows/Columns of datasets with Null values in\n",
    "different ways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae7ab85e",
   "metadata": {},
   "source": [
    "LC 14:\n",
    "\n",
    "#Ex.1: Pandas Series Application and Indexing\n",
    "#Source: https://medium.com/analytics-vidhya/pandas-tutorials-advanced-indexing-b35909b26bcd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "prices = [10.70, 10.80, 10.50, 10.90]\n",
    "shares = pd.Series(prices)\n",
    "print('This is shares:')\n",
    "print(shares)\n",
    "\n",
    "days = ['Mon','Tue','Wed','Thu']\n",
    "shares = pd.Series(prices, index=days)\n",
    "print('This is shares:')\n",
    "print(shares)\n",
    "print('This is shares.index:')\n",
    "print(shares.index)\n",
    "\n",
    "----------------------\n",
    "This is shares:\n",
    "0    10.7\n",
    "1    10.8\n",
    "2    10.5\n",
    "3    10.9\n",
    "dtype: float64\n",
    "This is shares:\n",
    "Mon    10.7\n",
    "Tue    10.8\n",
    "Wed    10.5\n",
    "Thu    10.9\n",
    "dtype: float64\n",
    "This is shares.index:\n",
    "Index(['Mon', 'Tue', 'Wed', 'Thu'], dtype='object')\n",
    "------------------------\n",
    "\n",
    "#accessing the index object - do not confuse this with accessing the data in the series\n",
    "print('This is shares.index[1]: ', shares.index[1])\n",
    "#This is different from\n",
    "print('This is shares[1]: ', shares[1])\n",
    "print('This is shares[Tue]: ', shares['Tue'])\n",
    "print('This is shares.index[ :2]: ', shares.index[ :2])\n",
    "print('This is shares.index[-2:]: ', shares.index[ -2:])\n",
    "\n",
    "---------------------\n",
    "\n",
    "This is shares.index[1]:  Tue\n",
    "This is shares[1]:  10.8\n",
    "This is shares[Tue]:  10.8\n",
    "This is shares.index[ :2]:  Index(['Mon', 'Tue'], dtype='object')\n",
    "This is shares.index[-2:]:  Index(['Wed', 'Thu'], dtype='object')\n",
    "----------------------------------------------------------\n",
    "\n",
    "#accessing data with implicit and explicit index\n",
    "print('This is shares[1]: ',shares[1])\n",
    "print('This is shares[Tue]', shares['Tue'])\n",
    "----------------------------------\n",
    "\n",
    "This is shares[1]:  10.8\n",
    "This is shares[Tue] 10.8\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "#slicing data with implicit and explicit index\n",
    "print('This is shares[1:3] \\n', shares[1:3])\n",
    "print('This is shares[Tue:Thu]\\n', shares['Tue':'Thu'])\n",
    "--------------------------\n",
    "\n",
    "This is shares[1:3] \n",
    " Tue    10.8\n",
    "Wed    10.5\n",
    "dtype: float64\n",
    "This is shares[Tue:Thu]\n",
    " Tue    10.8\n",
    "Wed    10.5\n",
    "Thu    10.9\n",
    "dtype: float64\n",
    "\n",
    "----------------------\n",
    "\n",
    "#accessing data with loc and iloc methods\n",
    "print('This is shares.iloc[0]: ',shares.iloc[0])\n",
    "print('This is shares.loc[Mon]', shares.loc['Mon'])\n",
    "--------------------------------------------------------\n",
    "\n",
    "This is shares.iloc[0]:  10.7\n",
    "This is shares.loc[Mon] 10.7\n",
    "---------------------------------\n",
    "\n",
    "#slicing date with loc and iloc methods\n",
    "print('This is shares.iloc[1:3] \\n ',shares.iloc[1:3])\n",
    "print('This is shares.iloc[Tue:Thu] \\n  ',shares.loc['Tue':'Thu'])\n",
    "--------------------------------------------------------------\n",
    "This is shares.iloc[1:3] \n",
    "  Tue    10.8\n",
    "Wed    10.5\n",
    "dtype: float64\n",
    "This is shares.iloc[Tue:Thu] \n",
    "   Tue    10.8\n",
    "Wed    10.5\n",
    "Thu    10.9\n",
    "dtype: float64\n",
    "\n",
    "----------------\n",
    "\n",
    "Pandas DataFrame\n",
    "----------------\n",
    "#Ex2-1. Creating a Pandas DataFrame from a dictionary\n",
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "print('This is area :\\n')\n",
    "print(area)\n",
    "print('This is the type: ', type(area))\n",
    "-------------------------------------------\n",
    "\n",
    "This is area :\n",
    "\n",
    "California    423967\n",
    "Texas         695662\n",
    "New York      141297\n",
    "Florida       170312\n",
    "Illinois      149995\n",
    "dtype: int64\n",
    "This is the type:  <class 'pandas.core.series.Series'>\n",
    "-------------------------------------------------------------\n",
    "\n",
    "pop_dict = {'California': 123, 'Texas': 456, 'New York': 321,\n",
    "             'Florida': 654, 'Illinois': 199}\n",
    "population = pd.Series(pop_dict)\n",
    "print('This is population :\\n')\n",
    "print(population)\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "This is population :\n",
    "\n",
    "California    123\n",
    "Texas         456\n",
    "New York      321\n",
    "Florida       654\n",
    "Illinois      199\n",
    "dtype: int64\n",
    "\n",
    "--------------------\n",
    "\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "#print('This is states: \\n', states)\n",
    "#print('This is the index of states:', states.index)\n",
    "#print('This is the column of states: \\n ', states.columns)\n",
    "#print(states[:] = 5)\n",
    "\n",
    "--------------------------\n",
    "\n",
    "states\n",
    "\n",
    "            population\tarea\n",
    "California\t123\t423967\n",
    "Texas\t456\t695662\n",
    "New York\t321\t141297\n",
    "Florida\t654\t170312\n",
    "Illinois\t199\t149995\n",
    "\n",
    "-----------------------------------------\n",
    "#states_in_area = states['area']\n",
    "states_in_area = states.area\n",
    "print('This is the content of the area series in states: ')\n",
    "print(states_in_area)\n",
    "print('One specific state (NY) is: ', states['area']['New York'])\n",
    "\n",
    "--------------------------\n",
    "This is the content of the area series in states: \n",
    "California    423967\n",
    "Texas         695662\n",
    "New York      141297\n",
    "Florida       170312\n",
    "Illinois      149995\n",
    "Name: area, dtype: int64\n",
    "One specific state (NY) is:  141297\n",
    "--------------------------------------\n",
    "\n",
    "#Ex2-2. Creating a DataFrame from a single series object\n",
    "dataf2 = pd.DataFrame(population, columns=['population'])\n",
    "print('This is dataf2: \\n', dataf2)\n",
    "print('This is the type: ', type(dataf2))\n",
    "----------------------------\n",
    "\n",
    "This is dataf2: \n",
    "             population\n",
    "California         123\n",
    "Texas              456\n",
    "New York           321\n",
    "Florida            654\n",
    "Illinois           199\n",
    "This is the type:  <class 'pandas.core.frame.DataFrame'>\n",
    "---------------------------------------\n",
    "\n",
    "#Ex2-3. From a list of dicts\n",
    "data_list_of_dict = [{'a': i, 'b': 2 * i}\n",
    "          for i in range(3)]\n",
    "print('List of dict')\n",
    "print(data_list_of_dict)\n",
    "dataf3= pd.DataFrame(data_list_of_dict)\n",
    "print('This is dataf3: \\n')\n",
    "print(dataf3)\n",
    "dataf3\n",
    "--------------------------------------\n",
    "List of dict\n",
    "[{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}]\n",
    "This is dataf3: \n",
    "\n",
    "   a  b\n",
    "0  0  0\n",
    "1  1  2\n",
    "2  2  4\n",
    "a\tb\n",
    "0\t0\t0\n",
    "1\t1\t2\n",
    "2\t2\t4\n",
    "------------------------------------\n",
    "#Ex2-4. if keys are missing they will be filled out with NaN\n",
    "dataf4 = pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\n",
    "print('This is dataf4: \\n')\n",
    "print(dataf4)\n",
    "-------------------------\n",
    "This is dataf4: \n",
    "\n",
    "     a  b    c\n",
    "0  1.0  2  NaN\n",
    "1  NaN  3  4.0\n",
    "\n",
    "------------------------------\n",
    "\n",
    "#Ex2-5. from a dictionary of series objects\n",
    "dataf5 = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "print('This is dataf5: \\n')\n",
    "dataf5\n",
    "---------------------------------------\n",
    "This is dataf5: \n",
    "\n",
    "           population\tarea\n",
    "California\t123\t423967\n",
    "Texas\t456\t695662\n",
    "New York\t321\t141297\n",
    "Florida\t654\t170312\n",
    "Illinois\t199\t149995\n",
    "\n",
    "dataf5.population is dataf5['population']\n",
    "\n",
    "True\n",
    "\n",
    "dataf5.area is dataf5['area']\n",
    "\n",
    "True\n",
    "\n",
    "dataf5.loc[:'New York', :'pop']\n",
    "---------------------------------\n",
    "population\n",
    "California\t123\n",
    "Texas\t456\n",
    "New York\t321\n",
    "\n",
    "-------------------------\n",
    "\n",
    "dataf5.iloc[:3, :2] # [rows: columns] slicing\n",
    "------------------------------\n",
    "population\tarea\n",
    "California\t123\t423967\n",
    "Texas\t456\t695662\n",
    "New York\t321\t141297\n",
    "-------------------------------\n",
    "# how would we motify values\n",
    "dataf5.iloc[0,1] = 120\n",
    "dataf5\n",
    "------------------\n",
    "          population\tarea\n",
    "California\t123\t120\n",
    "Texas\t456\t695662\n",
    "New York\t321\t141297\n",
    "Florida\t654\t170312\n",
    "Illinois\t199\t149995\n",
    "\n",
    "----------------------------\n",
    "# use dictionary-style to modify the object\n",
    "dataf5['density'] = dataf5['population'] / dataf5['area']\n",
    "dataf5\n",
    "--------------------------------\n",
    "population\tarea\tdensity\n",
    "California\t123\t120\t1.025000\n",
    "Texas\t456\t695662\t0.000655\n",
    "New York\t321\t141297\t0.002272\n",
    "Florida\t654\t170312\t0.003840\n",
    "Illinois\t199\t149995\t0.00132\n",
    "-----------------------------------\n",
    "#Ex2-6. From a two-dimensional NumPy array\n",
    "dataf6 = pd.DataFrame(np.random.rand(3, 2),\n",
    "             columns=['foo', 'bar'],\n",
    "             index=['a', 'b', 'c'])\n",
    "print('This is dataf6: \\n')\n",
    "dataf6\n",
    "-------------------------------------\n",
    "foo\tbar\n",
    "a\t0.614825\t0.891953\n",
    "b\t0.007896\t0.148149\n",
    "c\t0.096747\t0.434308\n",
    "----------------------------------\n",
    "\n",
    "A=np.random.rand(3, 2)\n",
    "print(A)\n",
    "--------------\n",
    "[[0.54237599 0.30858979]\n",
    " [0.01747317 0.48165106]\n",
    " [0.01623178 0.20500659]]\n",
    " -----------------------------\n",
    " \n",
    "#Ex2-7. From a Numpy structured array\n",
    "A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\n",
    "print('This is A: ', A)\n",
    "--------------------\n",
    "This is A:  [(0, 0.) (0, 0.) (0, 0.)]\n",
    "-------------------------------\n",
    "\n",
    "dataf7 = pd.DataFrame(A)\n",
    "print('This is dataf7: \\n')\n",
    "dataf7\n",
    "---------------\n",
    "This is dataf7: \n",
    "\n",
    "    A\tB\n",
    "0\t0\t0.0\n",
    "1\t0\t0.0\n",
    "2\t0\t0.0\n",
    "-------------------------\n",
    "area = pd.Series ({'California': 423967, 'Texas': 695662,\n",
    "             'New York': 141297, 'Florida': 170312,\n",
    "            'Illinois': 149995})\n",
    "pop = pd.Series ({'California': 38332521, 'Text': 26448193,\n",
    "             'New York': 19651127, 'Florida': 19552860,\n",
    "             'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data\n",
    "--------------------------------\n",
    "\n",
    "                 area\tpop\n",
    "California\t423967.0\t38332521.0\n",
    "Florida\t170312.0\t19552860.0\n",
    "Illinois\t149995.0\t12882135.0\n",
    "New York\t141297.0\t19651127.0\n",
    "Texas\t695662.0\tNaN\n",
    "Text\tNaN\t26448193.0\n",
    "---------------------------------------\n",
    "# To access columns of the DataFrame use indexing or attribute-style access\n",
    "#(a) Access via dictionary-style indexing of the column name\n",
    "data['area']\n",
    "---------------------------------------------------\n",
    "California    423967.0\n",
    "Florida       170312.0\n",
    "Illinois      149995.0\n",
    "New York      141297.0\n",
    "Texas         695662.0\n",
    "Text               NaN\n",
    "Name: area, dtype: float64\n",
    "-------------------------------------------------\n",
    "#(b) Using attribute-style access with column names that are strings\n",
    "data.area\n",
    "\n",
    "California    423967.0\n",
    "Florida       170312.0\n",
    "Illinois      149995.0\n",
    "New York      141297.0\n",
    "Texas         695662.0\n",
    "Text               NaN\n",
    "Name: area, dtype: float64\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "#iloc: uses the implicitly Python-style index\n",
    "# but the DataFrame labels are maintained\n",
    "data.iloc[:3, :2]\n",
    "\n",
    "--------------------------------\n",
    "\tarea\tpop\n",
    "California\t423967.0\t38332521.0\n",
    "Florida\t170312.0\t19552860.0\n",
    "Illinois\t149995.0\t12882135.0\n",
    "---------------------------------------\n",
    "\n",
    "# loc: allows access using the explicit index\n",
    "data.loc[:'New York', :'pop']\n",
    "-----------------------------------------\n",
    "\t           area\tpop\n",
    "California\t423967.0\t38332521.0\n",
    "Florida\t170312.0\t19552860.0\n",
    "Illinois\t149995.0\t12882135.0\n",
    "New York\t141297.0\t19651127.0\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#Checking that these two access modes are equivalent\n",
    "data.area is data['area']\n",
    "\n",
    "True\n",
    "\n",
    "-------------------------\n",
    "data.pop is data['pop'] # .pop might be a built in function so\n",
    "                        # using it as an attribute wont work\n",
    "False\n",
    "\n",
    "Notes: \n",
    "Default index is called ‘implicit’\n",
    "• It is consecutive and numeric (e.g., 0, 1, 2,…etc.)\n",
    "• Slice like in lists using the implicit index\n",
    "• If another index is defined, it is called explicit or fancy\n",
    "indexing\n",
    "• Data can be accessed using explicit index\n",
    "• Slices using explicit index do include the upper bound\n",
    "• Accessing data via special Index attributes:\n",
    "• loc – always references the explicit index\n",
    "• iloc – always references the implicit index\n",
    "• Rule: “explicit is better than implicit”\n",
    "• Note: An index object can have its own name\n",
    "\n",
    "\n",
    "A DataFrame is like a two-dimensional array with\n",
    "both flexible row indices and flexible column\n",
    "names.\n",
    "• Think of a DataFrame as a sequence of aligned Series\n",
    "objects, where “aligned” means that they share the\n",
    "same index.\n",
    "• DataFrame can be thought of a generalization of a\n",
    "NumPy array, or as a specialization of a Python\n",
    "dictionary. \n",
    "\n",
    "Different ways of creating a pd.DataFrame (Ex.2)\n",
    "• From a dictionary (Ex 2-1)\n",
    "• From a single Series object (Ex 2-2)\n",
    "• From a list of dicts (Ex 2-3)\n",
    "• Note: missing keys will be filled out with Nan (Ex 2.4)\n",
    "• From a dictionary series objects (Ex 2.5)\n",
    "• From a two-dimensional NumPy array (Ex 2.6)\n",
    "• From a NumPy structured array (Ex 2.7)\n",
    "\n",
    "• In a Pandas DataFrame, the rows are indexed, and\n",
    "the columns are labeled\n",
    "• Index attribute provides information about row value\n",
    "labels and the column attribute provides information\n",
    "about column labels\n",
    "• To access a Panda DataFrame, we can pass integer,\n",
    "slice or Boolean values to index and column\n",
    "attributes\n",
    "• Depending on what is passed via slicing or list values, we\n",
    "can get single rows/columns or multiple row/columns\n",
    "• Note that these attributes are used to access the\n",
    "DataFrame, not to set their values.\n",
    "\n",
    "Import pandas as pd\n",
    "• Create a DataFrame from two series\n",
    "• Data defined as a dictionary\n",
    "• Area and Population of five states\n",
    "• Check the index object and column object\n",
    "• Rows are identified with the name of the state\n",
    "• Columns are identified with labels (area or pop)\n",
    "• Add a new calculated column to the DataFrame\n",
    "• Learn different ways to access the DataFrame\n",
    "• Dictionary-style access\n",
    "• Attribute-style access\n",
    "• Array-style access\n",
    "• Indexing conventions\n",
    "\n",
    "\n",
    "Pandas allow element-wise operations, both with basic\n",
    "arithmetic (addition, subtraction, multiplication, etc.)\n",
    "and with more sophisticated operations (trigonometric\n",
    "functions, exponential and logarithmic functions, etc.).\n",
    "• This functionality is inherited from NumPy and uFuncs\n",
    "but Pandas include a couple useful twists:\n",
    "• for unary operations like negation and trigonometric\n",
    "functions, these ufuncs will preserve index and column labels\n",
    "in the output, and\n",
    "• for binary operations such as addition and multiplication,\n",
    "Pandas will automatically align indices when passing the object to the ufunc. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "629675b0",
   "metadata": {},
   "source": [
    "LC 13: \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Ex1-1. Creating a panda series from a list or array with default index\n",
    "data1 = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "print('This is data1:')\n",
    "print(data1)\n",
    "print('This is data1.values: ', data1.values)\n",
    "print('This is data1.index: ', data1.index)\n",
    "\n",
    "This is data1:\n",
    "0    0.25\n",
    "1    0.50\n",
    "2    0.75\n",
    "3    1.00\n",
    "dtype: float64\n",
    "This is data1.values:  [0.25 0.5  0.75 1.  ]\n",
    "This is data1.index:  RangeIndex(start=0, stop=4, step=1)\n",
    "------------------------------\n",
    "\n",
    "#Ex1-2. With explictly defined index consisting of string\n",
    "data2 = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                  index=['a', 'b', 'c', 'd'])\n",
    "print('This is data2:')\n",
    "print(data2)\n",
    "print('This is data2.values: ', data2.values)\n",
    "print('This is data2.index: ', data2.index)\n",
    "\n",
    "#access data in b\n",
    "print('Data in b using explicit index is: ', data2['b'])\n",
    "print('Data in b using implicit index is: ', data2[1])\n",
    "---------------------------------------\n",
    "This is data2:\n",
    "a    0.25\n",
    "b    0.50\n",
    "c    0.75\n",
    "d    1.00\n",
    "dtype: float64\n",
    "This is data2.values:  [0.25 0.5  0.75 1.  ]\n",
    "This is data2.index:  Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "Data in b using explicit index is:  0.5\n",
    "Data in b using implicit index is:  0.5\n",
    "------------------------------------------------\n",
    "#Ex1-3. With explictly defined index consisting of non sequential integers\n",
    "data3 = pd.Series([0.25, 0.5, 0.75, 1.0], index=[2, 5, 3, 7])\n",
    "print('This is data3:')\n",
    "print(data3)\n",
    "print('This is data3.values: ', data3.values)\n",
    "print('This is data3.index: ', data3.index)\n",
    "\n",
    "print('this is data[2]: ', data3[2])\n",
    "#print('this is data[0]: ', data3[0]) gives an error\n",
    "-----------------------------------------------------\n",
    "\n",
    "\n",
    "This is data3:\n",
    "2    0.25\n",
    "5    0.50\n",
    "3    0.75\n",
    "7    1.00\n",
    "dtype: float64\n",
    "This is data3.values:  [0.25 0.5  0.75 1.  ]\n",
    "This is data3.index:  Int64Index([2, 5, 3, 7], dtype='int64')\n",
    "this is data[2]:  0.25\n",
    "------------------------------------------\n",
    "\n",
    "#Ex1-4. Creating a Series from a dictionary\n",
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "print('This is the population Pandas Series:')\n",
    "print(population)\n",
    "print('This is the population in New York:', population['New York'])\n",
    "print('This is the slice California:New York \\n', population['California': 'New York'])\n",
    "print('This is the slice California:New York with implicit index \\n', population[:3])\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "This is the population Pandas Series:\n",
    "California    38332521\n",
    "Texas         26448193\n",
    "New York      19651127\n",
    "Florida       19552860\n",
    "Illinois      12882135\n",
    "dtype: int64\n",
    "This is the population in New York: 19651127\n",
    "This is the slice California:New York \n",
    " California    38332521\n",
    "Texas         26448193\n",
    "New York      19651127\n",
    "dtype: int64\n",
    "This is the slice California:New York with implicit index \n",
    " California    38332521\n",
    "Texas         26448193\n",
    "New York      19651127\n",
    "dtype: int64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "#Application Example: Stock prices in a series\n",
    "prices = [10.70, 10.80, 10.50, 10.90]\n",
    "#create a series with the prices called shares\n",
    "#shares = pd.Series(prices)\n",
    "#print('This is shares')\n",
    "#print(shares)\n",
    "days = ['Mon', 'Tues', 'Wed', 'Thu']\n",
    "shares = pd.Series(prices, index=days)\n",
    "#set the index of shares to the days of the week\n",
    "print('This is shares')\n",
    "print(shares)\n",
    "#print the index object\n",
    "print('This is the index object of shares')\n",
    "print(shares.index)\n",
    "\n",
    "---------------------------\n",
    "\n",
    "\n",
    "\n",
    "This is shares\n",
    "Mon     10.7\n",
    "Tues    10.8\n",
    "Wed     10.5\n",
    "Thu     10.9\n",
    "dtype: float64\n",
    "This is the index object of shares\n",
    "Index(['Mon', 'Tues', 'Wed', 'Thu'], dtype='object')\n",
    "---------------------------\n",
    "#Index Operations (Access and Slicing)\n",
    "\n",
    "#print the index of the second element shares.index[1]\n",
    "print('This is shares.index[1]: ', shares.index[1], 'and the value is', shares[1])\n",
    "\n",
    "#print the first two indices\n",
    "print('This is shares.index[ :2]: ', shares.index[ :2])\n",
    "\n",
    "#print the last two indices\n",
    "print('This is shares.index[-2:]: ', shares.index[ -2:])\n",
    "\n",
    "------------------------------------------------------------\n",
    "\n",
    "This is shares.index[1]:  Tues and the value is 10.8\n",
    "This is shares.index[ :2]:  Index(['Mon', 'Tues'], dtype='object')\n",
    "This is shares.index[-2:]:  Index(['Wed', 'Thu'], dtype='object')\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Index operations may not always work!\n",
    "#change the index in 2 to 'Wednesday'\n",
    "#shares.index[2] = 'Wednesday'\n",
    "shares.index[ :4] = ['Monday', 'Tuesday']\n",
    "-------------------------------\n",
    "Index does not support mutable operations\n",
    "---------------------------------------------\n",
    "\n",
    "shares.index.name='Weekdays'\n",
    "print('This is the index object of shares')\n",
    "print(shares.index)\n",
    "-----------------------\n",
    "\n",
    "\n",
    "This is the index object of shares\n",
    "Index(['Mon', 'Tues', 'Wed', 'Thu'], dtype='object', name='Weekdays')\n",
    "---------------------------------------\n",
    "\n",
    "#Access data with the implicit (iloc) vs. explicit index (loc)\n",
    "#print the share prices on Tuesday using both methods\n",
    "shares.iloc[:2]\n",
    "------------------------------------\n",
    "Weekdays\n",
    "Mon     10.7\n",
    "Tues    10.8\n",
    "dtype: float64\n",
    "------------------------------\n",
    "#Slice data with the implicit (iloc) vs. explicit index (loc)\n",
    "#print the share prices from Tues to Thurs\n",
    "shares.loc['Mon':'Thu']\n",
    "-----------------------------------------\n",
    "Weekdays\n",
    "Mon     10.7\n",
    "Tues    10.8\n",
    "Wed     10.5\n",
    "Thu     10.9\n",
    "dtype: float64\n",
    "------------------------\n",
    "\n",
    "Notes: \n",
    "\n",
    "Pandas is a newer package built on top of NumPy\n",
    "and provides an efficient implementation of a\n",
    "DataFrame.\n",
    "• DataFrames are essentially multidimensional arrays\n",
    "with attached row and column labels, and often\n",
    "with heterogeneous types and/or missing data.\n",
    "• Import Pandas under the alias pd after importing NumPy\n",
    "because of their dependency\n",
    "\n",
    "• Pandas objects can be thought of as enhanced\n",
    "versions of NumPy structured arrays in which the\n",
    "rows and columns are identified with labels rather\n",
    "than simple integer indices.\n",
    "• There are two main objects: Series and DataFrames\n",
    "• Think of DataFrames as spreadsheets with more flexible\n",
    "row and column labels\n",
    "\n",
    "\n",
    "A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array\n",
    "• The main difference between a NumPy 1D array and a\n",
    "Panda series is the presence of the index\n",
    "• while the NumPy array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.\n",
    "• Think of a Pandas Series a bit like a specialization of a\n",
    "Python dictionary.\n",
    "• A dictionary is a structure that maps arbitrary keys to a set of\n",
    "arbitrary values, and a Series is a structure that maps typed\n",
    "keys to a set of typed values.\n",
    "• Different ways of creating a pd.series (Ex. 1)\n",
    "\n",
    "\n",
    "A Pandas Series can be thought of as a single column in a\n",
    "spreadsheet.\n",
    "• For ease of representation, Pandas Series are sometimes\n",
    "displayed horizontally because they have only one\n",
    "dimension.\n",
    "• The data type within a Pandas Series does not have to\n",
    "homogeneous (i.e., same type).\n",
    "• You can mix numeric and non-numeric data types within the same\n",
    "series. Also, a series can have missing values (None).\n",
    "\n",
    "Index object is an important structure in itself\n",
    "• It can be thought of either as an immutable array or as an\n",
    "ordered set\n",
    "• Index as immutable array\n",
    "• The Index object in many ways operates like an array. We can\n",
    "use standard Python indexing notation to retrieve values or\n",
    "slices\n",
    "• One difference between Index objects and NumPy arrays is\n",
    "that indices cannot be modified via the normal means (i.e.,\n",
    "indexes are immutable)\n",
    "• Index as ordered set\n",
    "• The Index object follows many of the conventions used by\n",
    "Python’s built-in set data structure, so that unions,\n",
    "intersections, differences, and other combinations can be\n",
    "computed in a familiar way\n",
    "\n",
    "\n",
    "Indexing Conventions\n",
    "• Slice like in lists using the implicit index\n",
    "• Fancy indexing refers to the explicit index or label\n",
    "• Note difference between implicit and explicit index\n",
    "when slicing with respect to the upper bound of the\n",
    "range.\n",
    "• Special Index attributes:\n",
    "• loc – always references the explicit index\n",
    "• iloc – always references the implicit index\n",
    "• Rule: “explicit is better than implicit”\n",
    "• Note: An index object can have its own name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba56d49b",
   "metadata": {},
   "source": [
    "LC 11: Numpy Operations\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#Comparing np.add.reduce to np.sum\n",
    "y = np.arange(1, 8)\n",
    "adred = np.add.reduce(y)\n",
    "print(\"y is: \\n\", y)\n",
    "print(\"result using np.add.reduce is: \", adred)\n",
    "s = np.sum(y)\n",
    "print(\"result using np.sum is: \", s)\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "y is: \n",
    " [1 2 3 4 5 6 7]\n",
    "result using np.add.reduce is:  28\n",
    "result using np.sum is:  28\n",
    "--------------------------------\n",
    "\n",
    "#Outer method\n",
    "#the outer method computes the product of all pairs of diff elements\n",
    "x = np.arange(1, 6)\n",
    "xx = np.multiply.outer(x, x)\n",
    "print(\"x is: \\n\", x)\n",
    "print(\"result using outer method: \\n\", xx)\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "x is: \n",
    " [1 2 3 4 5]\n",
    "result using outer method: \n",
    " [[ 1  2  3  4  5]\n",
    " [ 2  4  6  8 10]\n",
    " [ 3  6  9 12 15]\n",
    " [ 4  8 12 16 20]\n",
    " [ 5 10 15 20 25]]\n",
    " --------------------------------\n",
    " \n",
    " #Numpy Matrix Multiplication\n",
    "#Source: https://www.educative.io/blog/numpy-matrix-multiplication\n",
    "#Case 1: With .dot (also called scalar multiplication)\n",
    "X = [6, 7] \n",
    "    \n",
    "Y = [[1, 3],\n",
    "      [5, 7]]\n",
    "print('This is np.dot(X,Y): \\n', np.dot(X,Y))\n",
    "print(\"----------\")\n",
    "print('This is np.dot(Y,X): \\n', np.dot(Y,X))\n",
    "\n",
    "#Note that the order matters\n",
    "#Restriction: we can only multiply two matrices if the number of columns in the first matrix \n",
    "#is equal to the number of rows in the second matrix.\n",
    "#np.dot is not recommended for high dimensional arrays\n",
    "-------------------------------------------------------------\n",
    "This is np.dot(X,Y): \n",
    " [41 67]\n",
    "----------\n",
    "This is np.dot(Y,X): \n",
    " [27 79]\n",
    " -----------------------------\n",
    " \n",
    "#Case 2: With .matmul() \n",
    "X = [[6, 7],\n",
    "      [8, 9]]     \n",
    "Y = [[1, 3],\n",
    "      [5, 7]]\n",
    "MM=np.matmul(X,Y)\n",
    "print('This is np.matmul(X,Y): \\n', MM)\n",
    "-----------------------------------------\n",
    "\n",
    "\n",
    "This is np.matmul(X,Y): \n",
    " [[41 67]\n",
    " [53 87]]\n",
    " -----------------------------------\n",
    " \n",
    " \n",
    "#Case3: Element-wise matrix multiplication with .multiply()\n",
    "\n",
    "#The sizes of the rows, columns, or submatrices that we pass as our operands should be the same. \n",
    "M1 = np.array([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n",
    "M2 = np.array([[1, 2, 3, 4, 5], [5, 4, 3, 2, 1]])\n",
    "print('This is np.multiply(M1,M2):\\n', np.multiply(M1,M2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is np.multiply(M1,M2):\n",
    " [[ 1  6 15 28 45]\n",
    " [10 16 18 16 10]]\n",
    " -------------------------------------------------------\n",
    " \n",
    " #Efficiency of Numpy Operations\n",
    "#Comparing symbol (+) with uFunc (add)\n",
    "#importing the modules\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "x = np.arange(1, 1000) #Generate an array from 1 to 999\n",
    "print('Time taken by add method: ', end=\"\")\n",
    "%timeit np.add(2,x)\n",
    "\n",
    "print('Time taken by arithmetic symbol: ', end=\"\")\n",
    "%timeit 2+x\n",
    "-----------------------------------------------\n",
    "\n",
    "\n",
    "Time taken by add method: The slowest run took 5.43 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
    "10.9 µs ± 6.42 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
    "Time taken by arithmetic symbol: The slowest run took 4.98 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
    "7.08 µs ± 4.97 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Efficiency of numpy vectorized operations\n",
    "#Source: https://www.geeksforgeeks.org/vectorized-operations\n",
    "#importing the modules\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "#vectorized sum\n",
    "print('result  of np.sum on arange of 15000: ', np.sum(np.arange(15000)))\n",
    "print(\"Time taken by vectorized sum: \", end =\"\")\n",
    "%timeit np.sum(np.arange(15000))\n",
    "\n",
    "print('------------')\n",
    "#iterative sum\n",
    "total=0\n",
    "for item in range(0, 15000):\n",
    "    total +=item\n",
    "a=total\n",
    "\n",
    "print(\"result computed by for loop: \", a)\n",
    "\n",
    "print(\"Time taken by iterative sum: \", end=\"\")\n",
    "%timeit a\n",
    "\n",
    "result  of np.sum on arange of 15000:  112492500\n",
    "Time taken by vectorized sum: 73.3 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "------------\n",
    "result computed by for loop:  112492500\n",
    "Time taken by iterative sum: 105 ns ± 20.1 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
    "--------------------------------------------------\n",
    "\n",
    "\n",
    "#Stat aggregation examples\n",
    "x3 =  np.random.randint(1000, size=(3,4,5)) #Three dimensional array\n",
    "print(\"The three dimensional array is printed below in three parts: \\n\")\n",
    "print(x3)\n",
    "----------------------------------\n",
    "\n",
    "The three dimensional array is printed below in three parts: \n",
    "\n",
    "[[[ 88 293 266 646 650]\n",
    "  [862 773 613 349 121]\n",
    "  [733  78  18 210 186]\n",
    "  [549 341 539 658 763]]\n",
    "\n",
    " [[332  28 225 571 510]\n",
    "  [887 431 178 650 303]\n",
    "  [534 714   9 784 218]\n",
    "  [967 704 221 149 553]]\n",
    "\n",
    " [[315 698 694 656  16]\n",
    "  [935 571 316 315 135]\n",
    "  [129 180 525 716  97]\n",
    "  [964 150 621 863 620]]]\n",
    "  \n",
    "-------------------------------------------\n",
    "\n",
    "print('Min is: ', np.min(x3))\n",
    "print('Max is: ', np.max(x3))\n",
    "\n",
    "\n",
    "Min is:  9\n",
    "Max is:  967\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "print('Average is: ', np.mean(x3))\n",
    "\n",
    "Average is:  453.6666666666667\n",
    "\n",
    "\n",
    "---------------------------------\n",
    "#Broadcasting examples \n",
    "\n",
    "#Adding a 2D array and 1D array\n",
    "M = np.ones((2, 3))\n",
    "a = np.arange(3)\n",
    "print('M = \\n', M)\n",
    "print('a = ', a)\n",
    "print('M+a = ', M+a)\n",
    "\n",
    "M = \n",
    " [[1. 1. 1.]\n",
    " [1. 1. 1.]]\n",
    "a =  [0 1 2]\n",
    "M+a =  [[1. 2. 3.]\n",
    " [1. 2. 3.]]\n",
    " ----------------------------------------------\n",
    "#Case where both arrays need to broadcast\n",
    "a = np.arange(3).reshape((3, 1))\n",
    "b = np.arange(3)\n",
    "a + b\n",
    "print('a = \\n', a)\n",
    "print('b = ', b)\n",
    "print('a+b = \\n', a+b)\n",
    "\n",
    "\n",
    "a = \n",
    " [[0]\n",
    " [1]\n",
    " [2]]\n",
    "b =  [0 1 2]\n",
    "a+b = \n",
    " [[0 1 2]\n",
    " [1 2 3]\n",
    " [2 3 4]]\n",
    " \n",
    " ---------------------------------------\n",
    " #Case where two arrays are not compatible and gives an error\n",
    "M = np.ones((3, 2))\n",
    "a = np.arange(3)\n",
    "print('M = \\n', M)\n",
    "print('a = ', a)\n",
    "print('M+a = ', M+a)\n",
    "\n",
    "\n",
    "M = \n",
    " [[1. 1.]\n",
    " [1. 1.]\n",
    " [1. 1.]]\n",
    "a =  [0 1 2]\n",
    "\n",
    "operands could not be broadcast together with shapes (3,2) (3,)\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "Notes: \n",
    "\n",
    "A universal function (or ufunc for short) is a function that operates on ndarrays in an element-by-element fashion\n",
    "Example: np.add(2, x) where x is a Numpy array adds 2 to each element of the array.\n",
    "Unary uFuncs vs. Binary uFuncs if they take one or two inputs\n",
    "Example: np.add(2,x) vs. np.sum(x)\n",
    "np.add() is used to add two arrays (or a scalar and an array) element-wise whereas np.sum() returns the sum of all elements in a single array.\n",
    "ufuncs are used to implement vectorization in NumPy.\n",
    "Vectorization refers to the use of pre-compiled functions and mathematical operations on Numpy array objects.\n",
    "Vectorization also provide broadcasting and additional methods like reduce, accumulate etc. that are very helpful for computation\n",
    "The alternative to vectorization would be to iterate through the elements of the array and perform the desired operations. Vectorization is much faster than iterating over elements\n",
    "\n",
    "Numpy Computations\n",
    "\n",
    "Unary uFuncs \n",
    "Simple element-wise transformations in a single array\n",
    "Ex: np.sqrt(x) calculates the square root of each element in x\n",
    "Arithmetic operations\n",
    "Use symbol or equivalent uFunc\n",
    "Binary uFuncs take two inputs\n",
    "Aggregation Operations\n",
    "Reduce or Accumulate methods\n",
    "Statistical Aggregations (min, max, etc.)\n",
    "Broadcasting operates on arrays of different shapes\n",
    "\n",
    "\n",
    "Accumulate and Reduce Operations\n",
    "\n",
    "When used in conjunction with add or multiply, .reduce operations have the effect of reducing an np array to a single number whereas .accumulate operations show running totals.\n",
    "Is there another way to add and reduce all the elements of an array?\n",
    "\n",
    "\n",
    "Exercise: Explore the .outer method when used with multiply\n",
    "\n",
    "\n",
    "Three ways of performing matrix multiplication in Numpy\n",
    "np.dot(A,B) returns the scalar or dot product of two arrays\n",
    "np.matmul(A,B) returns the matrix product of two arrays \n",
    "np.multiply(A,B) returns the element-wise matrix multiplication of two arrays\n",
    "\n",
    "\n",
    "\n",
    "In Python, the timeit module is used to compute the execution time of small bits of Python code. This module uses a platform-specific time function to make the most accurate calculations possible.\n",
    "\n",
    "Note: if you try this code in your machine, your results might be different. \n",
    "Also, if you run it different times, the results might vary.\n",
    "\n",
    "If np.sum was not available,\n",
    "we would have to use a for loop.\n",
    "\n",
    "Vectorized sum is more efficient than\n",
    "Iterative sum.\n",
    "\n",
    "terms of averages and st.dev of multiple runs in nanoseconds (ns or 10^-9) or microseconds (us or 10^-6)\n",
    "In contrast, the method timeit.timeit() gives a single estimate in seconds (as a float)\n",
    "timeit.timeit(stmt, setup) executes the setup statement once, and returns the time it takes to execute the main statement (‘stmt’) a number of times, where number = 1000000\n",
    "Since the number of runs is equal in all cases, the results are more comparable than if you use the magic command (%timeit) \n",
    "The disadvantage of the method is that it takes a bit more of code to produce. \n",
    "\n",
    "\n",
    "Broadcasting means the ability to perform operations between arrays of different sizes.\n",
    "Adding a number to a whole array is an example of broadcasting. \n",
    "Broadcasting is a feature that saves memory and coding, but it is somewhat difficult to understand at first.  \n",
    "We will do a few exercises at the end of this session.\n",
    "To learn more about Broadcasting and see examples consult the book chapter listed below.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9683e5ec",
   "metadata": {},
   "source": [
    "LC 10: \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Exercise 1: Slicing Arrays\n",
    "\n",
    "#Create a one-dimensional array of consecutive numbers 0 to 9 and slice it\n",
    "x = np.arange(10) \n",
    "print('This is the x array: ', x)\n",
    "\n",
    "#slice all elements in the first five positions (0th to 4th)\n",
    "print('This is slice :5 (first five positions) ', x[:5])\n",
    "\n",
    "#slice all elements in the last five positions (5th to 9th)\n",
    "print('This is slice 5: (last five positions) ', x[5:])\n",
    "\n",
    "#slice the middle subarray between indexes 4 and 7, not including the end position\n",
    "print('This is slice 4:7 ', x[4:7])\n",
    "\n",
    "#slice every other element or go in steps of two\n",
    "print('This is slice ::2 ', x[::2])\n",
    "\n",
    "# slice every other element, starting at index 1\n",
    "print('This is slice 1::2 ', x[1::2])\n",
    "\n",
    "# slice all elements, reversed\n",
    "print('This is slice ::-1 ', x[::-1])\n",
    "\n",
    "# slice with every other element reversed from index 5\n",
    "print('This is slice 5::-2 ', x[5::-2])\n",
    "\n",
    "=========\n",
    "\n",
    "This is the x array:  [0 1 2 3 4 5 6 7 8 9]\n",
    "This is slice :5 (first five positions)  [0 1 2 3 4]\n",
    "This is slice 5: (last five positions)  [5 6 7 8 9]\n",
    "This is slice 4:7  [4 5 6]\n",
    "This is slice ::2  [0 2 4 6 8]\n",
    "This is slice 1::2  [1 3 5 7 9]\n",
    "This is slice ::-1  [9 8 7 6 5 4 3 2 1 0]\n",
    "This is slice 5::-2  [5 3 1]\n",
    "-----------------------------------\n",
    "\n",
    "#In class exercise: Generate a 2D-array x2 of random numbers with 3 rows and 4 columns and print the following slices:\n",
    "#Slice 1: first two rows and first three columns\n",
    "#Slice 2: all rows, every other column\n",
    "\n",
    "x2=np.random.randint(10, size=(3,4))\n",
    "print('x2 is: ',x2)\n",
    "\n",
    "#Slice 1: two rows, three columns\n",
    "print('Slice 1: two rows, three columns \\n', x2[:2:, :3:])\n",
    "\n",
    "#Slice 2: all rows, every other column\n",
    "print('Slice 2: all rows, every other columns \\n', x2[::, ::2])\n",
    "\n",
    "\n",
    "[[5 1 2 8]\n",
    " [6 3 8 3]\n",
    " [6 8 9 3]]\n",
    "Slice 1: two rows, three columns \n",
    " [[5 1 2]\n",
    " [6 3 8]]\n",
    "Slice 2: all rows, every other columns \n",
    " [[5 2]\n",
    " [6 8]\n",
    " [6 9]]\n",
    " \n",
    " ---------------------------------\n",
    " \n",
    " \n",
    "#Exercise 2: Reshaping Arrays\n",
    "#Reshape Pattern 1: reshape 1D into a grid \n",
    "grid1 = np.arange(1, 10)\n",
    "\n",
    "print('original grid: ', grid1)\n",
    "#Reshape into a 3x3 grid and print\n",
    "grid1 = np.arange(1, 10).reshape((3,3))\n",
    "print('This is grid1: \\n', grid1)\n",
    "\n",
    "\n",
    "\n",
    "original grid:  [1 2 3 4 5 6 7 8 9]\n",
    "This is grid1: \n",
    " [[1 2 3]\n",
    " [4 5 6]\n",
    " [7 8 9]]\n",
    " \n",
    "--------------------------------------\n",
    "\n",
    "\n",
    "#Reshape Pattern 2: conversion of 1D array into a row or column matrix\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "# row vector via reshape\n",
    "row_v=x.reshape((1,3))\n",
    "print('This is a row vector: \\n', row_v)\n",
    "\n",
    "# column vector via reshape\n",
    "col_v=x.reshape((3,1))\n",
    "print('This is a column vector: \\n', col_v)\n",
    "\n",
    "\n",
    "This is a row vector: \n",
    " [[1 2 3]]\n",
    "This is a column vector: \n",
    " [[1]\n",
    " [2]\n",
    " [3]]\n",
    " \n",
    " -------------------------------------\n",
    " \n",
    "#Exercise 3: Concatenating arrays of the same dimension\n",
    "\n",
    "#np.concatenate can join two or three unidimensional arrays\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([3, 2, 1])\n",
    "\n",
    "#concatenate x and y\n",
    "print('Joining x and y: \\n', np.concatenate([x,y]))\n",
    "\n",
    "z = [99, 99, 99]\n",
    "\n",
    "#concatenate z to x and y\n",
    "print('Joining x, y and z: \\n', np.concatenate([x,y, z]))\n",
    "\n",
    "#np.concatenate can also join two-dimensional arrays\n",
    "grid = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "\n",
    "#Concatenation of grid with itself along first axis (vertically):\n",
    "print('Concatenation along first axis :\\n', np.concatenate([grid, grid]))\n",
    "\n",
    "#Concatenation of grid along second axis (or horizontally):\n",
    "print('Concatenation along second axis :\\n', np.concatenate([grid, grid], axis=1))\n",
    "\n",
    "\n",
    "Joining x and y: \n",
    " [1 2 3 3 2 1]\n",
    "Joining x, y and z: \n",
    " [ 1  2  3  3  2  1 99 99 99]\n",
    "Concatenation along first axis :\n",
    " [[1 2 3]\n",
    " [4 5 6]\n",
    " [1 2 3]\n",
    " [4 5 6]]\n",
    "Concatenation along second axis :\n",
    " [[1 2 3 1 2 3]\n",
    " [4 5 6 4 5 6]]\n",
    " \n",
    " ------------------------------------------\n",
    " \n",
    "#Exercise 4: Concatenating arrays of the same dimension\n",
    "\n",
    "#For arrays of different dimensions, it is clearer to use vertical or horizontal stack\n",
    "x = np.array([1, 2, 3])\n",
    "grid = np.array([[9, 8, 7],\n",
    "                 [6, 5, 4]])\n",
    "\n",
    "#Vertically stack the arrays x and grid\n",
    "print('Vertically stack the arrays x and grid: \\n', np.vstack([x,grid]))\n",
    "\n",
    "y = np.array([[99],\n",
    "              [99]])\n",
    "\n",
    "#Horizontally stack the arrays grid and y\n",
    "print('Horizontally stack the arrays grid and y: \\n', np.hstack([grid,y]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Vertically stack the arrays x and grid: \n",
    " [[1 2 3]\n",
    " [9 8 7]\n",
    " [6 5 4]]\n",
    "Horizontally stack the arrays grid and y: \n",
    " [[ 9  8  7 99]\n",
    " [ 6  5  4 99]]\n",
    "\n",
    "---------------------------------------------\n",
    "#Exercise 5: Splitting Arrays\n",
    "x = [1, 2, 3, 99, 99, 3, 2, 1]\n",
    "x1, x2, x3 = np.split(x, [3, 5])\n",
    "print('This is the result of the split in x: \\n', x1, x2, x3)\n",
    "\n",
    "This is the result of the split in x: \n",
    " [1 2 3] [99 99] [3 2 1]\n",
    " \n",
    " ----------------------------------------\n",
    " \n",
    "grid = np.arange(16).reshape((4, 4))\n",
    "print('This is grid: \\n', grid)\n",
    "upper, lower = np.vsplit(grid, [2])\n",
    "print('This is upper: \\n', upper)\n",
    "print('This is lower: \\n', lower)\n",
    "left, right = np.hsplit(grid, [2])\n",
    "print('This is left: \\n', left)\n",
    "print('This is right: \\n', right)\n",
    "\n",
    "\n",
    "This is grid: \n",
    " [[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "This is upper: \n",
    " [[0 1 2 3]\n",
    " [4 5 6 7]]\n",
    "This is lower: \n",
    " [[ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "This is left: \n",
    " [[ 0  1]\n",
    " [ 4  5]\n",
    " [ 8  9]\n",
    " [12 13]]\n",
    "This is right: \n",
    " [[ 2  3]\n",
    " [ 6  7]\n",
    " [10 11]\n",
    " [14 15]]\n",
    "\n",
    " ------------------------------\n",
    " \n",
    " #Exercise 6: Examples of array arithmetic operations\n",
    "\n",
    "x = np.arange(4)\n",
    "print(\"x =\", x)\n",
    "print(\"x + 5 =\", x + 5)\n",
    "print(\"x - 5 =\", x - 5)\n",
    "print(\"x * 2 =\", x * 2)\n",
    "print(\"x / 2 =\", x / 2)\n",
    "print(\"x // 2 =\", x // 2) # floor division\n",
    "print(\"-x = \", -x)\n",
    "print(\"x ** 2 = \", x ** 2)\n",
    "print(\"x % 2 = \", x % 2)\n",
    "y = -(0.5*x + 1) ** 2\n",
    "print(\"y = \", y)\n",
    "\n",
    "\n",
    "x = [0 1 2 3]\n",
    "x + 5 = [5 6 7 8]\n",
    "x - 5 = [-5 -4 -3 -2]\n",
    "x * 2 = [0 2 4 6]\n",
    "x / 2 = [0.  0.5 1.  1.5]\n",
    "x // 2 = [0 0 1 1]\n",
    "-x =  [ 0 -1 -2 -3]\n",
    "x ** 2 =  [0 1 4 9]\n",
    "x % 2 =  [0 1 0 1]\n",
    "y =  [-1.   -2.25 -4.   -6.25]\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "#Use np.add to add 2 to x\n",
    "print('Original Array x: ', x)\n",
    "z=np.add(2,x)\n",
    "print('Resulting Array z:', z)\n",
    "print('is the result the same as x+2?: ', x+2)\n",
    "\n",
    "\n",
    "\n",
    "Original Array x:  [0 1 2 3]\n",
    "Resulting Array z: [2 3 4 5]\n",
    "is the result the same as x+2?:  [2 3 4 5]\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "#Exercise 7: Examples of array aggregation operations (add.reduce and multiply.reduce)\n",
    "x = np.arange(1, 6)\n",
    "print('This is the original array: ', x)\n",
    "r = np.add.reduce(x) #add.reduce sums all elements in array\n",
    "print(\"sum of all elements: \", r)\n",
    "\n",
    "m = np.multiply.reduce(x)\n",
    "print(\"product of all elements: \", m)\n",
    "\n",
    "\n",
    "This is the original array:  [1 2 3 4 5]\n",
    "sum of all elements:  15\n",
    "product of all elements:  120\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "a1 = np.add.accumulate(x) #accumulate stores intermediate results\n",
    "print(\"a1 = \", a1)\n",
    "a2 = np.multiply.accumulate(x)\n",
    "print(\"a2 = \", a2)\n",
    "\n",
    "\n",
    "a1 =  [ 1  3  6 10 15]\n",
    "a2 =  [  1   2   6  24 120]\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "NOtes:\n",
    "\n",
    "\n",
    "Used to access subarrays with the slice notation,\n",
    "marked by the colon (:) character.\n",
    "• The NumPy slicing syntax is the same used in a\n",
    "standard Python list\n",
    "• To access a slice of an array x, use this:\n",
    "x[start:stop:step]\n",
    "• If any of these are unspecified, they default to the values\n",
    "start=0, stop=size of dimension, step=1.\n",
    "• When the step value is negative, the defaults for start\n",
    "and stop are swapped\n",
    "\n",
    "Multidimensional slices work by having multiple slices\n",
    "separated by commas\n",
    "• x[start:stop:step, start:stop:step]\n",
    "• Accessing single rows or columns of an array is done by\n",
    "combining indexing and slicing, using an empty slice\n",
    "marked by a single colon (:)\n",
    "• Subarrays as no-copy views: array slices return views rather than copies of the array data.\n",
    "• This is different from Python list slicing (in lists, slices are\n",
    "copies).\n",
    "• In arrays, if a subarray is modified, the original array is\n",
    "changed!\n",
    "• To explicitly copy the data within an array or a subarray use\n",
    "the copy() method\n",
    "\n",
    "The reshape method converts or reorganizes an\n",
    "array into another with a different shape\n",
    "• Reshaping Pattern 1\n",
    "• Convert an array into another one with a different\n",
    "number of rows and columns\n",
    "• The size of the initial array must match the size of the\n",
    "reshaped array\n",
    "• Reshaping Pattern 2\n",
    "• Convert a one-dimensional array into a two-dimensional\n",
    "row or column matrix.\n",
    "\n",
    "\n",
    "Concatenation, or joining of two arrays in NumPy, is\n",
    "done through: np.concatenate, np.vstack, and\n",
    "np.hstack. • np.concatenate takes a tuple or list of arrays as its first\n",
    "argument\n",
    "• np.vstack joins via vertical stacking, while np.hstack joins via horizontal stacking\n",
    "• To join by the third dimension, use np.dstack\n",
    "• The opposite of concatenation is splitting, which is\n",
    "implemented by the functions np.split, np.hsplit, and\n",
    "np.vsplit. • For each of these, we can pass a list of indices giving the split\n",
    "points\n",
    "• N split points lead to N + 1 subarrays\n",
    "\n",
    "\n",
    "• Computation on NumPy arrays is efficient due to\n",
    "the optimized operations or vectorized operations.\n",
    "• Vectorized operations are implemented through\n",
    "NumPy’s universal functions (ufuncs).\n",
    "• Depending on the number of inputs they receive, ufuncs\n",
    "can be unary (one input) or binary (two inputs).\n",
    "• Array arithmetic\n",
    "• Aggregation Functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a51dd0d",
   "metadata": {},
   "source": [
    "LC 9:\n",
    "\n",
    "#Ex. 1\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a rank 1 array a by seeding values\n",
    "a = np.array([7, 2, 9, 10])\n",
    "\n",
    "# Print the type of a \n",
    "print('Type of a: ', type(a))\n",
    "\n",
    "# Print the shape \n",
    "print('The shape of a is:', a.shape)\n",
    "\n",
    "# Print each element\n",
    "print('The elements are: ', a[0], a[1], a[2], a[3])\n",
    "print('All elements are: ', a)\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "Type of a:  <class 'numpy.ndarray'>\n",
    "The shape of a is: (4,)\n",
    "The elements are:  7 2 9 10\n",
    "All elements are:  [ 7  2  9 10]\n",
    "\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\n",
    "# Change the 2 to an 8 in the array\n",
    "a[1]=8\n",
    "\n",
    "# Print the entire array\n",
    "print(a)\n",
    "\n",
    "#print the type of data in the a array\n",
    "print('type of data inside the array is: ', a.dtype)\n",
    "print(type(a[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[ 7  8  9 10]\n",
    "type of data inside the array is:  int32\n",
    "<class 'numpy.int32'>\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "\n",
    "# 2. Create a rank 2 array b by seeding values\n",
    "b = np.array([[5.2,3.0,4.5],[9.1,0.1,0.3]])\n",
    "\n",
    "print('shape: ', b.shape)\n",
    "print('dtype: ', b.dtype)\n",
    "print('all elements: \\n', b)\n",
    "\n",
    "\n",
    "\n",
    "shape:  (2, 3)\n",
    "dtype:  float64\n",
    "all elements: \n",
    " [[5.2 3.  4.5]\n",
    " [9.1 0.1 0.3]]\n",
    " \n",
    " \n",
    " ------------------------------------\n",
    " \n",
    " \n",
    "print('Element in first corner: ', b[0,0])\n",
    "\n",
    "\n",
    "Element in first corner:  5.2\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "\n",
    "print('Element in first corner: ', b[0][0])\n",
    "\n",
    "\n",
    "Element in first corner:  5.2\n",
    "\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "\n",
    "#Ex. 2: Array creation\n",
    "\n",
    "# 2.1. Create a length-10 integer array a filled with zeros and print it\n",
    "#a=np.zeros(10, dtype=int)\n",
    "a=np.zeros(10, dtype=float)\n",
    "print('Array of zeros is: ', a)\n",
    "\n",
    "Array of zeros is:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "a=np.zeros((2,2), dtype=float)\n",
    "print('2D Array of zeros is: \\n', a)\n",
    "\n",
    "\n",
    "2D Array of zeros is: \n",
    " [[0. 0.]\n",
    " [0. 0.]]\n",
    " \n",
    " --------------------------------\n",
    " \n",
    "# 2.2. Create a 3x5 floating-point array b filled with 1s and print it\n",
    "b=np.ones((3,5), dtype=float)\n",
    "print('2D Array of ones is: \\n', b)\n",
    "\n",
    "\n",
    "2D Array of ones is: \n",
    " [[1. 1. 1. 1. 1.]\n",
    " [1. 1. 1. 1. 1.]\n",
    " [1. 1. 1. 1. 1.]]\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "# 2.3. Create a 3x5 array c filled with 3.14 and print it\n",
    "c=np.full((3,5), 3.14)\n",
    "print('2D array of same number 3.14 is: \\n', c)\n",
    "\n",
    "\n",
    "2D array of same number 3.14 is: \n",
    " [[3.14 3.14 3.14 3.14 3.14]\n",
    " [3.14 3.14 3.14 3.14 3.14]\n",
    " [3.14 3.14 3.14 3.14 3.14]]\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "# 2.4. Generate an array from a list of 8 consecutive numbers\n",
    "d=np.arange(1,9)\n",
    "print('list of consecutive numbers: ', d)\n",
    "\n",
    "\n",
    "list of consecutive numbers:  [1 2 3 4 5 6 7 8]\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "d1=np.arange(9)\n",
    "print('list of consecutive numbers using arange(9): ', d1)\n",
    "\n",
    "\n",
    "list of consecutive numbers using arange(9):  [0 1 2 3 4 5 6 7 8]\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "#Ex.3: Array attributes\n",
    "#Check more info about the seed at https://www.w3schools.com/python/ref_random_seed.asp\n",
    "np.random.seed(0) # seed for reproducibility\n",
    "x1 = np.random.randint(10, size=6) # One-dimensional array\n",
    "x2 = np.random.randint(10, size=(3, 4)) # Two-dimensional array\n",
    "x3 = np.random.randint(10, size=(3, 4, 5)) # Three-dimensional array\n",
    " \n",
    "print('x1 is: ', x1)\n",
    "print('x2 is: \\n', x2)\n",
    "print('x3 is: \\n', x3)\n",
    "\n",
    "\n",
    "x1 is:  [5 0 3 3 7 9]\n",
    "x2 is: \n",
    " [[3 5 2 4]\n",
    " [7 6 8 8]\n",
    " [1 6 7 7]]\n",
    "x3 is: \n",
    " [[[8 1 5 9 8]\n",
    "  [9 4 3 0 3]\n",
    "  [5 0 2 3 8]\n",
    "  [1 3 3 3 7]]\n",
    "\n",
    " [[0 1 9 9 0]\n",
    "  [4 7 3 2 7]\n",
    "  [2 0 0 4 5]\n",
    "  [5 6 8 4 1]]\n",
    "\n",
    " [[4 9 8 1 1]\n",
    "  [7 9 9 3 6]\n",
    "  [7 2 0 3 5]\n",
    "  [9 4 4 6 4]]]\n",
    "  \n",
    "------------------------------------------------\n",
    "\n",
    "# print the dimension of array x3\n",
    "print('x3 ndim: ', x3.ndim)\n",
    "\n",
    "# print the shape of array x3\n",
    "print('x3 shape: ', x3.shape)\n",
    "\n",
    "# print the size of array x3 \n",
    "print('x3 size: ', x3.size) \n",
    "\n",
    "# print the datatype of array x3\n",
    "print('dtype: ', x3.dtype)\n",
    "\n",
    "# print the itemsize of array x3\n",
    "print('itemsize: ', x3.itemsize)\n",
    "\n",
    "# print the number of bytes of array x3\n",
    "print('nbytes:', x3.nbytes)\n",
    "\n",
    "x3 ndim:  3\n",
    "x3 shape:  (3, 4, 5)\n",
    "x3 size:  60\n",
    "dtype:  int32\n",
    "itemsize:  4\n",
    "nbytes: 240\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "#Add code to check if it is true that nbytes is equal to itemsize times size and print true or false depending on the result\n",
    "if x3.nbytes == x3.itemsize*x3.size:\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')\n",
    "    \n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "#More compact way\n",
    "print(x3.nbytes == x3.itemsize*x3.size)\n",
    "\n",
    "\n",
    "True\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "#Write code to print the 1st element of x1, the last element of x1, and to change the second element to 8\n",
    "print('x1 is: ', x1)\n",
    "print('first element of x1 is: ', x1[0])\n",
    "print('last element of x1 is: ', x1[-1])\n",
    "x1[1] = 8\n",
    "\n",
    "print('x1 is now: ', x1)\n",
    "\n",
    "x1 is:  [5 0 3 3 7 9]\n",
    "first element of x1 is:  5\n",
    "last element of x1 is:  9\n",
    "x1 is now:  [5 8 3 3 7 9]\n",
    "\n",
    "\n",
    "\n",
    "Notes: \n",
    "The arry object in Numpy is called ndarray, it provides a lot of supporting functions that make working with ndarray very easy. \n",
    "Data is represented as an array in the ndarray object of\n",
    "NumPy\n",
    "• This object is helpful in many applications such as:\n",
    "• Image processing: digital images can be represented in two\n",
    "dimensional arrays of pixel brightness in an area.\n",
    "• Sound processing: sound clips can be thought of as one- dimensional arrays of intensity vs time.\n",
    "• Text analysis: arrays can be used to indicate frequency of\n",
    "words or pairs of words\n",
    "• The ndarray object is a fixed type array that stores data\n",
    "buffers more efficiently than lists\n",
    "• In arrays, all elements must be of the same numeric type.\n",
    "\n",
    "\n",
    "A NumPy array is a grid of values, all of the same type\n",
    "• The number of dimensions is the rank of the array;\n",
    "• We’ll focus on 1D, 2D and 3D (one-, two- or three-dimensions)\n",
    "• The shape of an array is a tuple of integers giving the size of\n",
    "the array along each dimension.\n",
    "• In one-dimensional arrays, the size of the only dimension is\n",
    "followed by a “, )” because one-element tuple require comma and ).\n",
    "• We can initialize NumPy arrays from nested Python lists or\n",
    "from scratch, and access elements using square brackets\n",
    "\n",
    "\n",
    "Creating Arrays: Three alternatives\n",
    "• From scratch by seeding an array with given set of\n",
    "values\n",
    "• As in the code examples. Good for 1D or 2D arrays.\n",
    "• From scratch by generating an array with specific\n",
    "characteristics\n",
    "• Filled with 0’s or 1’s or the same number\n",
    "• From lists using np.array\n",
    "• ICE: Generate an array from a list of 8 consecutive\n",
    "numbers\n",
    "• Remember the data type restriction (all elements must be\n",
    "of same type), if unsure use dtype to find out\n",
    "• Nested lists will generate multidimensional arrays\n",
    "\n",
    "\n",
    "• Attributes of arrays\n",
    "• Determining the size, shape, memory consumption, and data\n",
    "types of array\n",
    "Indexing of arrays\n",
    "• Getting and setting the value of individual array elements\n",
    "• Slicing of arrays\n",
    "• Getting and setting smaller subarrays within a larger array\n",
    "• Reshaping of arrays\n",
    "• Changing the shape of a given array\n",
    "• Joining and splitting of arrays\n",
    "• Combining multiple arrays into one, and splitting one array\n",
    "into many\n",
    "\n",
    "\n",
    "ndim (the number of dimensions)\n",
    "• shape (the size of each dimension)\n",
    "• size (the total number of elements in the array)\n",
    "• dtype, the data type of the array\n",
    "• itemsize, lists the size (in bytes) of each array\n",
    "element\n",
    "• nbytes, lists the total size (in bytes) of the array\n",
    "• In general, nbytes is equal to itemsize times size.\n",
    "\n",
    "the randint() method takes a size parameter where you can specify the shape of an array\n",
    "\n",
    "Numpy offers random module with generates pseudo random numbers\n",
    "\n",
    "\n",
    "• Arrays are indexed using square brackets (like lists), which\n",
    "allows to access single elements.\n",
    "• In a one-dimensional array:\n",
    "• the ith value is accessed by specifying the desired index in square brackets (counting from zero). For example, x1[0]\n",
    "• To index from the end of the array, negative indices can be used. For\n",
    "example, x1[-1]\n",
    "• In a multidimensional array:\n",
    "• items are accessed using a comma-separated tuple of indices. For example, x2[0,0], x3[1,0,1]\n",
    "• Array values can be modified using any of the above index\n",
    "notation but pay attention to the data type restriction\n",
    "• ICE: Write code to print the 1st element of x1, the last\n",
    "element of x1, and to change the second element to 8\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76c27d82",
   "metadata": {},
   "source": [
    "LC 8:\n",
    "#WebScraping US Presidents Table\n",
    "#PART A - Scraping headers\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.loc.gov/rr/print/list/057_chron.html'\n",
    "\n",
    "r = requests.get(url)\n",
    "src=r.content\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "print(soup.title.text)\n",
    "alltables = soup.find_all('table')\n",
    "\n",
    "Chronological List of Presidents, First Ladies, and Vice Presidents of the United States - Guides, Reference Aids, and Finding Aids (Prints andPhotographs Reading Room, Library of Congress)\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "print(type(alltables))\n",
    "\n",
    "<class 'bs4.element.ResultSet'>\n",
    "\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "usptable = alltables[3]\n",
    "\n",
    "#print(usptable.prettify())\n",
    "#We'll use a list of dictionaries to store the information\n",
    "preslist = []\n",
    "\n",
    "#First, we grab the headers and put them in a list\n",
    "headers = []\n",
    "\n",
    "# Fetch header cells from first row to determine field names\n",
    "for header in usptable.find('tr').find_all('th'):\n",
    "    headers.append(header.text)\n",
    "\n",
    "print(headers)\n",
    "\n",
    "\n",
    "['YEAR', 'PRESIDENT', 'FIRST LADY', 'VICE PRESIDENT']\n",
    "\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "#PART B - Scraping data rows\n",
    "# Then, we grab the rows after the header (i.e. skip the first row with [1:])\n",
    "rows = usptable.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    values = []\n",
    "    # And get the column cells with all td tags\n",
    "    for col in row.find_all('td'):\n",
    "        values.append(col.text)\n",
    "        \n",
    "        #Each president info is stored in a dictionary with the headers and the values\n",
    "    if values:\n",
    "        pres_dict = {}\n",
    "        for i in range(len(values)):\n",
    "            pres_dict[headers[i]] = values[i]\n",
    "        preslist.append(pres_dict)\n",
    "\n",
    "# Show the results\n",
    "for ipres in preslist:\n",
    "    print(ipres)\n",
    "    \n",
    "\n",
    "\n",
    "{'YEAR': '1789-1797', 'PRESIDENT': 'George Washington', 'FIRST LADY': 'Martha Washington', 'VICE PRESIDENT': 'John Adams'}\n",
    "{'YEAR': '1797-1801', 'PRESIDENT': 'John Adams', 'FIRST LADY': 'Abigail Adams', 'VICE PRESIDENT': 'Thomas Jefferson'}\n",
    "{'YEAR': '1801-1805', 'PRESIDENT': 'Thomas Jefferson', 'FIRST LADY': '[Martha Wayles Skelton Jefferson \\n    died before Jefferson assumed office;\\nno image of her in P&P collections]', 'VICE PRESIDENT': 'Aaron Burr'}\n",
    "----------------------------------------------------\n",
    "\n",
    "#PART C - Getting only the names and creating a list\n",
    "name_list = []\n",
    "for ipres in preslist:\n",
    "    name= ipres['PRESIDENT']\n",
    "    print(name)\n",
    "    name_list.append(name)\n",
    "    \n",
    "George Washington\n",
    "John Adams\n",
    "Thomas Jefferson\n",
    "Thomas Jefferson\n",
    "James Madison\n",
    "James Madison\n",
    "James Madison\n",
    "James Madison\n",
    "James Monroe\n",
    "John Quincy Adams\n",
    "Andrew Jackson\n",
    "Andrew Jackson\n",
    "Martin Van Buren\n",
    "William Henry Harrison\n",
    "John Tyler\n",
    "James K. Polk\n",
    "Zachary Taylor\n",
    "...\n",
    "...\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "\n",
    "#PART D - Getting non-dup names in list and verifying count\n",
    "name_list = []\n",
    "for ipres in preslist:\n",
    "    name= ipres['PRESIDENT']\n",
    "    if name not in name_list:\n",
    "        name_list.append(name)\n",
    "print(name_list)\n",
    "print()\n",
    "print('Current president is # ', len(name_list))\n",
    "\n",
    "\n",
    "['George Washington', 'John Adams', 'Thomas Jefferson', 'James Madison', 'James Monroe', 'John Quincy Adams', 'Andrew Jackson', 'Martin Van Buren', 'William Henry Harrison', 'John Tyler', 'James K. Polk', 'Zachary Taylor', 'Millard Fillmore', 'Franklin Pierce', 'James Buchanan', 'Abraham Lincoln', 'Andrew Johnson', 'Ulysses S. Grant', 'Rutherford Birchard Hayes', 'James A. Garfield', 'Chester A. Arthur', 'Grover Cleveland', 'Benjamin Harrison', 'William McKinley', 'Theodore Roosevelt', 'William H. Taft', 'Woodrow Wilson', 'Warren G. Harding', 'Calvin Coolidge', 'Herbert Hoover', 'Franklin D. Roosevelt', 'Harry S. Truman', 'Dwight D. Eisenhower', 'John F. Kennedy', 'Lyndon B. Johnson', 'Richard M. Nixon', 'Gerald R. Ford', 'Jimmy Carter', 'Ronald Reagan', 'George Bush', 'Bill Clinton', 'George W. Bush', 'Barack Obama', 'Donald J. Trump', 'Joseph R. Biden']\n",
    "\n",
    "Current president is #  45\n",
    "--------------------------------------------\n",
    "\n",
    "#PART E - Fixing Cleveland's non-continuous term\n",
    "name_list = []\n",
    "prev_pres = ''\n",
    "for ipres in preslist:\n",
    "    name = ipres['PRESIDENT']\n",
    "    if name != prev_pres:\n",
    "        name_list.append(name)\n",
    "        prev_pres = name\n",
    "print(name_list)   \n",
    "print()\n",
    "print('Current president is # ', len(name_list))\n",
    "\n",
    "['George Washington', 'John Adams', 'Thomas Jefferson', 'James Madison', 'James Monroe', 'John Quincy Adams', 'Andrew Jackson', 'Martin Van Buren', 'William Henry Harrison', 'John Tyler', 'James K. Polk', 'Zachary Taylor', 'Millard Fillmore', 'Franklin Pierce', 'James Buchanan', 'Abraham Lincoln', 'Andrew Johnson', 'Ulysses S. Grant', 'Rutherford Birchard Hayes', 'James A. Garfield', 'Chester A. Arthur', 'Grover Cleveland', 'Benjamin Harrison', 'Grover Cleveland', 'William McKinley', 'Theodore Roosevelt', 'William H. Taft', 'Woodrow Wilson', 'Warren G. Harding', 'Calvin Coolidge', 'Herbert Hoover', 'Franklin D. Roosevelt', 'Harry S. Truman', 'Dwight D. Eisenhower', 'John F. Kennedy', 'Lyndon B. Johnson', 'Richard M. Nixon', 'Gerald R. Ford', 'Jimmy Carter', 'Ronald Reagan', 'George Bush', 'Bill Clinton', 'George W. Bush', 'Barack Obama', 'Donald J. Trump', 'Joseph R. Biden']\n",
    "\n",
    "Current president is #  46\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#PART F - Printing the numbered list with enumerate\n",
    "for i, name in enumerate(name_list, 1):\n",
    "    print(\" %d. %s\" % (i, name))\n",
    "    #print(i, name)\n",
    "    \n",
    "    \n",
    "1. George Washington\n",
    " 2. John Adams\n",
    " 3. Thomas Jefferson\n",
    " 4. James Madison\n",
    " 5. James Monroe\n",
    " 6. John Quincy Adams\n",
    " 7. Andrew Jackson\n",
    " 8. Martin Van Buren\n",
    " 9. William Henry Harrison\n",
    " 10. John Tyler\n",
    " ....\n",
    " 46.\n",
    " \n",
    "----------------------------------\n",
    "\n",
    "A web scraping Python program has three parts:\n",
    "1. Making a request to the website using the requests\n",
    "module\n",
    "2. Using the BeautifulSoup library to get the HTML (raw)\n",
    "data from the website (i.e. “soupify” the page by\n",
    "creating a soup object)\n",
    "3. Extract the raw data needed from the soup object, and\n",
    "store it in a structured format for analysis\n",
    "• Inspect the HTML code on the page and identify the tags that\n",
    "hold the data (<a href>, <h1>…<h6>,<table> <tr> <th> <td>)\n",
    "• Use find or find_all to travel through the soup object along\n",
    "with for loops to scrape the data from the soup object\n",
    "\n",
    "\n",
    "Google search landing page\n",
    "• to scrape <a> link with a specific text\n",
    "• White House briefing room\n",
    "• to scrape <h2> headlines and links\n",
    "• IMDb movies page\n",
    "• Single Table page with well structured tags\n",
    "• Goal: Compute average ratings\n",
    "• US Presidents page\n",
    "• Multi-table page with <tr> for all rows\n",
    "• Goal: Print list of all 46 US Presidents\n",
    "\n",
    "\n",
    "Examples covered in class refer to:\n",
    "• Static (non-interactive) pages\n",
    "• No login required, no sessions, no cookies\n",
    "• What if we want to scrape a:\n",
    "• Page asking for login credentials\n",
    "• We need to use another request method instead of get, we will\n",
    "use .post first (See chp. B4)\n",
    "• Dynamic (interactive) pages with JavaScript\n",
    "• We need to use another library called Selenium and a browser\n",
    "driver (See chp. B5)\n",
    "• Selenium works by automating browsers to load a\n",
    "website, retrieve its contents, and perform actions like a\n",
    "user would when using the browser.\n",
    "\n",
    "\n",
    "                        <table>\n",
    "                        /      \\\n",
    "              \n",
    "                       /        \\\n",
    "                      /         <tbody>\n",
    "                     /\n",
    "                  <thead>         | <tr>\n",
    "                  /     \\\n",
    "                  <th>   <th>     | <td>\n",
    "                  \n",
    " What if…\n",
    "• there is more than one table on the page?\n",
    "• there is no <thead> and <tbody>, only <tr>?                 \n",
    "                  \n",
    "Part I: Connecting with site Retrieving HTML data\n",
    "Part II: Extract headers (th) under thead and store them\n",
    "in a list\n",
    "Part III: Loop through rows (tr) under tbody, extract data\n",
    "in td and store each row in a dictionary\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54214aec",
   "metadata": {},
   "source": [
    "LC 7:\n",
    "\n",
    "#Source: Adapted from FreeCodeCamp.org\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.whitehouse.gov//briefing-room/\")\n",
    "src = r.content\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "#store urls in a Python list called urls and initialized with []\n",
    "urls = []\n",
    "\n",
    "#Find all h2 tags in the page and within each one find the a tags\n",
    "for h2_tag in soup.find_all(\"h2\"):\n",
    "    a_tag=h2_tag.find('a')\n",
    "    #some tags are None and will give an error in the .attrs statement\n",
    "    #print(a_tag)\n",
    "    if a_tag == None:\n",
    "        pass\n",
    "    else:\n",
    "        urls.append(a_tag.attrs['href'])\n",
    "print(urls)\n",
    "print(\"There are \", len(urls), \"links on the page\")\n",
    "\n",
    "\n",
    "\n",
    "['https://www.whitehouse.gov/briefing-room/statements-releases/2022/02/23/statement-by-president-biden-on-nord-stream-2/', 'https://www.whitehouse.gov/briefing-room/statements-releases/2022/02/23/fact-sheet-biden-%e2%81%a0harris-administration-delivering-results-100-days-into-infrastructure-implementation/',..... ]\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "#Source: Adapted from FreeCodeCamp.org\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.whitehouse.gov//briefing-room/\")\n",
    "src = r.content\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "#store urls in a Python list called urls and initialized with []\n",
    "headline_list = []\n",
    "\n",
    "#Find all h2 tags in the page and within each one find the a tags\n",
    "for h2_tag in soup.find_all(\"h2\"):\n",
    "    a_tag=h2_tag.find('a')\n",
    "    #some tags are None and will give an error in the .attrs statement\n",
    "    #print(a_tag)\n",
    "    if a_tag == None:\n",
    "        pass\n",
    "    else:\n",
    "        headline_list.append(a_tag.text)\n",
    "#print(headline_list) prints all at once\n",
    "#To print one at the time\n",
    "print(\"Here are the headlines: \\n \")\n",
    "for e in headline_list:\n",
    "    print(e)\n",
    "print(\"There are \", len(headline_list), \"headlines on the page\")\n",
    "\n",
    "\n",
    "['\\n\\t\\t\\t\\tStatement by President\\xa0Biden on Nord Stream\\xa02\\t\\t\\t', '\\n\\t\\t\\t\\tFact Sheet: Biden-\\u2060Harris Administration Delivering Results 100 Days into Infrastructure\\xa0Implementation\\t\\t\\t', '\\n\\t\\t\\t\\tJoint Statement by President Joseph R. Biden Jr. and Prime Minister Justin Trudeau on the One-Year Anniversary of the Roadmap for a Renewed U.S.-Canada\\xa0Partnership\\t\\t\\t'....]\n",
    "\n",
    "\n",
    "\n",
    "Statement by President Biden on Nord Stream 2\t\t\t\n",
    "\n",
    "\t\t\t\tFact Sheet: Biden-⁠Harris Administration Delivering Results 100 Days into Infrastructure Implementation\t\t\t\n",
    "\n",
    "\t\t\t\tJoint Statement by President Joseph R. Biden Jr. and Prime Minister Justin Trudeau on the One-Year Anniversary of the Roadmap for a Renewed U.S.-Canada Partnership\t\t\t\n",
    "\n",
    "\t\t\t\tPresident Joseph R. Biden, Jr. Approves Iowa Disaster Declaration\t\t\t\n",
    "\n",
    "\t\t\t\tPresident Joseph R. Biden, Jr. Approves Nebraska Disaster Declaration\t\t\t\n",
    "\n",
    "\t\t\t\tA Letter to the Speaker of the House of Representatives and the President of the Senate, on the Continuation of the National Emergency with Respect to Cuba and of the Emergency Authority Relating to the Regulation of the Anchorage and Movement of Vessels\t\t\t\n",
    "\n",
    "\t\t\t\tNotice on on the Continuation of the National Emergency with Respect to Cuba and of the Emergency Authority Relating to the Regulation of the Anchorage and Movement of Vessels\t\t\t\n",
    "\n",
    "\t\t\t\tBills Signed: H.R. 1281\t\t\t\n",
    "\n",
    "\t\t\t\tPress Briefing by Press Secretary Jen Psaki and Deputy National Security Advisor for International Economics and Deputy NEC Director Daleep Singh, February 22, 2022\t\t\t\n",
    "\n",
    "\t\t\t\tBills Signed: S. 566 and S. 583\t\t\t\n",
    "There are  10 headlines on the page\n",
    "\n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "\n",
    "# Web scraping IMDb table\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "print('This is the title:', soup.title.text)\n",
    "mo_table = soup.find('table')\n",
    "#print(mo_table)\n",
    "\n",
    "#get headers and put them in a list\n",
    "headers = []\n",
    "#Go through first row and get headers\n",
    "for header in mo_table.find(\"tr\").find_all('th'):\n",
    "    headers.append(header.text)\n",
    "print('These are the headers: \\n', headers)\n",
    "\n",
    "\n",
    "This is the title: Top 250 Movies - IMDb\n",
    "These are the headers: \n",
    " ['', 'Rank & Title', 'IMDb Rating', 'Your Rating', '']\n",
    " \n",
    "------------------------------------\n",
    "\n",
    "#Movies is a ist of dictionaries that keeps table info\n",
    "movies = []\n",
    "\n",
    "#There are three nested loops below\n",
    "#First for loop goes through rows except first one\n",
    "for row in mo_table.find_all('tr')[1:]:\n",
    "    values = [] #list to hold row values in each row\n",
    "    #Second for loop goes through columns\n",
    "    for col in row.find_all('td'):\n",
    "        values.append(col.text)\n",
    "    if values: #Not empty - skips empty values\n",
    "        mo_dict = {} #Empty dictionary to store row values\n",
    "        #Third for loop connects values to labels from headers\n",
    "        for i in range(len(values)):\n",
    "            mo_dict[headers[i]]=values[i]\n",
    "    movies.append(mo_dict) #adds dict to movies list\n",
    "print(movies)\n",
    "print(\"The number of movies is: \", len(movies))\n",
    "\n",
    "\n",
    "[{'': '\\n\\n', 'Rank & Title': '\\n      1.\\n      The Shawshank Redemption\\n(1994)\\n', 'IMDb Rating': '\\n9.2\\n', 'Your Rating': '\\n\\n\\n\\n\\xa012345678910 \\n\\n\\n\\nNOT YET RELEASED\\n \\n\\nSeen\\n\\n\\n'}, {'': '\\n\\n', 'Rank & Title': '\\n      2.\\n      The Godfather\\n(1972)\\n', 'IMDb Rating': '\\n9.1\\n', 'Your Rating': '\\n\\n\\n\\n\\xa012345678910 \\n\\n\\n\\nNOT YET RELEASED\\n \\n\\nSeen\\n\\n\\n'}, {'': '\\n\\n', 'Rank & Title': '\\n      3.\\n      The Godfather: Part II\\n(1974)\\n', 'IMDb Rating': '\\n9.0\\n', 'Your Rating': '\\n\\n\\n\\n\\xa012345678910 \\n\\n\\n\\nNOT YET RELEASED\\n \\n\\nSeen\\n\\n\\n'}, {'': '\\n\\n', 'Rank & Title': '\\n      4.\\n      The Dark Knight\\n(2008)\\n', 'IMDb Rating': '\\n9.0\\n',...}]\n",
    "There are this many movies:  250\n",
    "=======================================================\n",
    "\n",
    "\n",
    "#Now, we will calculate the average ratings for all the movies\n",
    "trt = 0  #Total ratings trt is initialized to zero\n",
    "#Go through each row and extract the rating\n",
    "for movie in movies:\n",
    "    #rt is the rating for the current movie\n",
    "    #replace removes formatting characters\n",
    "    rt=movie['IMDb Rating'].replace('\\n','')\n",
    "    trt+= float(rt) #accumulates rating after converting to float\n",
    "    \n",
    "#To calculate the average, divide total by number of movies\n",
    "rt_avg= trt/len(movies)\n",
    "print('The average rating of all the movies is: ', rt_avg)\n",
    "\n",
    "\n",
    "The average rating of all the movies is:  8.261199999999972\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "#To format the average to two decimals, use f in print\n",
    "format_rtavg = \"{:.2f}\".format(rt_avg)\n",
    "print('The average rating of all the movies with only two decimals is: ', format_rtavg)\n",
    "\n",
    "\n",
    "The average rating of all the movies with only two decimals is:  8.26\n",
    "\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "Notes: \n",
    "\n",
    "A web scraping Python program has three parts:\n",
    "1. Making a request to the website using the requests library\n",
    "2. Using the BeautifulSoup library to get the HTML (raw) data\n",
    "from the website.\n",
    "3. Convert the raw data into a structured format for analysis\n",
    "\n",
    "First part of program:\n",
    "Connecting with site\n",
    "Retrieving HTML data\n",
    "Second Part:\n",
    "Extracting headers <th>\n",
    "in first row (.find(‘tr’)) and storing\n",
    "Them in a list of headers\n",
    "Third part:\n",
    "Go through the remaining rows\n",
    "(.find_all(‘tr’)), and get the data\n",
    "stored in the td tags\n",
    "We need a nested data structure:\n",
    "Movies is a list of dictionaries mo_dict\n",
    "Values temporarily holds the value in each row\n",
    "\n",
    "\n",
    "\n",
    "Two levels (outer and inner) and there are data\n",
    "structures in each level\n",
    "1. List of lists\n",
    "• Example list of teams\n",
    "2. Dictionary of Dictionaries\n",
    "• Employee information\n",
    "3. List of dictionaries\n",
    "• List of contacts\n",
    "4. Dictionaries with embedded lists\n",
    "• Description of species (animals, plants)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42d989f1",
   "metadata": {},
   "source": [
    "LC 6: \n",
    "\n",
    "#Ex. 1: Google Landing Page \n",
    "#Adapted from FreeCampCode.org\n",
    "\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"http://www.google.com\")\n",
    "print(r.status_code)\n",
    "\n",
    "200\n",
    "\n",
    "=======================================\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://www.google.com\")\n",
    "src=r.content\n",
    "soup = BeautifulSoup(src,'html.parser')\n",
    "soup\n",
    "\n",
    "!DOCTYPE html>\n",
    "<html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en\"><head><meta content=\"Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\" name=\"description\"/><meta content=\"noodp\" name=\"robots\"/><meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"/><title>.....\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "print('This is the soup object: \\n', soup)\n",
    "\n",
    "\n",
    "This is the soup object: \n",
    " <!DOCTYPE html>\n",
    "<html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en\"><head><meta content=\"Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\" name=\"description\"/><meta content=\"noodp\" name=\"robots\"/><meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"/><title>\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "links=soup.find_all(\"a\")\n",
    "#Print links to see that looks like a list\n",
    "print(links)\n",
    "\n",
    "\n",
    "[<a class=\"gb1\" href=\"http://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>, <a class=\"gb1\" href=\"http://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>,...........\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "print(type(links))\n",
    "\n",
    "\n",
    "<class 'bs4.element.ResultSet'>\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Calculate the number of links with len\n",
    "count=len(links)\n",
    "print('There are a total of ', count, \"links in this page\")\n",
    "\n",
    "\n",
    "There are a total of  18 links in this page\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\n",
    "#Print each link separately\n",
    "for link in links:\n",
    "    print(link)\n",
    "    \n",
    "<a class=\"gb1\" href=\"http://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>\n",
    "<a class=\"gb1\" href=\"http://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>\n",
    "<a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>\n",
    "<a class=\"gb1\" href=\"http://www.youtube.com/?gl=US&amp;tab=w1\">YouTube</a>\n",
    "<a class=\"gb1\" href=\"https://news.google.com/?tab=wn\">News</a>\n",
    ".......\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "#Print each link separately\n",
    "for link in links:\n",
    "    print(link.attrs['href'])\n",
    "    \n",
    "http://www.google.com/imghp?hl=en&tab=wi\n",
    "http://maps.google.com/maps?hl=en&tab=wl\n",
    "https://play.google.com/?hl=en&tab=w8\n",
    "http://www.youtube.com/?gl=US&tab=w1\n",
    "https://news.google.com/?tab=wn\n",
    "https://mail.google.com/mail/?tab=wm\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "\n",
    "#Find the link with About in the text and print its url\n",
    "for link in links:\n",
    "    if \"About\" in link.text:\n",
    "        print(link.attrs['href'])\n",
    "\n",
    "\n",
    "\n",
    "/intl/en/about.html\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "#Ex.2: WhiteHouse Briefing Room Scraping \n",
    "#Source: Adapted from FreeCodeCamp.org\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.whitehouse.gov//briefing-room/\")\n",
    "src=r.content\n",
    "soup = BeautifulSoup(src,'html.parser')\n",
    "\n",
    "#store urls in a Python list called urls and initialized with []\n",
    "urls = []\n",
    "\n",
    "lh2tags = soup.find_all(\"h2\")\n",
    "\n",
    "#Find all h2 tags in the page and within each one find the a tags\n",
    "# e is an element in the lh2tag list and allows to iterate through the list, one element at the time\n",
    "# each e element is an h2 tag in the page, with a tags with links (called h2_tag in the screenshots in the slides)\n",
    "for e in lh2tags:\n",
    "    a_tag=e.find('a')\n",
    "    #some tags are None and will give an error in the .attrs statement\n",
    "    #print(a_tag)\n",
    "    if a_tag ==None:\n",
    "        pass\n",
    "    else: \n",
    "        print(a_tag.attrs['href'])\n",
    "        urls.append(a_tag.attrs['href'])\n",
    "        \n",
    "https://www.whitehouse.gov/briefing-room/press-briefings/2022/02/16/press-briefing-by-white-house-covid-19-response-team-and-public-health-officials-83/\n",
    "https://www.whitehouse.gov/briefing-room/press-briefings/2022/02/15/press-briefing-by-press-secretary-jen-psaki-february-15-2022/\n",
    "https://www.whitehouse.gov/briefing-room/statements-releases/2022/02/15/statement-by-nsc-spokesperson-emily-horne-on-national-security-advisor-jake-sullivans-call-with-president-of-european-council-frederic-bernard/\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "print(urls)\n",
    "\n",
    "\n",
    "\n",
    "['https://www.whitehouse.gov/briefing-room/press-briefings/2022/02/16/press-briefing-by-white-house-covid-19-response-team-and-public-health-officials-83/', 'https://www.whitehouse.gov/briefing-room/press-briefings/2022/02/15/press-briefing-by-press-secretary-jen-psaki-february-15-2022/', 'https://www.whitehouse.gov/briefing-room/statements-releases/2022/02/15/statement-by-nsc-spokesperson-emily-horne-on-national-security-advisor-jake-sullivans-call-with-president-of-european-council-frederic-bernard/',\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "#Ex. 3: Web scraping IMDb table\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('http://www.imdb.com/chart/top')\n",
    "soup = BeautifulSoup(r.content,'html.parser')\n",
    "print(soup.title.text)\n",
    "\n",
    "\n",
    "Top 250 Movies - IMDb\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "mo_table = soup.find('table')\n",
    "print(mo_table)\n",
    "\n",
    "\n",
    "table class=\"chart full-width\" data-caller-name=\"chart-top250movie\">\n",
    "<colgroup>\n",
    "<col class=\"chartTableColumnPoster\"/>\n",
    "<col class=\"chartTableColumnTitle\"/>\n",
    "<col class=\"chartTableColumnIMDbRating\"/>\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "\n",
    "HTML attributes\n",
    "\n",
    "all HTML elements have attributes\n",
    "\n",
    "attributes provide additional information about elements\n",
    "\n",
    "attributes are always specified in the start tag\n",
    "\n",
    "attributes usually come in name/value pairs like: name = 'value'\n",
    "\n",
    "\n",
    "The <a> tag defines a hyperlink. The href attribute specifies the URL of the page the link goes to\n",
    "\n",
    "Find methods return Tag objects, special objects in bs4.\n",
    "The type of the objects can be checked with print(type(…))\n",
    "Find results a single object, whereas find_all returns a list-like object\n",
    "Tag objects have the following attributes\n",
    "name attribute retrieves the tag name.\n",
    "contents attribute gets a Python list containing the tag’s children (its direct descendant tags) as a list.\n",
    "children attribute does the same as contents but provides an iterator instead; the descendants attribute also returns an iterator, now including all the tag’s descendants in a recursive manner.\n",
    "text attribute gets the contents of the Tag object as clear text (without HTML tags).\n",
    "Note that not all find and find_all searches need to start from the original BeautifulSoup objects. \n",
    "Every Tag object itself can be used as a new root from which new searches can be started.\n",
    "Therefore, find and find_all can be connected in the same line of code and separated with periods.\n",
    "\n",
    "Parsers convert the input into a graph or a tree structure for processing.\n",
    "The default parser is 'lxml' which is lenient but platform dependent.\n",
    "We will use ‘html.parser’,  which is part of BeautifulSoup.\n",
    "\n",
    "\n",
    "whitehouse example\n",
    "\n",
    "Visit the page\n",
    "\n",
    "Decide what to scrape\n",
    "Headline links in blue\n",
    "Inspect the source code\n",
    "Look at the html source code of the page\n",
    "Inspect the element using Chrome Developer tools\n",
    "Store headline links in a list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01378ebe",
   "metadata": {},
   "source": [
    "LC 5: \n",
    "\n",
    "#Ex.1 - Familiarization with request object\n",
    "\n",
    "import requests\n",
    "#url = 'http://www.webscrapingfordatascience.com/basichttp/'\n",
    "url='https://web.ics.purdue.edu/~gchopra/class/public/pages/webdesign/05_simple.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "print('Status code from the server: ', r.status_code)\n",
    "\n",
    "print('Reason or text associated with status code: ', r.reason)\n",
    "\n",
    "print('HTTP Response headers: \\n', r.headers)\n",
    "\n",
    "print('request information saved as a Python object in r.request: \\n', r.request)\n",
    "\n",
    "print('HTTP request headers: \\n', r.request.headers)\n",
    "\n",
    "print('HTTP request text in string format: \\n', r.text)\n",
    "\n",
    "print('HTTP request content in bytes: \\n', r.content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Status code from the server:  200\n",
    "Reason or text associated with status code:  OK\n",
    "HTTP Response headers: \n",
    " {'Date': 'Mon, 14 Feb 2022 19:42:42 GMT', 'Server': 'Apache/2.4.6 () OpenSSL/1.0.2k-fips', 'Last-Modified': 'Wed, 13 Jan 2016 09:47:41 GMT', 'ETag': '\"70f-529340fa7b28d\"', 'Accept-Ranges': 'bytes', 'Content-Length': '1807', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html', 'Strict-Transport-Security': 'max-age=16070400', 'X-Content-Type-Options': 'nosniff', 'X-XSS-Protection': '1; mode=block', 'X-Frame-Options': 'SAMEORIGIN'}\n",
    "request information saved as a Python object in r.request: \n",
    " <PreparedRequest [GET]>\n",
    "HTTP request headers: \n",
    " {'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
    "HTTP request text in string format: \n",
    " <html>\n",
    "\n",
    "<head>\n",
    "<title>A very simple webpage</title>\n",
    "<basefont size=4>\n",
    "</head>\n",
    "\n",
    "<body bgcolor=FFFFFF>\n",
    "\n",
    "<h1>A very simple webpage. This is an \"h1\" level header.</h1>\n",
    "\n",
    "<h2>This is a level h2 header.</h2>\n",
    "\n",
    "<h6>This is a level h6 header.  Pretty small!</h6>\n",
    "\n",
    "<p>This is a standard paragraph.</p>\n",
    "\n",
    "<p align=center>Now I've aligned it in the center of the screen.</p>\n",
    "\n",
    "<p align=right>Now aligned to the right</p>\n",
    "\n",
    "<p><b>Bold text</b></p>\n",
    "\n",
    "<p><strong>Strongly emphasized text</strong>  Can you tell the difference vs. bold?</p>\n",
    "\n",
    "<p><i>Italics</i></p>\n",
    "\n",
    "<p><em>Emphasized text</em>  Just like Italics!</p>\n",
    "\n",
    "<p>Here is a pretty picture: <img src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\n",
    "\n",
    "<p>Same thing, aligned differently to the paragraph: <img align=top src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2>How about a nice ordered list!</h2>\n",
    "<ol>\n",
    "  <li>This little piggy went to market\n",
    "  <li>This little piggy went to SB228 class\n",
    "  <li>This little piggy went to an expensive restaurant in Downtown Palo Alto\n",
    "  <li>This little piggy ate too much at Indian Buffet.\n",
    "  <li>This little piggy got lost\n",
    "</ol>\n",
    "\n",
    "<h2>Unordered list</h2>\n",
    "<ul>\n",
    "  <li>First element\n",
    "  <li>Second element\n",
    "  <li>Third element\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2>Nested Lists!</h2>\n",
    "<ul>\n",
    "  <li>Things to to today:\n",
    "    <ol>\n",
    "      <li>Walk the dog\n",
    "      <li>Feed the cat\n",
    "      <li>Mow the lawn\n",
    "    </ol>\n",
    "  <li>Things to do tomorrow:\n",
    "    <ol>\n",
    "      <li>Lunch with mom\n",
    "      <li>Feed the hamster\n",
    "      <li>Clean kitchen\n",
    "    </ol>\n",
    "</ul>\n",
    "\n",
    "<p>And finally, how about some <a href=http://www.yahoo.com/>Links?</a></p>\n",
    "\n",
    "<p>Or let's just link to <a href=../../index.html>another page on this server</a></p>\n",
    "\n",
    "<p>Remember, you can view the HTMl code from this or any other page by using the \"View Page Source\" command of your browser.</p>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\n",
    "HTTP request content in bytes: \n",
    " b'<html>\\n\\n<head>\\n<title>A very simple webpage</title>\\n<basefont size=4>\\n</head>\\n\\n<body bgcolor=FFFFFF>\\n\\n<h1>A very simple webpage. This is an \"h1\" level header.</h1>\\n\\n<h2>This is a level h2 header.</h2>\\n\\n<h6>This is a level h6 header.  Pretty small!</h6>\\n\\n<p>This is a standard paragraph.</p>\\n\\n<p align=center>Now I\\'ve aligned it in the center of the screen.</p>\\n\\n<p align=right>Now aligned to the right</p>\\n\\n<p><b>Bold text</b></p>\\n\\n<p><strong>Strongly emphasized text</strong>  Can you tell the difference vs. bold?</p>\\n\\n<p><i>Italics</i></p>\\n\\n<p><em>Emphasized text</em>  Just like Italics!</p>\\n\\n<p>Here is a pretty picture: <img src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\\n\\n<p>Same thing, aligned differently to the paragraph: <img align=top src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\\n\\n<hr>\\n\\n<h2>How about a nice ordered list!</h2>\\n<ol>\\n  <li>This little piggy went to market\\n  <li>This little piggy went to SB228 class\\n  <li>This little piggy went to an expensive restaurant in Downtown Palo Alto\\n  <li>This little piggy ate too much at Indian Buffet.\\n  <li>This little piggy got lost\\n</ol>\\n\\n<h2>Unordered list</h2>\\n<ul>\\n  <li>First element\\n  <li>Second element\\n  <li>Third element\\n</ul>\\n\\n<hr>\\n\\n<h2>Nested Lists!</h2>\\n<ul>\\n  <li>Things to to today:\\n    <ol>\\n      <li>Walk the dog\\n      <li>Feed the cat\\n      <li>Mow the lawn\\n    </ol>\\n  <li>Things to do tomorrow:\\n    <ol>\\n      <li>Lunch with mom\\n      <li>Feed the hamster\\n      <li>Clean kitchen\\n    </ol>\\n</ul>\\n\\n<p>And finally, how about some <a href=http://www.yahoo.com/>Links?</a></p>\\n\\n<p>Or let\\'s just link to <a href=../../index.html>another page on this server</a></p>\\n\\n<p>Remember, you can view the HTMl code from this or any other page by using the \"View Page Source\" command of your browser.</p>\\n\\n</body>\\n\\n</html>\\n'\n",
    " \n",
    " --------------------------------------------------------\n",
    " \n",
    "\n",
    "%%HTML\n",
    "<h1> This is heading 1 </h1>\n",
    "<h2> This is heading 2 </h2>\n",
    "\n",
    "This is heading 1 (h1 is the bigger header) \n",
    "This is heading 2 (slightly smaller header) \n",
    "smallest header is h6\n",
    "\n",
    "\n",
    "%%HTML\n",
    "<a href='www.baruch.cuny.edu'> My Baruch Homepage Link </a>\n",
    "\n",
    "\n",
    "My Baruch Homepage Link\n",
    "\n",
    ".........................\n",
    "\n",
    "\n",
    "%%HTML\n",
    "<img src=\"https://toolkit.baruch.cuny.edu/wp-content/uploads/sites/11/2020/03/Baruch-College-stacked-Logo.jpg\">\n",
    "\n",
    "img src\n",
    "--------------------------------------------\n",
    "\n",
    "\n",
    "%%HTML\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> NameCol 1 </th>\n",
    "        <th> NameCol 2 </th>\n",
    "\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> row 1, col 1 </td>\n",
    "        <td> row 1, col 2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Row 2, col 1 </td>\n",
    "        <td> Row 2, col 2 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<!comment\n",
    "\n",
    "   NameCol 1\tNameCol 2\n",
    "row 1, col 1\trow 1, col 2\n",
    "Row 2, col 1\tRow 2, col 2\n",
    "comment\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Ex.4- HTML source in a Python variable called html_doc\n",
    "\n",
    "#Source:http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_doc)\n",
    "print(soup.prettify())\n",
    "\n",
    "\n",
    "<html>\n",
    " <head>\n",
    "  <title>\n",
    "   The Dormouse's story\n",
    "  </title>\n",
    " </head>\n",
    " <body>\n",
    "  <p class=\"title\">\n",
    "   <b>\n",
    "    The Dormouse's story\n",
    "   </b>\n",
    "  </p>\n",
    "  <p class=\"story\">\n",
    "   Once upon a time there were three little sisters; and their names were\n",
    "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
    "    Elsie\n",
    "   </a>\n",
    "   ,\n",
    "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
    "    Lacie\n",
    "   </a>\n",
    "   and\n",
    "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
    "    Tillie\n",
    "   </a>\n",
    "   ;\n",
    "and they lived at the bottom of a well.\n",
    "  </p>\n",
    "  <p class=\"story\">\n",
    "   ...\n",
    "  </p>\n",
    " </body>\n",
    "</html>\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "print('This is soup.title: \\n', soup.title)\n",
    "\n",
    "This is soup.title: \n",
    " <title>The Dormouse's story</title>\n",
    " \n",
    "-------------------------------------------------------\n",
    "\n",
    "print('This is soup.title.name: \\n', soup.title.text)\n",
    "\n",
    "\n",
    "This is soup.title.name: \n",
    " The Dormouse's story\n",
    " \n",
    "--------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('This is the p tag in soup :\\n', soup.p)\n",
    "\n",
    "\n",
    "This is the p tag in soup :\n",
    " <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "print('This is the a tag in soup :\\n', soup.a)\n",
    "\n",
    "\n",
    "This is the a tag in soup :\n",
    " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    " \n",
    "----------------------------------------------\n",
    "\n",
    "print(soup.find_all('a'))\n",
    "\n",
    "\n",
    "\n",
    "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "AllaTags = soup.find_all('a')\n",
    "print(AllaTags)\n",
    "\n",
    "\n",
    "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "print(type(AllaTags))\n",
    "\n",
    "\n",
    "<class 'bs4.element.ResultSet'>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "print(AllaTags[1])\n",
    "\n",
    "\n",
    "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "\n",
    "print(soup.find(id='link3'))\n",
    "\n",
    "\n",
    "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "print(soup.find(id='link2'))\n",
    "\n",
    "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "\n",
    "Repeated errors like 404 (Not Found), 403\n",
    "(Forbidden), 408 (Request Timeout) might\n",
    "indicate that you got blocked.\n",
    "\n",
    "\n",
    "\n",
    "Note: r.content returns the data in bytes\n",
    "and r.text returns the data in string\n",
    "format\n",
    "\n",
    "\n",
    "HTML is the standard markup language for creating\n",
    "web pages.\n",
    "• Provides the building blocks to provide structure\n",
    "and formatting to documents through “tags.”\n",
    "• HTML tags often come in pairs and are enclosed in\n",
    "angled brackets:“<tagname>” is the opening tag and\n",
    "“</tagname>” indicating the closing tag.\n",
    "• Tags can be nested, come in pairs, and may have\n",
    "attributes.\n",
    "\n",
    "\n",
    "Another Python library is needed to parse the html source\n",
    "code of web pages and to extract the information needed.\n",
    "• In Python, BeautifulSoup tries to organize complexity: it\n",
    "helps to parse, structure and organize the oftentimes very\n",
    "messy web by taking HTML and delivering with an easy-towork-with Python structure (a soup object).\n",
    "\n",
    "\n",
    "\n",
    "• To navigate the “soup” structure:\n",
    "• Use the html tags (e.g.: soup.title, soup.p, soup.a)\n",
    "\n",
    "\n",
    "\n",
    "Beautiful Soup’s main task is to take HTML content and transform it into\n",
    "a tree-based representation.\n",
    "• Once created, a BeautifulSoup object has two methods to fetch (filter)\n",
    "data from the page:\n",
    "find(name, attrs, recursive, string, **keywords)\n",
    "find_all(name, attrs, recursive, string, limit, **keywords)\n",
    "• The limit argument is only used in the find_all method and can be used\n",
    "to limit the number of elements that are retrieved.\n",
    "• Both find and find_all can be used to:\n",
    "• Access the name attribute to retrieve the tag name.\n",
    "• Access the contents attribute to get a Python list containing the tag’s children (its\n",
    "direct descendant tags) as a list."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea439585",
   "metadata": {},
   "source": [
    "LC 4: \n",
    "#Ex. 1: Age Calculations (Step 1)\n",
    "#This program calculates a person's age given his/her date of birth\n",
    "\n",
    "import datetime\n",
    "#dobirth_str = input(\"Enter your date of birth in this format mm/dd/yyyy: \")\n",
    "#To avoid asking for the same input many times, we will just assign the dobirth to the variable\n",
    "dobirth_str = '02/14/2001'\n",
    "\n",
    "print(dobirth_str)\n",
    "\n",
    "02/14/2001\n",
    "\n",
    "------------------------------------------------------------\n",
    "#We will use the datetime module to parse the string and extract the year\n",
    "#Note that datetime is twice because the first instance is for the module, and the second instance is for the class\n",
    "dobirth = datetime.datetime.strptime(dobirth_str, '%m/%d/%Y')\n",
    "#dobirth\n",
    "\n",
    "#Today's date is obtained from the datetime module, date class, today method\n",
    "today = datetime.date.today()\n",
    "\n",
    "#today\n",
    "--------------------------------------------------------------\n",
    "#Simple age calculation subtracting years\n",
    "age = today.year - dobirth.year\n",
    "#Printing the result\n",
    "print(\"Your age is: \", age)\n",
    "\n",
    "\n",
    "Your age is:  21\n",
    "--------------------------------\n",
    "\n",
    "#Ex. 2: Age Calculations (Step 2)\n",
    "#continuation from code above\n",
    "if today.month < dobirth.month:\n",
    "    age -= 1\n",
    "print(\"Your age is: \",age)\n",
    "\n",
    "\n",
    "Your age is:  21\n",
    "\n",
    "-------------------------------------------\n",
    "#continuation from code above\n",
    "if today.month < dobirth.month:\n",
    "    age -= 1\n",
    "elif today.month == dobirth.month:\n",
    "    if today.day < dobirth.day:\n",
    "        age -= 1\n",
    "    elif today.day == dobirth.day:\n",
    "        print (\"Happy Birthday\")\n",
    "print(\"Your age is: \",age)\n",
    "\n",
    "\n",
    "Your age is:  20\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "#Age Calculations with Function (Step 3)\n",
    "import datetime\n",
    "\n",
    "def CalculateAge(dobirth):\n",
    "    dobirth = datetime.datetime.strptime(dobirth_str, '%m/%d/%Y')\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    age = today.year - dobirth.year\n",
    "    if today.month < dobirth.month:\n",
    "        age -= 1\n",
    "    elif today.month == dobirth.month:\n",
    "        if today.day < dobirth.day:\n",
    "            age -= 1\n",
    "        elif today.day == dobirth.day:\n",
    "            print (\"Happy Birthday\")\n",
    "    return(age)\n",
    "\n",
    "\n",
    "no OP \n",
    "\n",
    "\n",
    "#This is the program\n",
    "#dobirth_str = str(input(\"Enter your date of birth in this format mm/dd/yyyy: \"))\n",
    "dobirth_str = '2/9/2001'\n",
    "a = CalculateAge(dobirth_str)\n",
    "print(\"Your age is: \",a)\n",
    "\n",
    "\n",
    "Happy Birthday\n",
    "Your age is:  21\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "\n",
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/basichttp/'\n",
    "r= requests.get(url)\n",
    "print(r.text)\n",
    "\n",
    "Hello from the web!\n",
    "\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "print(r.content)\n",
    "\n",
    "\n",
    "b'Hello from the web!\\r\\n'\n",
    "\n",
    "---------------------------------------------\n",
    "\n",
    "Notes: \n",
    "\n",
    "Refine your code to calculate the exact age given the\n",
    "current month.\n",
    "• The person may (or may not have) had his/her birthday\n",
    "this year, as of today and the age result should reflect\n",
    "that.\n",
    "Correcting for month with an if:\n",
    "Correcting for month and day with embedded ifs:\n",
    "\n",
    "Web “scraping” is also called “web harvesting,”\n",
    "“web data extraction,” or “web data mining”\n",
    "• It can be defined as “the construction of an agent\n",
    "to download, parse, and organize data from the\n",
    "web in an automated manner.”\n",
    "• Web scraping gives you structured data from any\n",
    "public website\n",
    "\n",
    "\n",
    "Retrieve data displayed on a web page to perform some statistical\n",
    "analyses.\n",
    "• Get a list of reviews (products, movies, restaurants, etc.) to perform text\n",
    "mining, create a recommendation engine, or build a predictive model to\n",
    "spot fake reviews.\n",
    "• Obtain a listing of properties on a real-estate site to build an appealing\n",
    "geo-visualization.\n",
    "• Gather additional features (e. g. weather information) to enrich a data\n",
    "set based on information found on the web.\n",
    "• Doing social network analytics using profile data found on a web forum.\n",
    "• Monitor a news site for trending new stories on a particular topic of\n",
    "interest.\n",
    "\n",
    "HyperText Transfer Protocol (HTTP) is a set of protocols that\n",
    "handle the requests of web pages.\n",
    "• For example, when you enter “www.baruch.cuny.edu” into your web\n",
    "browser, its first step is to figure out the correct IP (Internet Protocol) address\n",
    "using another protocol, called DNS (Domain Name System). See figure below.\n",
    "• The goals is to map “www.baruch.cuny.edu” to its IP address\n",
    "“150.210.155.246”\n",
    "• Your browser can now establish a connection to Baruch’s web server, which\n",
    "sends back an HTTP reply with the contents of the page requested.\n",
    "• Typically, this textual content is formatted using HTML (HyperText Markup\n",
    "Language)\n",
    "\n",
    "where would you insert the lambda function in s.apply() \n",
    "it would be the first parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33fd8662",
   "metadata": {},
   "source": [
    "LC 3:\n",
    "\n",
    "#Ex. 1: Handling Strings\n",
    "UserName = input(\"Your name?: \")\n",
    "\n",
    "Your name?: Raquel\n",
    "\n",
    "---------------------------------------\n",
    "print(\"Hello \" + UserName )\n",
    "\n",
    "Hello Raquel\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "c = \"CIS3120: Programming for Analytics in Python\"\n",
    "print('the length of the string is: ', len(c))\n",
    "print('the character in index i is: ', c[1])\n",
    "\n",
    "\n",
    "the length of the string is:  44\n",
    "the character in index i is:  I\n",
    "----------------------------------------------------\n",
    "\n",
    "cname = c[:7]\n",
    "print(cname)\n",
    "\n",
    "CIS3120\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "ctitle = c[9:]\n",
    "print(ctitle)\n",
    "\n",
    "\n",
    "Programming for Analytics in Python\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "lang = c[:-10]\n",
    "print(lang)\n",
    "\n",
    "CIS3120: Programming for Analytics\n",
    "\n",
    "---------------------------------------------\n",
    "\n",
    "#print(c.replace(\":\", \"-\"))\n",
    "\n",
    "print(c.center(52,\"*\"))\n",
    "print(len(c))\n",
    "\n",
    "****CIS3120: Programming for Analytics in Python****\n",
    "44\n",
    "1\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "print(c)\n",
    "c1= c.replace(\"in \",\"with \")\n",
    "print(c1)\n",
    "\n",
    "\n",
    "\n",
    "CIS3120: Programming for Analytics with Python\n",
    "CIS3120: Programming for Analytics with Python\n",
    "--------------------------------------------------------------\n",
    "\n",
    "c= \"CIS3120: Programming for Analytics in Python\"\n",
    "print(c.split())\n",
    "\n",
    "['CIS3120:', 'Programming', 'for', 'Analytics', 'in', 'Python']\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "\n",
    "SCALAR TYPES • Numeric: int, float • Logical: Boolean • String Literals • Special: None\n",
    "2. BUILT\n",
    "-IN DATA STRUCTURES\n",
    "• Tuple • List • Dictionary • Set\n",
    "3. CONTROL FLOW • IF, ELIF, ELSE • FOR Loops and RANGE • WHILE Loops and PASS\n",
    "4. DEFINING FUNCTIONS\n",
    "5. IMPORTING LIBRARIES or MODULES\n",
    "\n",
    "\n",
    "Tuple\n",
    "• A tuple is a fixed-length, immutable sequence of Python objects.\n",
    "It is created with a comma-separated sequence of values.\n",
    "• To define tuples in more complicated expressions, it is often\n",
    "necessary to enclose the values in parentheses ().\n",
    "List\n",
    "• lists are variable-length, and their contents can be modified inplace.\n",
    "• A list is defined using square brackets [] or using the list type\n",
    "function.\n",
    "• Lists can be accessed using the index, and sliced using start:stop\n",
    "passed to the indexing operator []\n",
    "\n",
    "\n",
    "\n",
    "Dictionary\n",
    "• It is a flexibly sized collection of key-value pairs, where key\n",
    "and value are Python objects.\n",
    "• Dicts can be created by using curly braces {} and colons to\n",
    "separate keys and values\n",
    "Set\n",
    "• A set is an unordered collection of unique elements. Are\n",
    "similar to dicts, but with keys only, no values.\n",
    "• Can be created via the set function or via literal with curly\n",
    "braces\n",
    "\n",
    "\n",
    "IF, ELIF, AND ELSE\n",
    "• The if statement is one of the most well-known control\n",
    "flow statement types. It checks a condition that, if True,\n",
    "evaluates the code in the block that follows\n",
    "• An if statement can be optionally followed by one or\n",
    "more elif blocks and a catch-all else block if all of the\n",
    "conditions are False\n",
    "TERNARY EXPRESSIONS\n",
    "• A ternary expression in Python allows you to combine an\n",
    "if-else block that produces a value into a single line or\n",
    "expression.\n",
    "\n",
    "\n",
    "FOR LOOPS\n",
    "• “for loops” are for iterating over a collection (like a list or tuple) or\n",
    "an iterable.\n",
    "• You can advance a for loop to the next iteration, skipping the\n",
    "remainder of the block, using the continue keyword.\n",
    "• A for loop can be exited altogether with the break keyword.\n",
    "RANGE\n",
    "• The range function returns an iterator that yields a sequence of\n",
    "evenly spaced integers\n",
    "• Range produces integers up to but not including the endpoint. A\n",
    "common use of range is for iterating through sequences by index\n",
    "\n",
    "\n",
    "\n",
    "WHILE LOOPS\n",
    "• A “while loop” specifies a condition and a block of code\n",
    "that is to be executed until the condition evaluates to\n",
    "False or the loop is explicitly ended with break\n",
    "PASS\n",
    "• pass is the “no-op” statement in Python.\n",
    "• It can be used in blocks where no action is to be taken (or\n",
    "as a placeholder for code not yet implemented);\n",
    "• it is only required because Python uses whitespace to\n",
    "delimit blocks\n",
    "\n",
    "\n",
    "Functions are used to organize code when it needs\n",
    "to be reused in different places.\n",
    "• Each time you anticipate the need to repeat the\n",
    "same set of statements, it is recommended to write\n",
    "a reusable function.\n",
    "• Sometimes, functions help to make the code more\n",
    "readable by giving a name to a group of Python\n",
    "statements.\n",
    "• Functions are defined using def and ending in\n",
    "return\n",
    "• They can receive values and/or return results\n",
    "\n",
    "\n",
    "\n",
    "Indentation Python uses whitespace (tabs or spaces) to structure code\n",
    "instead of using braces as in many other languages like R, C++, Java.\n",
    "• Convention for indentation is four spaces - use it as your default indentation\n",
    "and replace tabs with four spaces.\n",
    "• A colon (:) denotes the start of an indented code block after which all\n",
    "of the code must be indented by the same amount until the end of the\n",
    "block.\n",
    "• Semicolons (;) at the end of each line are not needed but they can be\n",
    "used to separate multiple statements on a single line\n",
    "• Pound sign (#) any text preceded by this symbol will be ignored by the\n",
    "Python interpreter, and it has two main uses:\n",
    "(1) To add explanations or comments to code, and/or\n",
    "(2) To exclude certain blocks of code without deleting them\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Two approaches to import\n",
    "• import datetime\n",
    "• from datetime import datetime, date\n",
    "Short names can be optionally assigned to a library\n",
    "import datetime as dtl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "• Handled by the Python datetime module, which\n",
    "provides datetime, date, and time types\n",
    "• datetime is the most commonly used\n",
    "• Two useful methods for dates\n",
    "• The strftime method formats a datetime as a string\n",
    "and strings can be converted (parsed) into datetime\n",
    "objects with strptime and the corresponding format\n",
    "specifications (see table)\n",
    "Note: When you import the datetime library without\n",
    "specifying objects, you need to type datetime twice\n",
    "to use the strptime method, as follows:\n",
    "datetime.datetime.strptime()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cdd1ff7",
   "metadata": {},
   "source": [
    "LC 2:\n",
    "\n",
    "#String tips and tricks\n",
    "#1. Count positions starting from 0, due to zero-index convention\n",
    "#2. When slicing strings, the lower bound of the range (at the left of the colon) is inclusive\n",
    "#3. When slicing strings, the upper bound of the range (at the right of the colon) is NOT inclusive\n",
    "#4. If lower bound in slicing is ommitted str[:3], it defaults to the beginning of the string or the zero position\n",
    "#5. If upper bound in slicing is ommitted str[2:], it defaults to the end of string.\n",
    "\n",
    "5. DYNAMIC REFERENCES, STRONG TYPES\n",
    "• In contrast with many compiled languages, such as Java and C++,\n",
    "object references in Python have no type associated with them.\n",
    "• Variables are names for objects within a particular namespace; the\n",
    "type information is stored in the object itself.\n",
    "6. DUCK TYPING (DETECT THE TYPE BASED ON OBJECT BEHAVIOR)\n",
    "• Often you may not care about the type of an object but only\n",
    "whether it has certain methods or behavior. This is sometimes\n",
    "called “duck typing” (“If it walks like a duck and quacks like a duck,\n",
    "then it’s a duck.”)\n",
    "• For example, you can verify that an object is iterable with a small\n",
    "module that tries iter(obj) and handles the error it if fails.\n",
    "\n",
    "\n",
    "7. IMPORTS\n",
    "• In Python a module is simply a file with the .py extension\n",
    "containing Python code.\n",
    "• By importing a module, you can use it in the current\n",
    "program.\n",
    "• Import is used to bring libraries into the current program\n",
    "8. MUTABLE AND IMMUTABLE OBJECTS\n",
    "• Most objects in Python, such as lists, dictionaries, NumPy\n",
    "arrays, and most user-defined types (classes), are mutable. This means that the object or values that they contain can\n",
    "be modified.\n",
    "• There are other objects, like strings and tuples, that are\n",
    "immutable.\n",
    "\n",
    "\n",
    "\n",
    "Scalar Types are “single value” types and include numbers, strings, Boolean,\n",
    "none.\n",
    "• Numbers: The primary Python types for numbers are int and float.\n",
    "• String: Text or characters enclosed within single quotes ' or double quotes “\n",
    "• Boolean: The two Boolean values in Python are written as True and False.\n",
    "• None: Is the Python null value type. If a function does not explicitly return a value, it implicitly returns None\n",
    "Type Casting: The str, bool, int, and float can be used as functions to cast\n",
    "values to those types\n",
    "\n",
    "\n",
    "\n",
    "String literals can be written by using either single quotes ' or double\n",
    "quotes “ (or triple quotes for multiline strings)\n",
    "• Example: “I love CIS3120 with Prof. Fich!”\n",
    "• Python strings are immutable; you cannot modify a string.\n",
    "• Strings can be enhanced with input from the user using input\n",
    "• Many Python objects can be converted to a string using the str function\n",
    "• String elements are zero-indexed and accessible by their position in []\n",
    "• Strings can be sliced using :\n",
    "• String methods: len, split, join, replace, capitalize, center, find, strip\n",
    "• Note since strings are immutable, replace creates a new modified string\n",
    "\n",
    "\n",
    "\n",
    "Comparisons and other conditional expressions evaluate to\n",
    "either True or False.\n",
    "• Boolean values are combined with the and and or keywords\n",
    "• In Python, “==” indicates an equality comparison and hence\n",
    "returns True or False as a result.\n",
    "• False is considered equal to zero (0) and True is considered\n",
    "equal to one (1) in Python. None is neither True nor False.\n",
    "• Note that the equality operator (“==”) is different from the\n",
    "assignment operator (“=”)\n",
    "• The inequality operator is the combination of two symbols\n",
    "(“!=”)\n",
    "\n",
    "None ==  False \n",
    "\n",
    "False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4923ac23",
   "metadata": {},
   "source": [
    "Project 4 Pivot tables\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "df = pd.read_csv('NYCFilmPermits_2017-21.csv')\n",
    "#df\n",
    "\n",
    "\n",
    "#Step 1a: add new col with the year of StartDateTime\n",
    "for i in range(len(df['StartDateTime'])):\n",
    "    dateobj = datetime.strptime(df.loc[i,'StartDateTime'], '%m/%d/%Y %H:%M')\n",
    "    df.loc[i,'YearIssued'] = dateobj.year\n",
    "\n",
    "df['YearIssued'] = df['YearIssued'].astype(int) \n",
    "df\n",
    "\n",
    "\n",
    "CountPolicePrecincts\tCategory\tSubCategoryName\tCountry\tZipCode(s)\tCountZipCode YearIssued\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "#Step 1b: Reduce dataframe to keep time_period\n",
    "time_period_begin = df['YearIssued'].astype(float) > 2017 \n",
    "time_period_end = df['YearIssued'].astype(float) < 2021\n",
    "rdf = df[time_period_begin & time_period_end]\n",
    "rdf\n",
    "yearissued\n",
    "2019 \n",
    "2020\n",
    "2018\n",
    "2018\n",
    "2018\n",
    "............\n",
    "\n",
    "#Step 2a: Print number of rows and col in reduced dataset and if there are missing values\n",
    "print(\"The dataset has {0} rows and {1} columns\".format(rdf.shape[0], rdf.shape[1]))\n",
    "print(\"Are there missing values? {}\".format(rdf.isnull().any().any()))\n",
    "\n",
    "\n",
    "The dataset has 20010 rows and 19 columns\n",
    "Are there missing values? True\n",
    "\n",
    "..............................\n",
    "\n",
    "\n",
    "#Step 2b: Create Pivot Table to show number of permits by borough\n",
    "table1 = rdf.pivot_table('EventID',index='Borough', aggfunc='count',margins=True)\n",
    "table1\n",
    "\n",
    "EventID\n",
    "Borough\t\n",
    "Bronx\t800\n",
    "Brooklyn\t6409\n",
    "Manhattan\t9055\n",
    "Queens\t3427\n",
    "Staten Island\t319\n",
    "All\t           20010\n",
    "\n",
    ".........................................\n",
    "\n",
    "#Step 2c: Create Pivot Table to show the maximum duration by year and by category\n",
    "table2 = rdf.pivot_table('DurationHrs',index='Category',columns='YearIssued',aggfunc='max')\n",
    "table2\n",
    "\n",
    "YearIssued\t2018\t2019\t2020\n",
    "Category\t\t\t\n",
    "Commercial\t65.98\t167.98\t36.00\n",
    "Documentary\t130.00\t27.00\t23.98\n",
    "Film\t228.00\t61.00\t36.00\n",
    "Music Video\t41.50\t28.00\t17.00\n",
    "Still Photography\t179.00\t84.00\t19.00\n",
    "Student\t14.00\t13.98\t15.00\n",
    "Television\t744.00\t8610.98\t8640.98\n",
    "\n",
    "\n",
    "................................\n",
    "\n",
    "#Step 2d: Filter dataframe to show number of permits by borough and year for assigned category\n",
    "filmdf = rdf[rdf.Category=='Film']\n",
    "table3 = filmdf.pivot_table('EventID',index='Borough',columns='YearIssued', aggfunc='count')\n",
    "table3\n",
    "\n",
    "YearIssued\t2018\t2019\t2020\n",
    "Borough\t\t\t\n",
    "Bronx\t     74.0\t54.0\t4.0\n",
    "Brooklyn\t448.0\t373.0\t39.0\n",
    "Manhattan\t545.0\t499.0\t87.0\n",
    "Queens\t       91.0\t117.0\t24.0\n",
    "Staten Island\t48.0\t52.0\tNaN\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b13f13a5",
   "metadata": {},
   "source": [
    "Project 3 webscraping\n",
    "\n",
    "import requests\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'Authorization': '***********************'}\n",
    "query_params = {\"q\": \"blockchain\",\n",
    "                \"pageSize\": 47}\n",
    "endpoint = \"https://newsapi.org/v2/everything?\"\n",
    "fetch = requests.get(endpoint, \n",
    "                     headers = headers, \n",
    "                     params = query_params)\n",
    "site = fetch.json()\n",
    "\n",
    "#pprint.pprint(site)\n",
    "\n",
    "articles = site['articles']\n",
    "headlines = []\n",
    "\n",
    "for article in articles:\n",
    "    headlines.append(article[\"title\"])\n",
    "\n",
    "#print(type(headlines))\n",
    "#print(headlines)\n",
    "\n",
    "===================================================\n",
    "col1 = pd.Series(headlines)\n",
    "Character_Length = []\n",
    "Total_Words = []\n",
    "numbersk = []\n",
    "for i in range(len(col1)):\n",
    "    Character_Length.append(len(col1[i]))\n",
    "    Total_Words.append(len(col1[i].split()))\n",
    "    numbersk.append(i+1)\n",
    "    #print(len(col1[i]))\n",
    "col2 = pd.Series(Character_Length)\n",
    "col3 = pd.Series(Total_Words)\n",
    "col4 = pd.Series(numbersk)\n",
    "df = pd.concat([col4,col1,col2,col3],axis=1)\n",
    "df.columns = [\"#\",\"Headlines\",\"Character Length\", \"Total Words\"]\n",
    "df.to_csv('G16Results.csv')\n",
    "df\n",
    "\n",
    "#\tHeadlines\tCharacter Length\tTotal Words\n",
    "0\t1\tThe Future of Digital Cash Is Not on the Block...\t51\t10\n",
    "1\t2\tBlockchains Have a ‘Bridge’ Problem, and Hacke...\t56\t9\n",
    "2\t3\tNot Everyone Wants NFTs to Be the Future of Ga...\t50\t10\n",
    "3\t4\tCrypto Goes to War in Ukraine\t29\t6\n",
    "4\t5\tConti Leak: A Ransomware Gang's Chats Expose I...\t61\t10\n",
    "5\t6\tBiden to Feds: Figure Out This Crypto Thing, Stat\t49\t9\n",
    "6\t7\tEx-Party Producer Charged in $2.7 Million Bitc...\t67\t8\n",
    "7\t8\tBlockchain company Ronin hit by $615 million c...\t67\t11\n",
    "8\t9\tBlockchain Limitations to Consider Today\t40\t5\n",
    "\n",
    "\n",
    "#Average number of characters\n",
    "total_char = 0\n",
    "for i in range(len(df)):\n",
    "    total_char += df[\"Character Length\"][i]\n",
    "#print(total_char)\n",
    "avg_char = total_char/len(df)\n",
    "print(\"The average number of characters in the headlines is: {:.2f}\".format(avg_char))\n",
    "\n",
    "\n",
    "The average number of characters in the headlines is: 63.40\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "\n",
    "#Average number of words\n",
    "total_word = 0\n",
    "for i in range(len(df)):\n",
    "    total_word += df[\"Total Words\"][i]\n",
    "#print(total_word)\n",
    "avg_word = total_word/len(df)\n",
    "print(\"The average number of words in the headlines is: {:.2f}\".format(avg_word))\n",
    "\n",
    "The average number of words in the headlines is: 10.47\n",
    "\n",
    "\n",
    "The approach i took was to break it down one column at a time. So i decided to start off with a empty series. After obtaining the headlines from the website and putting that into an empty list, i then converted the list into a series. With the headline series done, i then used a range loop to access each entry in the series and got the length of the string giving us the number of chracters in the string in each entry. This then gets saved into a new empty series. Within the same range loop, using the string function of split(), we got the number of words in the headlines for each entry, which was then saved into a new empty series. We also created another series for the corresponding numbers which was included in the same range loop. At the end, the four series are concated into one dataframe. Using the same logic, we calculated the average number of characters and words by using a range loop to access each entry in the columns we needed. We then divided these totals by the length of the dataframe for the averages.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68ac7147",
   "metadata": {},
   "source": [
    "GP 2: \n",
    "total = sum(reviews['Price'].value_counts())\n",
    "free_apps = reviews['Price'] == 0\n",
    "reviews[free_apps].head()\n",
    "total_free = sum(reviews[free_apps].value_counts())\n",
    "print('The percentage of free apps is: %.2f' %((total_free/total) * 100))\n",
    "\n",
    "The percentage of free apps is: 10.62\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "264a2e1c",
   "metadata": {},
   "source": [
    "pivot table quiz question\n",
    "\n",
    "A pivot table is defined as a summary of\n",
    "\t\tone or more numeric variables based on two other categorical variables correct\n",
    "\t\t\n",
    "        wrong\n",
    "        a single categorical variable based on another categorical variable\n",
    "\t\ta single numerical variable based on categorical variable\n",
    "\t\tone or more categorical variables based on two numeric variables\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad047bf6",
   "metadata": {},
   "source": [
    "This statement will display the data types of each column of dataframe DF:\n",
    "\t\tA. DF.types()\n",
    "\t\tB. DF.dtypes   ans\n",
    "\t\tC. DF.dtypes()\n",
    "\t\tD. DF.typesD()\n",
    "\n",
    "\n",
    "Which of the following is (are) correct about the features of a DataFrame?\n",
    "\n",
    "\t\t\n",
    "A. Potentially columns are of different types\n",
    "\n",
    "\t\t\n",
    "B. Can Perform Arithmetic operations on rows and columns\n",
    "\n",
    "\t\t\n",
    "C. Labeled axes (rows and columns)\n",
    "\n",
    "\t\t\n",
    "D. All of the above  ans\n",
    "\n",
    "\n",
    "s.add(s3, fill_value=0)\n",
    " a 10.0\n",
    " b -5.0\n",
    " c 5.0\n",
    " d 7.0\n",
    ">>> s.sub(s3, fill_value=2)\n",
    ">>> s.div(s3, fill_value=4)\n",
    ">>> s.mul(s3, fill_value=3)\n",
    "\n",
    "\n",
    "s.add(s3 , fill_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s3 = pd.Series([7, -2, 3], index=['a', 'c', 'd'])\n",
    ">>> s + s3\n",
    " a 10.0\n",
    " b NaN\n",
    " c 5.0\n",
    " d 7.0\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " Using the NumPy random module, how can you return a random number from 0 to 100?\n",
    "\t\trandom.rand()\n",
    "\t\trandom.rand(100)\n",
    "\t\trandom.rand(99)\n",
    "\t\trandom.randint(100) answer\n",
    "\n",
    "Which tag is used to add a header in an HTML5 table\n",
    "th \n",
    "\n",
    "\n",
    "Which method can be used to return a string in upper case letters?\n",
    "\t\tuppercase()\n",
    "\t\tupper()   answer\n",
    "\t\ttoUpperCase()\n",
    "\t\tupperCase()\n",
    "        \n",
    "        \n",
    "        \n",
    "   Suppose that the array A=np.arange(-1,1, 0.5) is converted to a Pandas Series Z, and the index is set to consecutive letters such as a,b,c, etc. Which of the following is not a valid way to access data in the series?\n",
    "\n",
    "\t\t\n",
    "A. Z.loc['a']\n",
    "\n",
    "\t\t\n",
    "B. Z.loc[['a']]\n",
    "\n",
    "\t\t\n",
    "C. Z.loc['a', 'b'] answer\n",
    "\n",
    "\t\t\n",
    "D. Z.loc[['a', 'b']]\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "In the following Pandas Series: p1 = pd.Series([3.1, 3.2, 3.3, 3.4, 3.5], index = ['A', 'B', 'C', 'D', 'E']), what would be the result of p1.index[2:4]?\n",
    "\t\tA. the corresponding values in series slice\n",
    "\t\tB. the corresponding index labels in the series slice   answer\n",
    "\t\tC. Both the values and the labels in the series slice\n",
    "\t\tD. The statement will raise an error\n",
    "\n",
    "his statement will display the data types of each column of dataframe DF:\n",
    "\t\tA. DF.types()\n",
    "\t\tB. DF.dtypes answer\n",
    "\t\tC. DF.dtypes()\n",
    "\t\tD. DF.typesD()\n",
    "  \n",
    "A DataFrame created from single Series has: \n",
    "1 column\n",
    "\n",
    "========================================\n",
    "\n",
    "When using the json library, which of the following methods reads the json file and converts it to a dictionary?\n",
    "\n",
    "\t\t\n",
    "json.parse\n",
    "\n",
    "\t\t\n",
    "json.dump\n",
    "\n",
    "\t\t\n",
    "json.get\n",
    "\n",
    "\t\t\n",
    "json.load answer\n",
    "\n",
    "\n",
    "Given a data frame df with columns ['A', 'B', 'C', 'D'] and rows ['r1', 'r2', 'r3']. Which of the following expression filters the rows whose column B values are greater than 45?\n",
    "\t\tdf.B > 45\n",
    "\t\tdf[df.B > 45] answer\n",
    "\t\tdf.loc[B > 45]\n",
    "\t\tdf.iloc[df.B > 45]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dff50d",
   "metadata": {},
   "source": [
    "# SOLUTIONS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec54c0ce",
   "metadata": {},
   "source": [
    "# 1. yes mutable operations do not work on series indexes\n",
    "# 2. implicit is consecutive and numeric (0,1,2...etc) ; explicit index slices include upper bound.\n",
    "# 3. iloc : references the implicit index and loc : always references the explicit index ; rule : 'explicit is better than implicit'\n",
    "# 4 . Absolutely shares.index.name = 'Weekdays' print(shares.index.name)\n",
    "# 5. if series objects in a sequence are aligned they share the same index. \n",
    "# 6-8 true\n",
    "# 9. False, we do it to access dataframe\n",
    "# 10. obj.attribute_name\n",
    "# 11. checking to see whether it iterates\n",
    "# 12. Long integers : 4250358183513L , Complex Numbers: 3.1391134819j\n",
    "# 13. input\n",
    "# 14. True\n",
    "# 15. all the above\n",
    "# 16. == equality operator     = assignment operator   != combination of both \n",
    "# 17. via the set function or literal with curly braces\n",
    "# 18. allows you to combine an if-else block into a single line \n",
    "# 19. range produces all integers included in parameter NO! integers up to but not including endpoint\n",
    "# 20. semicolon\n",
    "# 21. Webscraping is used for which of the following: \n",
    "#    a. stat analyses b. text mining c. predicted modeling d. geo-visualization e. c and d only f. All the above\n",
    "# 21. HTML\n",
    "# ========== > HTTP Request with URL \n",
    "# <=========== HTTP reply with web page contents\n",
    " #22. Are strings and tuples immutable? Yes \n",
    "# 23. What are scalar types? single value types including numbers, strings, boolean, none\n",
    "# 24. Type Casting: str, bool, int , and float can be used as functions to cast values to those types\n",
    "# 25. except with keys only, no values , syntax = literal with curly braces {} ; ([]) - set\n",
    "# 26. a tenary expression is if-else block that produced a value constructed into one line format\n",
    "# 27. DNS \n",
    "# 28.response objects, with all the response data(content, encoding, status, etc) syntax requests.methodname()\n",
    "# 29. r.content ---> returns data in bytes r.text returns data in string format\n",
    "#  <th> </th> <td> </td> and <tr> </tr> header cell, data cell, row cell\n",
    "# <href = 'www.chicagotribune.com'>Name </a> ; <img src = '...path....'>\n",
    "# <table>...</table> for start & end of table\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a38831f0",
   "metadata": {},
   "source": [
    "30. %%HTML\n",
    "31. parsers convert the input into a graph or tree structure for processing. The default parser is 'lxml' which is leniant but\n",
    "platform dependent. We will use 'html.parser', which is part of BeatifulSoup\n",
    "32. find results a single object, whereas find_all returns a list-like object. \n",
    "33. A   34. T\n",
    "35. attributes are always specified in the start tag? \n",
    "36. list of lists, dictionary of dictionaries, list of dictionaries, dictionaries with embedded lists, Description of species(animals, plants)\n",
    "37.print(type(alltables)) expected: <class 'bs4.element.ResultSet'>\n",
    "38. Numbered list to each item in iteration\n",
    "39. single table structure\n",
    "40. view source on html webpage\n",
    "41. Nothing\n",
    "42. attribute\n",
    "43. ndarray ; fixed type attribute\n",
    "44. false\n",
    "45. dtype\n",
    "46. the number of dimensions in the rank ; the shape of an array is a tuple of integers giving the size of the array along each dimension. \n",
    "47. false we recieve psuedo random numbers since computers run on step by step algorithms\n",
    "randint takes size parameter\n",
    "48. np.random.seed(o) accomplishes reproducibility, makes the random numbers predictable, with seed resetm the same set of numbers appears every time. \n",
    "49. views (subarrays no copy system) in arrays, if a subarray is modified, the original array is changed! To explicitly copy the data within an array or a subarray use the copy() method. \n",
    "50. [[1 2 3]] size is three\n",
    "51. A universal function(or ufunc for short) is a function that operates on ndarrays in an element-by-element fashion\n",
    "Unary vs Binary ufuncs if they take one or two inputs\n",
    "e.g. np.add(2,x) vs np.sum(x)\n",
    "52. np.add() is used to add two arrays(or a scalar and an array) element-wise whereas np.sum() returns the sum of all elements in a single array. \n",
    "53. vectorization refers to the use of pre-compiled functions and mathematics operations on Numpy array objects.\n",
    "vectorization also provides broadcasting and additional methods like reduce, accumulate etc. that are very helpful for computation\n",
    "{ the alternative to vectorization would be to iterate through the elements of the array and perform the desired operations. Vectorization is much faster than iterating) \n",
    "54. np.sqrt(x) unary ufuncs\n",
    "55. 10 randome decimal float numbers could be negative\n",
    "56. broadcasting means the ability to perfome operations between arrays of different sizes\n",
    "e.g. adding one number to a whole array is an example of broadcasting\n",
    "57.nothing\n",
    "58. np.matmul(A,B) returns the matric product of two arrays\n",
    "    np.multiply(A,B) returns the element-wise matrix multiplication\n",
    "59. the timeit module is used to compute the execution time of small bits of Python code. The module uses a platfourm-specific time function to make the most accurate calculations possible. \n",
    "60. vectorized\n",
    "61. False\n",
    "62. True\n",
    "63. is that indices cannot be modified via the normail means(i.e., indexes are immutable) in case of pandas index objects\n",
    "64. loc - always references the explicit index\n",
    "     iloc =-  '''                 implicit\n",
    "65. sentence complete (nvm)\n",
    "66. indexed , labeled\n",
    "67. [all rows :, 1] column after 1 onwards\n",
    "68. answered in question\n",
    "69. Pandas concatenation preserves indices, even if the results will have duplicate indices.\n",
    "70. difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42f55118",
   "metadata": {},
   "source": [
    "71. False, the last pair is followed by a comma\n",
    "72. JSON is a file format used to represent and store data whereas a Python Dictionary is the actualy data structure(object) that is kept in memory while a Python program runs. The key-value pairs of the JSON file can be used to create a\n",
    "Python dictionary with the help of special libraries.\n",
    "JSON is the string representation of the data and\n",
    "dictionaries are the actual data structures in memory that\n",
    "are created when the program runs\n",
    "73. JSON, XML, or CSV\n",
    "74. REST (representational state transfer) designed for stability , accessed through requests, you can preview query through browser. Streaming API, you will want to use a library built to help manage data intake. API server returns response code 200 if successful\n",
    "75. returns JSon object\n",
    "76. API endpoints(urls to get info through the request library)\n",
    "77. descending order (highest to lowester) ascending = false\n",
    "78. Linear\n",
    "79. observation, features\n",
    "80. perfect positive correlation\n",
    "(weaker coorelations if between 0.5 ad -0.5)\n",
    "81. np.corrcoef\n",
    "82. A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it's mentioned within a given text and the more important it is. \n",
    "83. seaborn\n",
    "84. give me 6 numbers between 1 and 10 randomized\n",
    "85. Describe Pandas crossTab: the joint frequency of the two categorical variables that define the row and column groupings. \n",
    "cross-tabulation is bidimensional grid where the cells show the counts of elements with the characteritics indicated in the intersection of the row and the column.\n",
    "86. Groupby is useful to create abstractions(views) of the data in one-dimensional categories. Pivot Tables are used to run analyses on more than one dimension, and they are the main tool for most of the data analysis needs. CrossTab method allows you to build a cross-tabulation table to show the frequency with which certain groups of data appear. \n",
    "87. Some sites establish levels of the data volume transmitted, and charge accordingly. This is known as tiered access. \n",
    "88. dataframe converted to csv file\n",
    "89. AN API wrapper is a 'language-specific package or kit that encapsulates multiple API calls to make complicated functions easy to use. It is helpful to automate processes that rely on APIs and to facilitate the work of developers when they need to use multiple APIs. \n",
    "90. True\n",
    "91. .get\n",
    "92. exact location of ISS \n",
    "93. The coordinates of said location (lat, long) can be included in a dictionary and passed using the params argument\n",
    "94. open\n",
    "95. developer\n",
    "96. Reddit\n",
    "97. Hacker news is a social news website focusing on cs and entrepreneurship. Despite its name, it is the most trusted, widely-read, independent source of the latest news and tehccnical coverage on cybersecurity, hacker threads, and infosec trends, URL is : \n",
    "https://news.ycombinator.com/ , the site design is fairly simple. The landing page shows a list of news headlines. \n",
    "98. web site page TITLE\n",
    "99. If you only want the human readable test inside a document or tag, you can use the get_text() method. It returns all the text in a document or beneath a tag, as a single Unicode string. You can tell Beautiful Soup to strip whitespace from the beginning and end of each bit of text with strip = true\n",
    "100. Answered in Question\n",
    "101. 'a' , id = ''\n",
    "102. API key - identifies the application\n",
    "     API secret - acts as a password for the application\n",
    "     Token - identifies the user \n",
    "     Token secret - Acts as a password for the user\n",
    "103. NEws API is a simple HTTP REST API\n",
    "104. Answered\n",
    "105. pd.read_csv parameters\n",
    "sep: is used to specify a customer delimiter in the data file such as tab. Comma is the default\n",
    "usecols: is the list of columns to read. THis allows to load only the data you need from the data file into the DataFrame\n",
    "header: is used to specify whether the csv datafiles has headers or not, or where the header row is located (more details in lc 15). \n",
    "skiprows: is used to exclude rows from the data file and load only the rows needed into the DataFrame. \n",
    "engine: indicates the datafile parser engine such as 'c' or 'python'\n",
    "There are two parer engines - c and python \n",
    "c parser faster. Typically, this parameter is spcecified only when the separators are special and there is a ParserWarning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76a7ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffefbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
